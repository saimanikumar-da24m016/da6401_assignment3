{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:30:51.736403Z",
     "iopub.status.busy": "2025-05-20T15:30:51.736115Z",
     "iopub.status.idle": "2025-05-20T15:31:01.000308Z",
     "shell.execute_reply": "2025-05-20T15:31:00.999701Z",
     "shell.execute_reply.started": "2025-05-20T15:30:51.736373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m016\u001b[0m (\u001b[33mda24m016-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "my_secret = user_secrets.get_secret(\"WANDB_API\") \n",
    "\n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:31:01.001889Z",
     "iopub.status.busy": "2025-05-20T15:31:01.001544Z",
     "iopub.status.idle": "2025-05-20T15:31:05.549722Z",
     "shell.execute_reply": "2025-05-20T15:31:05.548959Z",
     "shell.execute_reply.started": "2025-05-20T15:31:01.001871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Dataset Definition with special tokens ----\n",
    "class DakshinaTSVDataset(Dataset):\n",
    "    def __init__(self, tsv_file, src_vocab=None, tgt_vocab=None, max_len=64, build_vocab=False):\n",
    "        df = pd.read_csv(tsv_file, sep='\\t', header=None,\n",
    "                         names=['native', 'roman', 'freq'], usecols=[0, 1], dtype=str)\n",
    "        # Fix the pandas warning by using a copy\n",
    "        df = df.copy()\n",
    "        df['native'] = df['native'].fillna('')\n",
    "        df['roman'] = df['roman'].fillna('')\n",
    "        self.pairs = list(zip(df['roman'], df['native']))\n",
    "        print(f\"Loaded {len(self.pairs)} examples from {tsv_file}\")\n",
    "        \n",
    "        # Print a few examples\n",
    "        if len(self.pairs) > 0:\n",
    "            print(\"Sample examples:\")\n",
    "            for i in range(min(3, len(self.pairs))):\n",
    "                print(f\"  Roman: '{self.pairs[i][0]}', Native: '{self.pairs[i][1]}'\")\n",
    "                \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.src_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self.tgt_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n",
    "            # Ensure special tokens exist\n",
    "            for v in ('<eos>', '<sos>'):\n",
    "                if v not in self.src_vocab: self.src_vocab[v] = len(self.src_vocab)\n",
    "                if v not in self.tgt_vocab: self.tgt_vocab[v] = len(self.tgt_vocab)\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        for src, tgt in self.pairs:\n",
    "            for ch in src:\n",
    "                if ch not in self.src_vocab: self.src_vocab[ch] = len(self.src_vocab)\n",
    "            for ch in tgt:\n",
    "                if ch not in self.tgt_vocab: self.tgt_vocab[ch] = len(self.tgt_vocab)\n",
    "        print(f\"Vocab sizes -> src: {len(self.src_vocab)}, tgt: {len(self.tgt_vocab)}\")\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        \n",
    "        # Add <sos> and <eos> tokens\n",
    "        src_idxs = [self.src_vocab['<sos>']] + [self.src_vocab.get(ch, self.src_vocab['<unk>']) for ch in src] + [self.src_vocab['<eos>']]\n",
    "        tgt_idxs = [self.tgt_vocab['<sos>']] + [self.tgt_vocab.get(ch, self.tgt_vocab['<unk>']) for ch in tgt] + [self.tgt_vocab['<eos>']]\n",
    "        \n",
    "        # Pad sequences\n",
    "        pad_src = [self.src_vocab['<pad>']] * max(0, self.max_len - len(src_idxs))\n",
    "        pad_tgt = [self.tgt_vocab['<pad>']] * max(0, self.max_len - len(tgt_idxs))\n",
    "        \n",
    "        # Truncate if necessary and convert to tensor\n",
    "        src_tensor = torch.tensor((src_idxs + pad_src)[:self.max_len], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor((tgt_idxs + pad_tgt)[:self.max_len], dtype=torch.long)\n",
    "        \n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "# ---- Encoder with bidirectional support ----\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout=0, bidirectional=True, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "            \n",
    "        # Initialize weights\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Create mask for attention\n",
    "        mask = (x != 0).float()  # 0 is <pad>\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_size]\n",
    "        \n",
    "        # Pass through RNN\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # No need to reshape hidden states - just return them as is\n",
    "        # The decoder will handle the format conversion\n",
    "        \n",
    "        # Return encoder outputs, hidden state, and mask\n",
    "        return outputs, hidden, mask\n",
    "\n",
    "# ---- Attention Mechanism ----\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super().__init__()\n",
    "        # Create a linear layer to convert the concatenated hidden states to attention scores\n",
    "        self.energy = nn.Linear(enc_hidden_size + dec_hidden_size, dec_hidden_size)\n",
    "        self.v = nn.Linear(dec_hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, dec_hidden_size]\n",
    "        # encoder_outputs: [batch_size, src_len, enc_hidden_size]\n",
    "        # mask: [batch_size, src_len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        # [batch_size, dec_hidden_size] -> [batch_size, src_len, dec_hidden_size]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Create energy by concatenating encoder outputs and decoder hidden\n",
    "        # [batch_size, src_len, enc_hidden_size + dec_hidden_size]\n",
    "        energy = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        \n",
    "        # Apply attention layer\n",
    "        # [batch_size, src_len, dec_hidden_size]\n",
    "        energy = torch.tanh(self.energy(energy))\n",
    "        \n",
    "        # Get attention scores\n",
    "        # [batch_size, src_len, 1]\n",
    "        attention = self.v(energy)\n",
    "        \n",
    "        # [batch_size, src_len]\n",
    "        attention = attention.squeeze(2)\n",
    "        \n",
    "        # Mask out padding positions\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        # [batch_size, src_len]\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "# ---- Decoder with attention and teacher forcing ----\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, enc_hidden_size, dec_hidden_size, \n",
    "                 num_layers, dropout=0, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        # Initialize attention mechanism\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        \n",
    "        # Context vector + embedding size as input to RNN\n",
    "        rnn_input_size = embedding_size + enc_hidden_size\n",
    "        \n",
    "        # Initialize RNN based on cell type\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        # Final output layer that combines decoder output, context and embedding\n",
    "        self.fc_out = nn.Linear(dec_hidden_size + enc_hidden_size + embedding_size, output_size)\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [num_layers, batch_size, dec_hidden_size] or tuple for LSTM\n",
    "        # encoder_outputs: [batch_size, src_len, enc_hidden_size]\n",
    "        # mask: [batch_size, src_len]\n",
    "        \n",
    "        # Embed input token\n",
    "        # [batch_size] -> [batch_size, 1, embedding_size]\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        \n",
    "        # Get appropriate hidden state for attention\n",
    "        if self.cell_type == 'lstm':\n",
    "            h_for_attn = hidden[0][-1]  # use last layer's hidden state\n",
    "        else:\n",
    "            h_for_attn = hidden[-1]  # use last layer's hidden state\n",
    "            \n",
    "        attn_weights = self.attention(h_for_attn, encoder_outputs, mask)\n",
    "        \n",
    "        # Create context vector by weighting encoder outputs with attention\n",
    "        # [batch_size, 1, src_len] * [batch_size, src_len, enc_hidden_size]\n",
    "        # -> [batch_size, 1, enc_hidden_size]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        # Combine embedded token and context vector\n",
    "        # [batch_size, 1, embedding_size + enc_hidden_size]\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        # output: [batch_size, 1, dec_hidden_size]\n",
    "        # hidden: [num_layers, batch_size, dec_hidden_size] or tuple for LSTM\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # Combine output, context and embedding for final prediction\n",
    "        # [batch_size, 1, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = torch.cat((output, context, embedded), dim=2)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        # [batch_size, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        # Pass through final linear layer\n",
    "        # [batch_size, output_size]\n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "# ---- Complete Seq2Seq Model with Teacher Forcing ----\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.7):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tgt: [batch_size, tgt_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len-1, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder\n",
    "        # For bidirectional LSTM, we need to initialize the decoder hidden state properly\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            # If encoder is bidirectional, we need to handle the hidden state\n",
    "            if self.encoder.bidirectional:\n",
    "                # Split the bidirectional layers\n",
    "                # hidden_state: [num_layers*2, batch_size, hidden_size]\n",
    "                # We need to reshape to [num_layers, batch_size, hidden_size*2]\n",
    "                \n",
    "                # Initialize new hidden_state\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                    self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                    self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                # For each decoder layer\n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    # If encoder has enough layers, use the corresponding layers\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        # Concatenate forward and backward directions from encoder\n",
    "                        # Forward: hidden_state[i*2]\n",
    "                        # Backward: hidden_state[i*2+1]\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        # Copy to decoder hidden state, possibly with projection if sizes don't match\n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            # Simple projection by truncation/padding\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                # Update hidden state\n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                # Similar approach as LSTM but with only one hidden state\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input to decoder is the <sos> token (already embedded in tgt)\n",
    "        input = tgt[:, 0]\n",
    "        \n",
    "        # Teacher forcing ratio determines how often to use true target as input\n",
    "        use_teacher_forcing = random.random() < self.teacher_forcing_ratio\n",
    "        \n",
    "        # Decode one token at a time\n",
    "        for t in range(1, tgt_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Store output\n",
    "            outputs[:, t-1] = output\n",
    "            \n",
    "            # Next input is either true target (teacher forcing) or predicted token\n",
    "            if use_teacher_forcing:\n",
    "                input = tgt[:, t]\n",
    "            else:\n",
    "                # Get highest scoring token\n",
    "                input = output.argmax(1)\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    # For inference (no teacher forcing)\n",
    "    def decode(self, src, max_len=100):\n",
    "        # src: [batch_size, src_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder - same as in forward method\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input is <sos> token\n",
    "        input = torch.ones(batch_size, dtype=torch.long).to(self.device) * 3  # <sos> = 3\n",
    "        \n",
    "        # Track generated tokens\n",
    "        outputs = [input]\n",
    "        attentions = []\n",
    "        \n",
    "        # Track if sequence has ended\n",
    "        ended = torch.zeros(batch_size, dtype=torch.bool).to(self.device)\n",
    "        \n",
    "        # Decode until max length or all sequences end\n",
    "        for t in range(1, max_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, attn = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Get next token\n",
    "            input = output.argmax(1)\n",
    "            \n",
    "            # Store output\n",
    "            outputs.append(input)\n",
    "            attentions.append(attn)\n",
    "            \n",
    "            # Check if all sequences have ended\n",
    "            ended = ended | (input == 2)  # 2 is <eos>\n",
    "            if ended.all():\n",
    "                break\n",
    "                \n",
    "        # Convert list of tensors to single tensor\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len]\n",
    "        attentions = torch.stack(attentions, dim=1)  # [batch_size, seq_len-1, src_len]\n",
    "        \n",
    "        return outputs, attentions\n",
    "\n",
    "# ---- Metrics & Utils ----\n",
    "def compute_exact_match_accuracy(preds, targets, tgt_vocab):\n",
    "    \"\"\"Compute exact match accuracy between predictions and targets\"\"\"\n",
    "    batch_size = preds.size(0)\n",
    "    correct = 0\n",
    "    \n",
    "    # Convert ids to strings\n",
    "    id_to_char = {v: k for k, v in tgt_vocab.items() if k not in ['<pad>', '<sos>', '<eos>', '<unk>']}\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Extract character sequences (removing special tokens)\n",
    "        pred_seq = ''.join([id_to_char.get(idx.item(), '') for idx in preds[i, 1:] \n",
    "                            if idx.item() not in [0, 1, 2, 3]])  # Skip <pad>, <unk>, <eos>, <sos>\n",
    "        \n",
    "        # For target, skip first token (<sos>) and stop at <eos> or <pad>\n",
    "        tgt_seq = ''\n",
    "        for idx in targets[i, 1:]:  # Skip first token\n",
    "            token_id = idx.item()\n",
    "            if token_id in [0, 2]:  # <pad> or <eos>\n",
    "                break\n",
    "            if token_id not in [1, 3]:  # Skip <unk> and <sos>\n",
    "                tgt_seq += id_to_char.get(token_id, '')\n",
    "        \n",
    "        # Check for exact match\n",
    "        if pred_seq == tgt_seq:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / batch_size\n",
    "\n",
    "def compute_char_accuracy(logits, targets):\n",
    "    \"\"\"Compute character-level accuracy between logits and targets\"\"\"\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    mask = (targets != 0)  # Ignore padding\n",
    "    correct = ((preds == targets) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# ---- Training & Evaluation Functions ----\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_char_acc = 0\n",
    "    epoch_exact_match_acc = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for src, tgt in tqdm(dataloader, desc=\"Training\"):\n",
    "        batch_size = src.size(0)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Flatten output and target tensors for loss calculation\n",
    "        # Ignore the first token in target (<sos>)\n",
    "        output_flat = output.reshape(-1, output.shape[-1])\n",
    "        target_flat = tgt[:, 1:].reshape(-1)  # Shift right to predict next token\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_flat, target_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        char_acc = compute_char_accuracy(output, tgt[:, 1:])\n",
    "        \n",
    "        # Decode for exact match accuracy\n",
    "        with torch.no_grad():\n",
    "            predictions, _ = model.decode(src)\n",
    "            exact_match_acc = compute_exact_match_accuracy(predictions, tgt, dataloader.dataset.tgt_vocab)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_char_acc += char_acc * batch_size\n",
    "        epoch_exact_match_acc += exact_match_acc * batch_size\n",
    "        total_batches += batch_size\n",
    "    \n",
    "    # Return average metrics\n",
    "    return {\n",
    "        'loss': epoch_loss / total_batches,\n",
    "        'char_acc': epoch_char_acc / total_batches,\n",
    "        'exact_match_acc': epoch_exact_match_acc / total_batches\n",
    "    }\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_char_acc = 0\n",
    "    epoch_exact_match_acc = 0\n",
    "    total_batches = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_size = src.size(0)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass (use teacher forcing for loss calculation)\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Flatten output and target tensors for loss calculation\n",
    "            output_flat = output.reshape(-1, output.shape[-1])\n",
    "            target_flat = tgt[:, 1:].reshape(-1)  # Shift right to predict next token\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            char_acc = compute_char_accuracy(output, tgt[:, 1:])\n",
    "            \n",
    "            # Decode for exact match accuracy (no teacher forcing)\n",
    "            predictions, _ = model.decode(src)\n",
    "            exact_match_acc = compute_exact_match_accuracy(predictions, tgt, dataloader.dataset.tgt_vocab)\n",
    "            \n",
    "            # Count exact matches for reporting\n",
    "            correct_batch = int(exact_match_acc * batch_size)\n",
    "            correct_predictions += correct_batch\n",
    "            total_predictions += batch_size\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            epoch_char_acc += char_acc * batch_size\n",
    "            epoch_exact_match_acc += exact_match_acc * batch_size\n",
    "            total_batches += batch_size\n",
    "    \n",
    "    # Return average metrics\n",
    "    return {\n",
    "        'loss': epoch_loss / total_batches,\n",
    "        'char_acc': epoch_char_acc / total_batches,\n",
    "        'exact_match_acc': epoch_exact_match_acc / total_batches,\n",
    "        'correct': correct_predictions,\n",
    "        'total': total_predictions\n",
    "    }\n",
    "\n",
    "# ---- WandB Sweep Configuration for Attention Model ----\n",
    "sweep_config_attention = {\n",
    "    \"name\": \"Seq2Seq_Attention\",\n",
    "    \"method\": \"bayes\",\n",
    "    'metric': {\n",
    "        'name': 'validation_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'cell_type': {\n",
    "            'values': ['lstm', 'gru']\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0, 0.1, 0.2]\n",
    "        },\n",
    "        'embedding_size': {\n",
    "            'values': [256, 512]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [64, 128]\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [256, 512]\n",
    "        },\n",
    "        'bidirectional': {\n",
    "            'values': [True]  # Fixed to True for attention model for better results\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            \"values\": [0.0005, 0.001, 0.002]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [15, 20]\n",
    "        },\n",
    "        'optim': {\n",
    "            \"values\": ['adam', 'nadam']\n",
    "        },\n",
    "        'teacher_forcing': {\n",
    "            \"values\": [0.5, 0.7, 0.9]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T05:33:06.529353Z",
     "iopub.status.busy": "2025-05-19T05:33:06.529073Z",
     "iopub.status.idle": "2025-05-19T05:33:38.210767Z",
     "shell.execute_reply": "2025-05-19T05:33:38.209962Z",
     "shell.execute_reply.started": "2025-05-19T05:33:06.529330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2mfomt77\n",
      "Sweep URL: https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/sweeps/2mfomt77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iq9yblcn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_053313-iq9yblcn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/runs/iq9yblcn' target=\"_blank\">worthy-sweep-1</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/sweeps/2mfomt77' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/sweeps/2mfomt77</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/sweeps/2mfomt77' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/sweeps/2mfomt77</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/runs/iq9yblcn' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration-attention/runs/iq9yblcn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded existing vocabulary\n",
      "Loaded 58550 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkita', Native: 'అంకిత'\n",
      "  Roman: 'ankita', Native: 'అంకిత'\n",
      "  Roman: 'ankitha', Native: 'అంకిత'\n",
      "Loaded 5683 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amka', Native: 'అంక'\n",
      "  Roman: 'anka', Native: 'అంక'\n",
      "  Roman: 'amkam', Native: 'అంకం'\n",
      "Model has 5,654,851 parameters (5,654,851 trainable)\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 31/458 [00:18<03:42,  1.92it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "Training:  23%|██▎       | 106/458 [00:49<02:16,  2.58it/s]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize wandb\n",
    "    wandb.login()\n",
    "    \n",
    "    # Create the sweep\n",
    "    sweep_id = wandb.sweep(sweep_config_attention, project=\"dakshina-transliteration-attention\")\n",
    "    \n",
    "    # Start the sweep agent\n",
    "    wandb.agent(sweep_id, sweep_run_attention, count=20)  # Adjust count as needed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# attn-gru-e512-h512-n1-d0-tf0.7-lr0.0005-bs128-nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:33:37.607576Z",
     "iopub.status.busy": "2025-05-19T18:33:37.606682Z",
     "iopub.status.idle": "2025-05-19T18:33:37.630498Z",
     "shell.execute_reply": "2025-05-19T18:33:37.629749Z",
     "shell.execute_reply.started": "2025-05-19T18:33:37.607523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import necessary classes and functions from your provided code\n",
    "# Since they're already defined in your files, I'll reference them directly\n",
    "\n",
    "def train_and_evaluate_best_model(save_dir):\n",
    "    \"\"\"\n",
    "    Train a model with the best hyperparameters, evaluate on test set, and save the weights\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory to save model weights and results\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Best hyperparameters based on previous sweep results\n",
    "    best_config = {\n",
    "        'cell_type': 'gru',\n",
    "        'dropout': 0,\n",
    "        'embedding_size': 512,\n",
    "        'num_layers': 1,\n",
    "        'batch_size': 128,\n",
    "        'hidden_size': 512,\n",
    "        'bidirectional': True,\n",
    "        'learning_rate': 0.0005,\n",
    "        'epochs': 15,\n",
    "        'teacher_forcing': 0.7,\n",
    "        'optim': 'nadam',\n",
    "    }\n",
    "    \n",
    "    # Data paths - adjust these if needed based on your environment\n",
    "    train_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'\n",
    "    dev_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
    "    test_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
    "    \n",
    "    # Load or build vocabulary\n",
    "    vocab_dir = os.path.join(save_dir, 'vocab')\n",
    "    os.makedirs(vocab_dir, exist_ok=True)\n",
    "    \n",
    "    vocab_file = os.path.join(vocab_dir, 'src_vocab.json')\n",
    "    if os.path.exists(vocab_file):\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'r') as f:\n",
    "            src_vocab = json.load(f)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'r') as f:\n",
    "            tgt_vocab = json.load(f)\n",
    "        print(\"Loaded existing vocabulary\")\n",
    "    else:\n",
    "        print(\"Building new vocabulary\")\n",
    "        train_dataset = DakshinaTSVDataset(train_tsv, build_vocab=True)\n",
    "        src_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n",
    "        \n",
    "        # Save vocabulary\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(src_vocab, f, ensure_ascii=False)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "        print(\"Saved vocabulary\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DakshinaTSVDataset(train_tsv, src_vocab, tgt_vocab)\n",
    "    val_dataset = DakshinaTSVDataset(dev_tsv, src_vocab, tgt_vocab)\n",
    "    test_dataset = DakshinaTSVDataset(test_tsv, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=best_config['batch_size'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_config['batch_size'])\n",
    "    \n",
    "    # Create model components\n",
    "    encoder = Encoder(\n",
    "        input_size=len(src_vocab),\n",
    "        embedding_size=best_config['embedding_size'],\n",
    "        hidden_size=best_config['hidden_size'],\n",
    "        num_layers=best_config['num_layers'],\n",
    "        dropout=best_config['dropout'],\n",
    "        bidirectional=best_config['bidirectional'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Calculate encoder output size (doubled if bidirectional)\n",
    "    enc_hidden_size = best_config['hidden_size'] * 2 if best_config['bidirectional'] else best_config['hidden_size']\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        output_size=len(tgt_vocab),\n",
    "        embedding_size=best_config['embedding_size'],\n",
    "        enc_hidden_size=enc_hidden_size,\n",
    "        dec_hidden_size=best_config['hidden_size'],\n",
    "        num_layers=best_config['num_layers'],\n",
    "        dropout=best_config['dropout'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Create full model\n",
    "    model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=best_config['teacher_forcing'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {total_params:,} parameters ({trainable_params:,} trainable)\")\n",
    "    \n",
    "    # Loss function (ignore padding token)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 3  # Early stopping patience\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(best_config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n",
    "        \n",
    "        # Train one epoch\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Char Acc: {train_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {train_metrics['exact_match_acc']:.4f}\")\n",
    "        print(f\"Val - Loss: {val_metrics['loss']:.4f}, Char Acc: {val_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {val_metrics['exact_match_acc']:.4f} ({val_metrics['correct']}/{val_metrics['total']})\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['exact_match_acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['exact_match_acc']\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_accuracy': val_metrics['exact_match_acc'],\n",
    "                'config': best_config\n",
    "            }, model_path)\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "                break\n",
    "    \n",
    "    # Load best model for testing\n",
    "    best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test evaluation\n",
    "    test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_metrics['loss']:.4f}, Char Acc: {test_metrics['char_acc']:.4f}, \"\n",
    "          f\"Exact Match: {test_metrics['exact_match_acc']:.4f} ({test_metrics['correct']}/{test_metrics['total']})\")\n",
    "    \n",
    "    # Save test results\n",
    "    results = {\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_char_acc': test_metrics['char_acc'],\n",
    "        'test_exact_match_acc': test_metrics['exact_match_acc'],\n",
    "        'test_correct': test_metrics['correct'],\n",
    "        'test_total': test_metrics['total']\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"test_results.json\"), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved test results to {os.path.join(save_dir, 'test_results.json')}\")\n",
    "    \n",
    "    # Generate and save some example translations\n",
    "    generate_examples(model, test_loader, save_dir, device, src_vocab, tgt_vocab)\n",
    "    \n",
    "    return model, test_metrics\n",
    "\n",
    "def generate_examples(model, dataloader, save_dir, device, src_vocab, tgt_vocab, num_examples=10):\n",
    "    \"\"\"Generate and save example translations to visualize model performance\"\"\"\n",
    "    model.eval()\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get one batch\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Only use a few examples\n",
    "            src = src[:num_examples]\n",
    "            tgt = tgt[:num_examples]\n",
    "            \n",
    "            # Generate translations\n",
    "            outputs, attentions = model.decode(src)\n",
    "            \n",
    "            # Convert tensors to strings\n",
    "            for i in range(min(num_examples, src.size(0))):\n",
    "                # Source text (skipping special tokens)\n",
    "                src_text = ''.join([id_to_src.get(idx.item(), '') for idx in src[i] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Target text\n",
    "                tgt_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in tgt[i, 1:] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Predicted text\n",
    "                pred_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in outputs[i, 1:] \n",
    "                                    if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                examples.append({\n",
    "                    'source': src_text,\n",
    "                    'target': tgt_text,\n",
    "                    'prediction': pred_text,\n",
    "                    'correct': pred_text == tgt_text\n",
    "                })\n",
    "            \n",
    "            break  # Only process one batch\n",
    "    \n",
    "    # Save examples\n",
    "    with open(os.path.join(save_dir, \"examples.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(examples, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(examples)} example translations to {os.path.join(save_dir, 'examples.json')}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the best model\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    \n",
    "    # Specify the directory to save the best model and results\n",
    "    save_dir = '/kaggle/working/best_model_results'\n",
    "    \n",
    "    # Check if a save directory was provided as an argument\n",
    "    if len(sys.argv) > 1:\n",
    "        save_dir = sys.argv[1]\n",
    "    \n",
    "    print(f\"Model and results will be saved to: {save_dir}\")\n",
    "    \n",
    "    # Train the model with the best hyperparameters and compute test accuracy\n",
    "    model, test_metrics = train_and_evaluate_best_model(save_dir)\n",
    "    \n",
    "    print(f\"Done! Best model saved to {save_dir}\")\n",
    "    print(f\"Test exact match accuracy: {test_metrics['exact_match_acc'] * 100:.2f}%\")\n",
    "    print(f\"Correctly predicted {test_metrics['correct']} out of {test_metrics['total']} test examples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- WandB Sweep Function for Attention Model ----\n",
    "def sweep_run_attention():\n",
    "    # Initialize WandB run\n",
    "    run = wandb.init()\n",
    "    \n",
    "    # Get hyperparameters from sweep\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Create run name\n",
    "    run_name = f\"attn-{config.cell_type}-e{config.embedding_size}-h{config.hidden_size}-n{config.num_layers}-d{config.dropout}-tf{config.teacher_forcing}-lr{config.learning_rate}-bs{config.batch_size}-{config.optim}\"\n",
    "    wandb.run.name = run_name\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data paths - use the correct paths based on your error\n",
    "    train_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'\n",
    "    dev_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
    "    test_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
    "    vocab_dir = '/kaggle/working/vocab_attention'\n",
    "    model_dir = '/kaggle/working/models_attention'\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(vocab_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Load or build vocabulary\n",
    "    vocab_file = os.path.join(vocab_dir, 'src_vocab.json')\n",
    "    if os.path.exists(vocab_file):\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'r') as f:\n",
    "            src_vocab = json.load(f)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'r') as f:\n",
    "            tgt_vocab = json.load(f)\n",
    "        print(\"Loaded existing vocabulary\")\n",
    "    else:\n",
    "        print(\"Building new vocabulary\")\n",
    "        train_dataset = DakshinaTSVDataset(train_tsv, build_vocab=True)\n",
    "        src_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n",
    "        \n",
    "        # Save vocabulary\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(src_vocab, f, ensure_ascii=False)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "        print(\"Saved vocabulary\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DakshinaTSVDataset(train_tsv, src_vocab, tgt_vocab)\n",
    "    val_dataset = DakshinaTSVDataset(dev_tsv, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
    "    \n",
    "    # Create model components\n",
    "    encoder = Encoder(\n",
    "        input_size=len(src_vocab),\n",
    "        embedding_size=config.embedding_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout,\n",
    "        bidirectional=config.bidirectional,\n",
    "        cell_type=config.cell_type\n",
    "    )\n",
    "    \n",
    "    # Calculate encoder output size (doubled if bidirectional)\n",
    "    enc_hidden_size = config.hidden_size * 2 if config.bidirectional else config.hidden_size\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        output_size=len(tgt_vocab),\n",
    "        embedding_size=config.embedding_size,\n",
    "        enc_hidden_size=enc_hidden_size,\n",
    "        dec_hidden_size=config.hidden_size,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout,\n",
    "        cell_type=config.cell_type\n",
    "    )\n",
    "    \n",
    "    # Create full model\n",
    "    model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=config.teacher_forcing)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {total_params:,} parameters ({trainable_params:,} trainable)\")\n",
    "    \n",
    "    # Loss function (ignore padding token)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Optimizer\n",
    "    if config.optim == 'nadam':\n",
    "        try:\n",
    "            optimizer = optim.NAdam(model.parameters(), lr=config.learning_rate)\n",
    "        except AttributeError:\n",
    "            print(\"NAdam optimizer not available, falling back to Adam\")\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Char Acc: {train_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {train_metrics['exact_match_acc']:.4f}\")\n",
    "        print(f\"Val - Loss: {val_metrics['loss']:.4f}, Char Acc: {val_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {val_metrics['exact_match_acc']:.4f} ({val_metrics['correct']}/{val_metrics['total']})\")\n",
    "        \n",
    "        # Convert exact match to percentage for wandb\n",
    "        val_accuracy_percent = val_metrics['exact_match_acc'] * 100\n",
    "        \n",
    "        # Log to WandB\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_char_accuracy': train_metrics['char_acc'],\n",
    "            'train_exact_match': train_metrics['exact_match_acc'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_char_accuracy': val_metrics['char_acc'],\n",
    "            'val_exact_match': val_metrics['exact_match_acc'],\n",
    "            'validation_accuracy': val_accuracy_percent  # This matches the metric name in sweep_config\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['exact_match_acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['exact_match_acc']\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(model_dir, f\"{run_name}_best.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_accuracy': val_metrics['exact_match_acc'],\n",
    "                'config': {k: v for k, v in config.__dict__.items() if not k.startswith('_')}\n",
    "            }, model_path)\n",
    "            \n",
    "            # Create a new artifact for this model\n",
    "            artifact_name = f\"attention-model-{run.id}-epoch{epoch+1}\"\n",
    "            artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
    "            artifact.add_file(model_path)\n",
    "            run.log_artifact(artifact)\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# ---- Run the sweep ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:43:00.961280Z",
     "iopub.status.busy": "2025-05-19T18:43:00.960584Z",
     "iopub.status.idle": "2025-05-19T18:43:00.991239Z",
     "shell.execute_reply": "2025-05-19T18:43:00.990611Z",
     "shell.execute_reply.started": "2025-05-19T18:43:00.961256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Import necessary classes and functions from your provided code\n",
    "# Since they're already defined in your files, I'll reference them directly\n",
    "\n",
    "def save_test_predictions_csv(model, dataloader, save_dir, device, src_vocab, tgt_vocab):\n",
    "    \"\"\"Save all test predictions to a CSV file in the required format\"\"\"\n",
    "    model.eval()\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    # Create predictions directory if it doesn't exist\n",
    "    predictions_dir = os.path.join(save_dir, 'predictions_vanilla')\n",
    "    os.makedirs(predictions_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare CSV file\n",
    "    csv_path = os.path.join(predictions_dir, 'test_predictions.csv')\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Generate translations\n",
    "            outputs, _ = model.decode(src)\n",
    "            \n",
    "            # Convert tensors to strings\n",
    "            for i in range(src.size(0)):\n",
    "                # Source text (skipping special tokens)\n",
    "                src_text = ''.join([id_to_src.get(idx.item(), '') for idx in src[i] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Target text\n",
    "                tgt_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in tgt[i, 1:] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Predicted text\n",
    "                pred_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in outputs[i, 1:] \n",
    "                                    if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Add to list\n",
    "                all_predictions.append({\n",
    "                    'Source': src_text,\n",
    "                    'Target': tgt_text,\n",
    "                    'Prediction': pred_text,\n",
    "                    'Correct': pred_text == tgt_text\n",
    "                })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"Saved all test predictions to {csv_path}\")\n",
    "    return csv_path\n",
    "\n",
    "def generate_creative_examples(model, dataloader, save_dir, device, src_vocab, tgt_vocab, num_examples=15):\n",
    "    \"\"\"Generate and save example translations with detailed visualization metrics\"\"\"\n",
    "    model.eval()\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get one batch\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Only use a few examples\n",
    "            src = src[:num_examples]\n",
    "            tgt = tgt[:num_examples]\n",
    "            \n",
    "            # Generate translations\n",
    "            outputs, attentions = model.decode(src)\n",
    "            \n",
    "            # Convert tensors to strings\n",
    "            for i in range(min(num_examples, src.size(0))):\n",
    "                # Source text (skipping special tokens)\n",
    "                src_text = ''.join([id_to_src.get(idx.item(), '') for idx in src[i] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Target text\n",
    "                tgt_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in tgt[i, 1:] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Predicted text\n",
    "                pred_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in outputs[i, 1:] \n",
    "                                    if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Calculate character-level metrics\n",
    "                correct_chars = sum(1 for a, b in zip(tgt_text, pred_text) if a == b)\n",
    "                total_chars = max(len(tgt_text), len(pred_text))\n",
    "                char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "                \n",
    "                # Character-level comparison\n",
    "                char_comparison = []\n",
    "                for j, (t_char, p_char) in enumerate(zip(tgt_text.ljust(len(pred_text)), \n",
    "                                                    pred_text.ljust(len(tgt_text)))):\n",
    "                    char_comparison.append({\n",
    "                        'position': j,\n",
    "                        'target': t_char,\n",
    "                        'prediction': p_char,\n",
    "                        'correct': t_char == p_char\n",
    "                    })\n",
    "                \n",
    "                examples.append({\n",
    "                    'source': src_text,\n",
    "                    'target': tgt_text,\n",
    "                    'prediction': pred_text,\n",
    "                    'correct': pred_text == tgt_text,\n",
    "                    'char_accuracy': char_accuracy,\n",
    "                    'char_comparison': char_comparison\n",
    "                })\n",
    "            \n",
    "            break  # Only process one batch\n",
    "    \n",
    "    # Save detailed examples\n",
    "    with open(os.path.join(save_dir, \"detailed_examples.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(examples, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Also save a more readable format for visualization\n",
    "    visual_examples = []\n",
    "    for ex in examples:\n",
    "        visual_row = {\n",
    "            'Source (Latin)': ex['source'],\n",
    "            'Target (Telugu)': ex['target'],\n",
    "            'Prediction': ex['prediction'],\n",
    "            'Status': '✓ Correct' if ex['correct'] else '✗ Incorrect',\n",
    "            'Character Accuracy': f\"{ex['char_accuracy']:.2%}\"\n",
    "        }\n",
    "        visual_examples.append(visual_row)\n",
    "    \n",
    "    # Save as CSV for easy visualization\n",
    "    visual_df = pd.DataFrame(visual_examples)\n",
    "    visual_csv_path = os.path.join(save_dir, \"visual_examples.csv\")\n",
    "    visual_df.to_csv(visual_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(examples)} detailed example translations to {os.path.join(save_dir, 'detailed_examples.json')}\")\n",
    "    print(f\"Saved visual examples to {visual_csv_path}\")\n",
    "    \n",
    "    return examples\n",
    "\n",
    "def train_and_evaluate_best_model(save_dir):\n",
    "    \"\"\"\n",
    "    Train a model with the best hyperparameters, evaluate on test set, and save the weights and predictions\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory to save model weights and results\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Best hyperparameters based on previous sweep results\n",
    "    best_config = {\n",
    "        'cell_type': 'gru',\n",
    "        'dropout': 0,\n",
    "        'embedding_size': 512,\n",
    "        'num_layers': 1,\n",
    "        'batch_size': 128,\n",
    "        'hidden_size': 512,\n",
    "        'bidirectional': True,\n",
    "        'learning_rate': 0.0005,\n",
    "        'epochs': 15,\n",
    "        'teacher_forcing': 0.7,\n",
    "        'optim': 'nadam',\n",
    "    }\n",
    "    \n",
    "    # Data paths - adjust these if needed based on your environment\n",
    "    train_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'\n",
    "    dev_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
    "    test_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
    "    \n",
    "    # Load or build vocabulary\n",
    "    vocab_dir = os.path.join(save_dir, 'vocab')\n",
    "    os.makedirs(vocab_dir, exist_ok=True)\n",
    "    \n",
    "    vocab_file = os.path.join(vocab_dir, 'src_vocab.json')\n",
    "    if os.path.exists(vocab_file):\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'r') as f:\n",
    "            src_vocab = json.load(f)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'r') as f:\n",
    "            tgt_vocab = json.load(f)\n",
    "        print(\"Loaded existing vocabulary\")\n",
    "    else:\n",
    "        print(\"Building new vocabulary\")\n",
    "        train_dataset = DakshinaTSVDataset(train_tsv, build_vocab=True)\n",
    "        src_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n",
    "        \n",
    "        # Save vocabulary\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(src_vocab, f, ensure_ascii=False)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "        print(\"Saved vocabulary\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DakshinaTSVDataset(train_tsv, src_vocab, tgt_vocab)\n",
    "    val_dataset = DakshinaTSVDataset(dev_tsv, src_vocab, tgt_vocab)\n",
    "    test_dataset = DakshinaTSVDataset(test_tsv, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=best_config['batch_size'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_config['batch_size'])\n",
    "    \n",
    "    # Create model components\n",
    "    encoder = Encoder(\n",
    "        input_size=len(src_vocab),\n",
    "        embedding_size=best_config['embedding_size'],\n",
    "        hidden_size=best_config['hidden_size'],\n",
    "        num_layers=best_config['num_layers'],\n",
    "        dropout=best_config['dropout'],\n",
    "        bidirectional=best_config['bidirectional'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Calculate encoder output size (doubled if bidirectional)\n",
    "    enc_hidden_size = best_config['hidden_size'] * 2 if best_config['bidirectional'] else best_config['hidden_size']\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        output_size=len(tgt_vocab),\n",
    "        embedding_size=best_config['embedding_size'],\n",
    "        enc_hidden_size=enc_hidden_size,\n",
    "        dec_hidden_size=best_config['hidden_size'],\n",
    "        num_layers=best_config['num_layers'],\n",
    "        dropout=best_config['dropout'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Create full model\n",
    "    model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=best_config['teacher_forcing'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {total_params:,} parameters ({trainable_params:,} trainable)\")\n",
    "    \n",
    "    # Loss function (ignore padding token)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 3  # Early stopping patience\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(best_config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n",
    "        \n",
    "        # Train one epoch\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Char Acc: {train_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {train_metrics['exact_match_acc']:.4f}\")\n",
    "        print(f\"Val - Loss: {val_metrics['loss']:.4f}, Char Acc: {val_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {val_metrics['exact_match_acc']:.4f} ({val_metrics['correct']}/{val_metrics['total']})\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['exact_match_acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['exact_match_acc']\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_accuracy': val_metrics['exact_match_acc'],\n",
    "                'config': best_config\n",
    "            }, model_path)\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "                break\n",
    "    \n",
    "    # Load best model for testing\n",
    "    best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test evaluation\n",
    "    test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_metrics['loss']:.4f}, Char Acc: {test_metrics['char_acc']:.4f}, \"\n",
    "          f\"Exact Match: {test_metrics['exact_match_acc']:.4f} ({test_metrics['correct']}/{test_metrics['total']})\")\n",
    "    \n",
    "    # Save test results\n",
    "    results = {\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_char_acc': test_metrics['char_acc'],\n",
    "        'test_exact_match_acc': test_metrics['exact_match_acc'],\n",
    "        'test_correct': test_metrics['correct'],\n",
    "        'test_total': test_metrics['total']\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"test_results.json\"), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved test results to {os.path.join(save_dir, 'test_results.json')}\")\n",
    "    \n",
    "    # Generate creative examples for visualization\n",
    "    examples = generate_creative_examples(model, test_loader, save_dir, device, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Save all test predictions to CSV\n",
    "    predictions_csv = save_test_predictions_csv(model, test_loader, save_dir, device, src_vocab, tgt_vocab)\n",
    "    \n",
    "    return model, test_metrics, examples, predictions_csv\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the best model\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    \n",
    "    # Specify the directory to save the best model and results\n",
    "    save_dir = '/kaggle/working/best_model_results'\n",
    "    \n",
    "    # Check if a save directory was provided as an argument\n",
    "    if len(sys.argv) > 1:\n",
    "        save_dir = sys.argv[1]\n",
    "    \n",
    "    print(f\"Model and results will be saved to: {save_dir}\")\n",
    "    \n",
    "    # Train the model with the best hyperparameters and compute test accuracy\n",
    "    model, test_metrics, examples, predictions_csv = train_and_evaluate_best_model(save_dir)\n",
    "    \n",
    "    print(f\"Done! Best model saved to {save_dir}\")\n",
    "    print(f\"Test exact match accuracy: {test_metrics['exact_match_acc'] * 100:.2f}%\")\n",
    "    print(f\"Correctly predicted {test_metrics['correct']} out of {test_metrics['total']} test examples\")\n",
    "    print(f\"All test predictions saved to: {predictions_csv}\")\n",
    "    \n",
    "    # Create a visualization of some sample predictions\n",
    "    print(\"\\nSample Test Predictions:\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, example in enumerate(examples[:5]):  # Show first 5 examples\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Source (Latin): {example['source']}\")\n",
    "        print(f\"  Target (Telugu): {example['target']}\")\n",
    "        print(f\"  Prediction:     {example['prediction']}\")\n",
    "        print(f\"  Status:         {'✓ Correct' if example['correct'] else '✗ Incorrect'}\")\n",
    "        print(f\"  Char Accuracy:  {example['char_accuracy']:.2%}\")\n",
    "        print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T18:43:12.504950Z",
     "iopub.status.busy": "2025-05-19T18:43:12.504656Z",
     "iopub.status.idle": "2025-05-19T19:28:04.527635Z",
     "shell.execute_reply": "2025-05-19T19:28:04.526995Z",
     "shell.execute_reply.started": "2025-05-19T18:43:12.504930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and results will be saved to: -f\n",
      "Using device: cuda\n",
      "Loaded existing vocabulary\n",
      "Loaded 58550 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkita', Native: 'అంకిత'\n",
      "  Roman: 'ankita', Native: 'అంకిత'\n",
      "  Roman: 'ankitha', Native: 'అంకిత'\n",
      "Loaded 5683 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amka', Native: 'అంక'\n",
      "  Roman: 'anka', Native: 'అంక'\n",
      "  Roman: 'amkam', Native: 'అంకం'\n",
      "Loaded 5747 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamloo', Native: 'అంకంలో'\n",
      "Model has 7,275,075 parameters (7,275,075 trainable)\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:49<00:00,  1.58it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.6749, Char Acc: 0.8180, Exact Match: 0.4080\n",
      "Val - Loss: 0.4540, Char Acc: 0.8787, Exact Match: 0.5140 (2921/5683)\n",
      "Saved new best model with validation accuracy: 0.5140\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:42<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.2951, Char Acc: 0.9231, Exact Match: 0.6333\n",
      "Val - Loss: 0.3908, Char Acc: 0.8959, Exact Match: 0.5654 (3213/5683)\n",
      "Saved new best model with validation accuracy: 0.5654\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:42<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.2220, Char Acc: 0.9432, Exact Match: 0.7090\n",
      "Val - Loss: 0.2928, Char Acc: 0.9209, Exact Match: 0.5881 (3342/5683)\n",
      "Saved new best model with validation accuracy: 0.5881\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:42<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1760, Char Acc: 0.9558, Exact Match: 0.7608\n",
      "Val - Loss: 0.3881, Char Acc: 0.9014, Exact Match: 0.5543 (3150/5683)\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:42<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1498, Char Acc: 0.9627, Exact Match: 0.7974\n",
      "Val - Loss: 0.3901, Char Acc: 0.9042, Exact Match: 0.4304 (2446/5683)\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:42<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1237, Char Acc: 0.9692, Exact Match: 0.8286\n",
      "Val - Loss: 0.3577, Char Acc: 0.9126, Exact Match: 0.6095 (3464/5683)\n",
      "Saved new best model with validation accuracy: 0.6095\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:42<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1108, Char Acc: 0.9727, Exact Match: 0.8494\n",
      "Val - Loss: 0.4302, Char Acc: 0.8993, Exact Match: 0.5613 (3190/5683)\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:43<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0932, Char Acc: 0.9774, Exact Match: 0.8504\n",
      "Val - Loss: 0.3269, Char Acc: 0.9185, Exact Match: 0.5787 (3289/5683)\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:42<00:00,  1.62it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0906, Char Acc: 0.9780, Exact Match: 0.8621\n",
      "Val - Loss: 0.3591, Char Acc: 0.9141, Exact Match: 0.6064 (3446/5683)\n",
      "Early stopping after 3 epochs without improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 45/45 [00:13<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Loss: 0.3800, Char Acc: 0.9077, Exact Match: 0.5860 (3368/5747)\n",
      "Saved test results to -f/test_results.json\n",
      "Saved 15 detailed example translations to -f/detailed_examples.json\n",
      "Saved visual examples to -f/visual_examples.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 45/45 [00:12<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all test predictions to -f/predictions_vanilla/test_predictions.csv\n",
      "Done! Best model saved to -f\n",
      "Test exact match accuracy: 58.60%\n",
      "Correctly predicted 3368 out of 5747 test examples\n",
      "All test predictions saved to: -f/predictions_vanilla/test_predictions.csv\n",
      "\n",
      "Sample Test Predictions:\n",
      "================================================================================\n",
      "Example 1:\n",
      "  Source (Latin): amkamlo\n",
      "  Target (Telugu): అంకంలో\n",
      "  Prediction:     అంకంలో\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 2:\n",
      "  Source (Latin): ankamlo\n",
      "  Target (Telugu): అంకంలో\n",
      "  Prediction:     అంకంలో\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 3:\n",
      "  Source (Latin): ankamloo\n",
      "  Target (Telugu): అంకంలో\n",
      "  Prediction:     అంకంలో\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 4:\n",
      "  Source (Latin): amkitamai\n",
      "  Target (Telugu): అంకితమై\n",
      "  Prediction:     అంకితమై\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 5:\n",
      "  Source (Latin): ankitamai\n",
      "  Target (Telugu): అంకితమై\n",
      "  Prediction:     అంకితమై\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:29:30.392019Z",
     "iopub.status.busy": "2025-05-20T08:29:30.391562Z",
     "iopub.status.idle": "2025-05-20T08:29:30.464189Z",
     "shell.execute_reply": "2025-05-20T08:29:30.463292Z",
     "shell.execute_reply.started": "2025-05-20T08:29:30.391992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/3463065894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35/1843327850.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_best_model\u001b[0;34m(save_dir)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;31m# Test evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;31m# ...existing code...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_best_model('/kaggle/working/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T08:40:20.527147Z",
     "iopub.status.busy": "2025-05-20T08:40:20.526562Z",
     "iopub.status.idle": "2025-05-20T08:45:58.285251Z",
     "shell.execute_reply": "2025-05-20T08:45:58.284604Z",
     "shell.execute_reply.started": "2025-05-20T08:40:20.527119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and results will be saved to: -f\n",
      "Using device: cuda\n",
      "Loaded existing vocabulary\n",
      "Loaded 58550 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkita', Native: 'అంకిత'\n",
      "  Roman: 'ankita', Native: 'అంకిత'\n",
      "  Roman: 'ankitha', Native: 'అంకిత'\n",
      "Loaded 5683 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amka', Native: 'అంక'\n",
      "  Roman: 'anka', Native: 'అంక'\n",
      "  Roman: 'amkam', Native: 'అంకం'\n",
      "Loaded 5747 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamloo', Native: 'అంకంలో'\n",
      "Model has 7,275,075 parameters (7,275,075 trainable)\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:50<00:00,  1.58it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.6687, Char Acc: 0.8201, Exact Match: 0.4161\n",
      "Val - Loss: 0.3810, Char Acc: 0.8958, Exact Match: 0.5149 (2926/5683)\n",
      "Saved new best model with validation accuracy: 0.5149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 45/45 [00:13<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Loss: 0.3898, Char Acc: 0.8928, Exact Match: 0.5164 (2968/5747)\n",
      "Saved test results to -f/test_results.json\n",
      "Saved 15 detailed example translations to -f/detailed_examples.json\n",
      "Saved visual examples to -f/visual_examples.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 45/45 [00:12<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all test predictions to -f/predictions_vanilla/test_predictions.csv\n",
      "\n",
      "Generating connectivity visualization...\n",
      "Downloaded Telugu font successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved connectivity visualization for example 1 to -f/connectivity/connectivity_example_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved connectivity visualization for example 2 to -f/connectivity/connectivity_example_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved connectivity visualization for example 3 to -f/connectivity/connectivity_example_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3135 (\\N{TELUGU VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3108 (\\N{TELUGU LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3118 (\\N{TELUGU LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3144 (\\N{TELUGU VOWEL SIGN AI}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3135 (\\N{TELUGU VOWEL SIGN I}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3108 (\\N{TELUGU LETTER TA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3118 (\\N{TELUGU LETTER MA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3144 (\\N{TELUGU VOWEL SIGN AI}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved connectivity visualization for example 4 to -f/connectivity/connectivity_example_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3135 (\\N{TELUGU VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3108 (\\N{TELUGU LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3118 (\\N{TELUGU LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3144 (\\N{TELUGU VOWEL SIGN AI}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3135 (\\N{TELUGU VOWEL SIGN I}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3108 (\\N{TELUGU LETTER TA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3118 (\\N{TELUGU LETTER MA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 3144 (\\N{TELUGU VOWEL SIGN AI}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:947: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved connectivity visualization for example 5 to -f/connectivity/connectivity_example_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/2098743068.py:1008: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1008: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1008: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1008: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1008: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1008: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1011: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined connectivity visualization to -f/connectivity/connectivity_combined.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 79 (O) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 117 (u) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 116 (t) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 67 (C) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 104 (h) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 97 (a) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 114 (r) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 99 (c) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 101 (e) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1071: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 79 (O) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 117 (u) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 116 (t) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 67 (C) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 104 (h) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 97 (a) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 114 (r) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 99 (c) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 101 (e) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 3077 (\\N{TELUGU LETTER A}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 3074 (\\N{TELUGU SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 3093 (\\N{TELUGU LETTER KA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 3122 (\\N{TELUGU LETTER LA}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/2098743068.py:1075: UserWarning: Glyph 3147 (\\N{TELUGU VOWEL SIGN OO}) missing from current font.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved character influence visualization to -f/connectivity/character_influence.png\n",
      "Connectivity visualizations saved to: -f/connectivity\n",
      "Done! Best model saved to -f\n",
      "Test exact match accuracy: 51.64%\n",
      "Correctly predicted 2968 out of 5747 test examples\n",
      "All test predictions saved to: -f/predictions_vanilla/test_predictions.csv\n",
      "\n",
      "Sample Test Predictions:\n",
      "================================================================================\n",
      "Example 1:\n",
      "  Source (Latin): amkamlo\n",
      "  Target (Telugu): అంకంలో\n",
      "  Prediction:     అంకంలో\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 2:\n",
      "  Source (Latin): ankamlo\n",
      "  Target (Telugu): అంకంలో\n",
      "  Prediction:     అంకంలో\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 3:\n",
      "  Source (Latin): ankamloo\n",
      "  Target (Telugu): అంకంలో\n",
      "  Prediction:     అంకంలో\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 4:\n",
      "  Source (Latin): amkitamai\n",
      "  Target (Telugu): అంకితమై\n",
      "  Prediction:     అంకితమై\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Example 5:\n",
      "  Source (Latin): ankitamai\n",
      "  Target (Telugu): అంకితమై\n",
      "  Prediction:     అంకితమై\n",
      "  Status:         ✓ Correct\n",
      "  Char Accuracy:  100.00%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "\n",
    "# ---- Dataset Definition with special tokens ----\n",
    "class DakshinaTSVDataset(Dataset):\n",
    "    def __init__(self, tsv_file, src_vocab=None, tgt_vocab=None, max_len=64, build_vocab=False):\n",
    "        df = pd.read_csv(tsv_file, sep='\\t', header=None,\n",
    "                         names=['native', 'roman', 'freq'], usecols=[0, 1], dtype=str)\n",
    "        # Fix the pandas warning by using a copy\n",
    "        df = df.copy()\n",
    "        df['native'] = df['native'].fillna('')\n",
    "        df['roman'] = df['roman'].fillna('')\n",
    "        self.pairs = list(zip(df['roman'], df['native']))\n",
    "        print(f\"Loaded {len(self.pairs)} examples from {tsv_file}\")\n",
    "        \n",
    "        # Print a few examples\n",
    "        if len(self.pairs) > 0:\n",
    "            print(\"Sample examples:\")\n",
    "            for i in range(min(3, len(self.pairs))):\n",
    "                print(f\"  Roman: '{self.pairs[i][0]}', Native: '{self.pairs[i][1]}'\")\n",
    "                \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.src_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self.tgt_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n",
    "            # Ensure special tokens exist\n",
    "            for v in ('<eos>', '<sos>'):\n",
    "                if v not in self.src_vocab: self.src_vocab[v] = len(self.src_vocab)\n",
    "                if v not in self.tgt_vocab: self.tgt_vocab[v] = len(self.tgt_vocab)\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        for src, tgt in self.pairs:\n",
    "            for ch in src:\n",
    "                if ch not in self.src_vocab: self.src_vocab[ch] = len(self.src_vocab)\n",
    "            for ch in tgt:\n",
    "                if ch not in self.tgt_vocab: self.tgt_vocab[ch] = len(self.tgt_vocab)\n",
    "        print(f\"Vocab sizes -> src: {len(self.src_vocab)}, tgt: {len(self.tgt_vocab)}\")\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        \n",
    "        # Add <sos> and <eos> tokens\n",
    "        src_idxs = [self.src_vocab['<sos>']] + [self.src_vocab.get(ch, self.src_vocab['<unk>']) for ch in src] + [self.src_vocab['<eos>']]\n",
    "        tgt_idxs = [self.tgt_vocab['<sos>']] + [self.tgt_vocab.get(ch, self.tgt_vocab['<unk>']) for ch in tgt] + [self.tgt_vocab['<eos>']]\n",
    "        \n",
    "        # Pad sequences\n",
    "        pad_src = [self.src_vocab['<pad>']] * max(0, self.max_len - len(src_idxs))\n",
    "        pad_tgt = [self.tgt_vocab['<pad>']] * max(0, self.max_len - len(tgt_idxs))\n",
    "        \n",
    "        # Truncate if necessary and convert to tensor\n",
    "        src_tensor = torch.tensor((src_idxs + pad_src)[:self.max_len], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor((tgt_idxs + pad_tgt)[:self.max_len], dtype=torch.long)\n",
    "        \n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "# ---- Encoder with bidirectional support ----\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout=0, bidirectional=True, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "            \n",
    "        # Initialize weights\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Create mask for attention\n",
    "        mask = (x != 0).float()  # 0 is <pad>\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_size]\n",
    "        \n",
    "        # Pass through RNN\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # No need to reshape hidden states - just return them as is\n",
    "        # The decoder will handle the format conversion\n",
    "        \n",
    "        # Return encoder outputs, hidden state, and mask\n",
    "        return outputs, hidden, mask\n",
    "\n",
    "# ---- Attention Mechanism ----\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super().__init__()\n",
    "        # Create a linear layer to convert the concatenated hidden states to attention scores\n",
    "        self.energy = nn.Linear(enc_hidden_size + dec_hidden_size, dec_hidden_size)\n",
    "        self.v = nn.Linear(dec_hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, dec_hidden_size]\n",
    "        # encoder_outputs: [batch_size, src_len, enc_hidden_size]\n",
    "        # mask: [batch_size, src_len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        # [batch_size, dec_hidden_size] -> [batch_size, src_len, dec_hidden_size]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Create energy by concatenating encoder outputs and decoder hidden\n",
    "        # [batch_size, src_len, enc_hidden_size + dec_hidden_size]\n",
    "        energy = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        \n",
    "        # Apply attention layer\n",
    "        # [batch_size, src_len, dec_hidden_size]\n",
    "        energy = torch.tanh(self.energy(energy))\n",
    "        \n",
    "        # Get attention scores\n",
    "        # [batch_size, src_len, 1]\n",
    "        attention = self.v(energy)\n",
    "        \n",
    "        # [batch_size, src_len]\n",
    "        attention = attention.squeeze(2)\n",
    "        \n",
    "        # Mask out padding positions\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        # [batch_size, src_len]\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "# ---- Decoder with attention and teacher forcing ----\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, enc_hidden_size, dec_hidden_size, \n",
    "                 num_layers, dropout=0, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        # Initialize attention mechanism\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        \n",
    "        # Context vector + embedding size as input to RNN\n",
    "        rnn_input_size = embedding_size + enc_hidden_size\n",
    "        \n",
    "        # Initialize RNN based on cell type\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        # Final output layer that combines decoder output, context and embedding\n",
    "        self.fc_out = nn.Linear(dec_hidden_size + enc_hidden_size + embedding_size, output_size)\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [num_layers, batch_size, dec_hidden_size] or tuple for LSTM\n",
    "        # encoder_outputs: [batch_size, src_len, enc_hidden_size]\n",
    "        # mask: [batch_size, src_len]\n",
    "        \n",
    "        # Embed input token\n",
    "        # [batch_size] -> [batch_size, 1, embedding_size]\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        \n",
    "        # Get appropriate hidden state for attention\n",
    "        if self.cell_type == 'lstm':\n",
    "            h_for_attn = hidden[0][-1]  # use last layer's hidden state\n",
    "        else:\n",
    "            h_for_attn = hidden[-1]  # use last layer's hidden state\n",
    "            \n",
    "        attn_weights = self.attention(h_for_attn, encoder_outputs, mask)\n",
    "        \n",
    "        # Create context vector by weighting encoder outputs with attention\n",
    "        # [batch_size, 1, src_len] * [batch_size, src_len, enc_hidden_size]\n",
    "        # -> [batch_size, 1, enc_hidden_size]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        # Combine embedded token and context vector\n",
    "        # [batch_size, 1, embedding_size + enc_hidden_size]\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        # output: [batch_size, 1, dec_hidden_size]\n",
    "        # hidden: [num_layers, batch_size, dec_hidden_size] or tuple for LSTM\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # Combine output, context and embedding for final prediction\n",
    "        # [batch_size, 1, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = torch.cat((output, context, embedded), dim=2)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        # [batch_size, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        # Pass through final linear layer\n",
    "        # [batch_size, output_size]\n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "# ---- Complete Seq2Seq Model with Teacher Forcing ----\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.7):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tgt: [batch_size, tgt_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len-1, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder\n",
    "        # For bidirectional LSTM, we need to initialize the decoder hidden state properly\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            # If encoder is bidirectional, we need to handle the hidden state\n",
    "            if self.encoder.bidirectional:\n",
    "                # Split the bidirectional layers\n",
    "                # hidden_state: [num_layers*2, batch_size, hidden_size]\n",
    "                # We need to reshape to [num_layers, batch_size, hidden_size*2]\n",
    "                \n",
    "                # Initialize new hidden_state\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                    self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                    self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                # For each decoder layer\n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    # If encoder has enough layers, use the corresponding layers\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        # Concatenate forward and backward directions from encoder\n",
    "                        # Forward: hidden_state[i*2]\n",
    "                        # Backward: hidden_state[i*2+1]\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        # Copy to decoder hidden state, possibly with projection if sizes don't match\n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            # Simple projection by truncation/padding\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                # Update hidden state\n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                # Similar approach as LSTM but with only one hidden state\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input to decoder is the <sos> token (already embedded in tgt)\n",
    "        input = tgt[:, 0]\n",
    "        \n",
    "        # Teacher forcing ratio determines how often to use true target as input\n",
    "        use_teacher_forcing = random.random() < self.teacher_forcing_ratio\n",
    "        \n",
    "        # Decode one token at a time\n",
    "        for t in range(1, tgt_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Store output\n",
    "            outputs[:, t-1] = output\n",
    "            \n",
    "            # Next input is either true target (teacher forcing) or predicted token\n",
    "            if use_teacher_forcing:\n",
    "                input = tgt[:, t]\n",
    "            else:\n",
    "                # Get highest scoring token\n",
    "                input = output.argmax(1)\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    # For inference (no teacher forcing)\n",
    "    def decode(self, src, max_len=100):\n",
    "        # src: [batch_size, src_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder - same as in forward method\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input is <sos> token\n",
    "        input = torch.ones(batch_size, dtype=torch.long).to(self.device) * 3  # <sos> = 3\n",
    "        \n",
    "        # Track generated tokens\n",
    "        outputs = [input]\n",
    "        attentions = []\n",
    "        \n",
    "        # Track if sequence has ended\n",
    "        ended = torch.zeros(batch_size, dtype=torch.bool).to(self.device)\n",
    "        \n",
    "        # Decode until max length or all sequences end\n",
    "        for t in range(1, max_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, attn = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Get next token\n",
    "            input = output.argmax(1)\n",
    "            \n",
    "            # Store output\n",
    "            outputs.append(input)\n",
    "            attentions.append(attn)\n",
    "            \n",
    "            # Check if all sequences have ended\n",
    "            ended = ended | (input == 2)  # 2 is <eos>\n",
    "            if ended.all():\n",
    "                break\n",
    "                \n",
    "        # Convert list of tensors to single tensor\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len]\n",
    "        attentions = torch.stack(attentions, dim=1)  # [batch_size, seq_len-1, src_len]\n",
    "        \n",
    "        return outputs, attentions\n",
    "\n",
    "# ---- Metrics & Utils ----\n",
    "def compute_exact_match_accuracy(preds, targets, tgt_vocab):\n",
    "    \"\"\"Compute exact match accuracy between predictions and targets\"\"\"\n",
    "    batch_size = preds.size(0)\n",
    "    correct = 0\n",
    "    \n",
    "    # Convert ids to strings\n",
    "    id_to_char = {v: k for k, v in tgt_vocab.items() if k not in ['<pad>', '<sos>', '<eos>', '<unk>']}\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Extract character sequences (removing special tokens)\n",
    "        pred_seq = ''.join([id_to_char.get(idx.item(), '') for idx in preds[i, 1:] \n",
    "                            if idx.item() not in [0, 1, 2, 3]])  # Skip <pad>, <unk>, <eos>, <sos>\n",
    "        \n",
    "        # For target, skip first token (<sos>) and stop at <eos> or <pad>\n",
    "        tgt_seq = ''\n",
    "        for idx in targets[i, 1:]:  # Skip first token\n",
    "            token_id = idx.item()\n",
    "            if token_id in [0, 2]:  # <pad> or <eos>\n",
    "                break\n",
    "            if token_id not in [1, 3]:  # Skip <unk> and <sos>\n",
    "                tgt_seq += id_to_char.get(token_id, '')\n",
    "        \n",
    "        # Check for exact match\n",
    "        if pred_seq == tgt_seq:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / batch_size\n",
    "\n",
    "def compute_char_accuracy(logits, targets):\n",
    "    \"\"\"Compute character-level accuracy between logits and targets\"\"\"\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    mask = (targets != 0)  # Ignore padding\n",
    "    correct = ((preds == targets) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# ---- Training & Evaluation Functions ----\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_char_acc = 0\n",
    "    epoch_exact_match_acc = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for src, tgt in tqdm(dataloader, desc=\"Training\"):\n",
    "        batch_size = src.size(0)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Flatten output and target tensors for loss calculation\n",
    "        # Ignore the first token in target (<sos>)\n",
    "        output_flat = output.reshape(-1, output.shape[-1])\n",
    "        target_flat = tgt[:, 1:].reshape(-1)  # Shift right to predict next token\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_flat, target_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        char_acc = compute_char_accuracy(output, tgt[:, 1:])\n",
    "        \n",
    "        # Decode for exact match accuracy\n",
    "        with torch.no_grad():\n",
    "            predictions, _ = model.decode(src)\n",
    "            exact_match_acc = compute_exact_match_accuracy(predictions, tgt, dataloader.dataset.tgt_vocab)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_char_acc += char_acc * batch_size\n",
    "        epoch_exact_match_acc += exact_match_acc * batch_size\n",
    "        total_batches += batch_size\n",
    "    \n",
    "    # Return average metrics\n",
    "    return {\n",
    "        'loss': epoch_loss / total_batches,\n",
    "        'char_acc': epoch_char_acc / total_batches,\n",
    "        'exact_match_acc': epoch_exact_match_acc / total_batches\n",
    "    }\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_char_acc = 0\n",
    "    epoch_exact_match_acc = 0\n",
    "    total_batches = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_size = src.size(0)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass (use teacher forcing for loss calculation)\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Flatten output and target tensors for loss calculation\n",
    "            output_flat = output.reshape(-1, output.shape[-1])\n",
    "            target_flat = tgt[:, 1:].reshape(-1)  # Shift right to predict next token\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            char_acc = compute_char_accuracy(output, tgt[:, 1:])\n",
    "            \n",
    "            # Decode for exact match accuracy (no teacher forcing)\n",
    "            predictions, _ = model.decode(src)\n",
    "            exact_match_acc = compute_exact_match_accuracy(predictions, tgt, dataloader.dataset.tgt_vocab)\n",
    "            \n",
    "            # Count exact matches for reporting\n",
    "            correct_batch = int(exact_match_acc * batch_size)\n",
    "            correct_predictions += correct_batch\n",
    "            total_predictions += batch_size\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            epoch_char_acc += char_acc * batch_size\n",
    "            epoch_exact_match_acc += exact_match_acc * batch_size\n",
    "            total_batches += batch_size\n",
    "    \n",
    "    # Return average metrics\n",
    "    return {\n",
    "        'loss': epoch_loss / total_batches,\n",
    "        'char_acc': epoch_char_acc / total_batches,\n",
    "        'exact_match_acc': epoch_exact_match_acc / total_batches,\n",
    "        'correct': correct_predictions,\n",
    "        'total': total_predictions\n",
    "    }\n",
    "\n",
    "# ---- Save Test Predictions ----\n",
    "def save_test_predictions_csv(model, dataloader, save_dir, device, src_vocab, tgt_vocab):\n",
    "    \"\"\"Save all test predictions to a CSV file in the required format\"\"\"\n",
    "    model.eval()\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    # Create predictions directory if it doesn't exist\n",
    "    predictions_dir = os.path.join(save_dir, 'predictions_vanilla')\n",
    "    os.makedirs(predictions_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare CSV file\n",
    "    csv_path = os.path.join(predictions_dir, 'test_predictions.csv')\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Generate translations\n",
    "            outputs, _ = model.decode(src)\n",
    "            \n",
    "            # Convert tensors to strings\n",
    "            for i in range(src.size(0)):\n",
    "                # Source text (skipping special tokens)\n",
    "                src_text = ''.join([id_to_src.get(idx.item(), '') for idx in src[i] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Target text\n",
    "                tgt_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in tgt[i, 1:] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Predicted text\n",
    "                pred_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in outputs[i, 1:] \n",
    "                                    if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Add to list\n",
    "                all_predictions.append({\n",
    "                    'Source': src_text,\n",
    "                    'Target': tgt_text,\n",
    "                    'Prediction': pred_text,\n",
    "                    'Correct': pred_text == tgt_text\n",
    "                })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"Saved all test predictions to {csv_path}\")\n",
    "    return csv_path\n",
    "\n",
    "def generate_creative_examples(model, dataloader, save_dir, device, src_vocab, tgt_vocab, num_examples=15):\n",
    "    \"\"\"Generate and save example translations with detailed visualization metrics\"\"\"\n",
    "    model.eval()\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get one batch\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Only use a few examples\n",
    "            src = src[:num_examples]\n",
    "            tgt = tgt[:num_examples]\n",
    "            \n",
    "            # Generate translations\n",
    "            outputs, attentions = model.decode(src)\n",
    "            \n",
    "            # Convert tensors to strings\n",
    "            for i in range(min(num_examples, src.size(0))):\n",
    "                # Source text (skipping special tokens)\n",
    "                src_text = ''.join([id_to_src.get(idx.item(), '') for idx in src[i] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Target text\n",
    "                tgt_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in tgt[i, 1:] \n",
    "                                   if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Predicted text\n",
    "                pred_text = ''.join([id_to_tgt.get(idx.item(), '') for idx in outputs[i, 1:] \n",
    "                                    if idx.item() not in [0, 1, 2, 3]])\n",
    "                \n",
    "                # Calculate character-level metrics\n",
    "                correct_chars = sum(1 for a, b in zip(tgt_text, pred_text) if a == b)\n",
    "                total_chars = max(len(tgt_text), len(pred_text))\n",
    "                char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "                \n",
    "                # Character-level comparison\n",
    "                char_comparison = []\n",
    "                for j, (t_char, p_char) in enumerate(zip(tgt_text.ljust(len(pred_text)), \n",
    "                                                    pred_text.ljust(len(tgt_text)))):\n",
    "                    char_comparison.append({\n",
    "                        'position': j,\n",
    "                        'target': t_char,\n",
    "                        'prediction': p_char,\n",
    "                        'correct': t_char == p_char\n",
    "                    })\n",
    "                \n",
    "                examples.append({\n",
    "                    'source': src_text,\n",
    "                    'target': tgt_text,\n",
    "                    'prediction': pred_text,\n",
    "                    'correct': pred_text == tgt_text,\n",
    "                    'char_accuracy': char_accuracy,\n",
    "                    'char_comparison': char_comparison\n",
    "                })\n",
    "            \n",
    "            break  # Only process one batch\n",
    "    \n",
    "    # Save detailed examples\n",
    "    with open(os.path.join(save_dir, \"detailed_examples.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(examples, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Also save a more readable format for visualization\n",
    "    visual_examples = []\n",
    "    for ex in examples:\n",
    "        visual_row = {\n",
    "            'Source (Latin)': ex['source'],\n",
    "            'Target (Telugu)': ex['target'],\n",
    "            'Prediction': ex['prediction'],\n",
    "            'Status': '✓ Correct' if ex['correct'] else '✗ Incorrect',\n",
    "            'Character Accuracy': f\"{ex['char_accuracy']:.2%}\"\n",
    "        }\n",
    "        visual_examples.append(visual_row)\n",
    "    \n",
    "    # Save as CSV for easy visualization\n",
    "    visual_df = pd.DataFrame(visual_examples)\n",
    "    visual_csv_path = os.path.join(save_dir, \"visual_examples.csv\")\n",
    "    visual_df.to_csv(visual_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(examples)} detailed example translations to {os.path.join(save_dir, 'detailed_examples.json')}\")\n",
    "    print(f\"Saved visual examples to {visual_csv_path}\")\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# ---- New Connectivity Visualization Functions ----\n",
    "\n",
    "def setup_telugu_font():\n",
    "    \"\"\"Download and set up Telugu font for visualization\"\"\"\n",
    "    font_path = '/tmp/NotoSansTelugu-Regular.ttf'\n",
    "    if not os.path.exists(font_path):\n",
    "        try:\n",
    "            import urllib.request\n",
    "            font_url = 'https://github.com/googlefonts/noto-fonts/raw/main/unhinted/ttf/NotoSansTelugu/NotoSansTelugu-Regular.ttf'\n",
    "            urllib.request.urlretrieve(font_url, font_path)\n",
    "            print(\"Downloaded Telugu font successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download Telugu font: {e}\")\n",
    "            font_path = None\n",
    "    \n",
    "    if font_path and os.path.exists(font_path):\n",
    "        try:\n",
    "            return fm.FontProperties(fname=font_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load Telugu font: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_connectivity_colormap():\n",
    "    \"\"\"Create a colormap similar to the one in the Distill article for connectivity visualization\"\"\"\n",
    "    # From transparent white to intense green\n",
    "    colors = [(1, 1, 1, 0),        # Transparent white for no connection\n",
    "              (0.9, 1, 0.9, 0.3),  # Very light green for weak connection\n",
    "              (0.7, 0.9, 0.7, 0.5), # Light green for medium connection\n",
    "              (0.4, 0.8, 0.4, 0.7), # Medium green for strong connection\n",
    "              (0.1, 0.6, 0.1, 0.9)] # Dark green for very strong connection\n",
    "    \n",
    "    return LinearSegmentedColormap.from_list('connectivity', colors, N=100)\n",
    "\n",
    "def generate_connectivity_visualization(model, dataloader, save_dir, device, src_vocab, tgt_vocab, num_examples=5):\n",
    "    \"\"\"\n",
    "    Generate connectivity visualization similar to the one in the Distill article\n",
    "    showing which input characters influence which output characters\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating connectivity visualization...\")\n",
    "    # Setup\n",
    "    connectivity_dir = os.path.join(save_dir, 'connectivity')\n",
    "    os.makedirs(connectivity_dir, exist_ok=True)\n",
    "    \n",
    "    # Create reverse mappings\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    # Setup Telugu font\n",
    "    telugu_font = setup_telugu_font()\n",
    "    \n",
    "    # Create custom colormap for connectivity visualization\n",
    "    connectivity_cmap = create_connectivity_colormap()\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get a few examples\n",
    "        for src, tgt in dataloader:\n",
    "            src = src[:num_examples].to(device)\n",
    "            tgt = tgt[:num_examples].to(device)\n",
    "            \n",
    "            # Generate translations with attention weights\n",
    "            outputs, attentions = model.decode(src)\n",
    "            \n",
    "            # Process each example\n",
    "            for i in range(src.size(0)):\n",
    "                # Extract source and target text (skip special tokens)\n",
    "                src_tokens = [id_to_src.get(idx.item(), '') for idx in src[i] \n",
    "                             if idx.item() not in [0, 1, 2, 3]]\n",
    "                \n",
    "                tgt_tokens = [id_to_tgt.get(idx.item(), '') for idx in tgt[i, 1:] \n",
    "                             if idx.item() not in [0, 1, 2, 3]]\n",
    "                \n",
    "                pred_tokens = [id_to_tgt.get(idx.item(), '') for idx in outputs[i, 1:] \n",
    "                              if idx.item() not in [0, 1, 2, 3]]\n",
    "                \n",
    "                # Join tokens to create full text strings\n",
    "                src_text = ''.join(src_tokens)\n",
    "                tgt_text = ''.join(tgt_tokens)\n",
    "                pred_text = ''.join(pred_tokens)\n",
    "                \n",
    "                # Get character-level attention matrix\n",
    "                # Start at index 1 to skip <sos> token, extract only actual token attentions\n",
    "                valid_src_indices = [j for j, idx in enumerate(src[i]) \n",
    "                                   if idx.item() not in [0, 1, 2, 3]]\n",
    "                attn_matrix = attentions[i, :len(pred_tokens), valid_src_indices].cpu().numpy()\n",
    "                \n",
    "                examples.append({\n",
    "                    'src_text': src_text,\n",
    "                    'tgt_text': tgt_text,\n",
    "                    'pred_text': pred_text,\n",
    "                    'src_tokens': src_tokens,\n",
    "                    'tgt_tokens': tgt_tokens, \n",
    "                    'pred_tokens': pred_tokens,\n",
    "                    'attn_matrix': attn_matrix,\n",
    "                    'correct': pred_text == tgt_text\n",
    "                })\n",
    "            \n",
    "            # Just process one batch\n",
    "            break\n",
    "    \n",
    "    # Generate visualizations for each example\n",
    "    for idx, example in enumerate(examples):\n",
    "        # Create figure for this example\n",
    "        fig = create_individual_connectivity_visualization(\n",
    "            example, \n",
    "            idx+1, \n",
    "            telugu_font, \n",
    "            connectivity_cmap, \n",
    "            os.path.join(connectivity_dir, f\"connectivity_example_{idx+1}.png\")\n",
    "        )\n",
    "    \n",
    "    # Create a combined visualization\n",
    "    create_combined_connectivity_visualization(\n",
    "        examples, \n",
    "        telugu_font, \n",
    "        connectivity_cmap, \n",
    "        os.path.join(connectivity_dir, \"connectivity_combined.png\")\n",
    "    )\n",
    "    \n",
    "    # Generate character influence visualization\n",
    "    generate_character_influence_visualization(\n",
    "        examples, \n",
    "        telugu_font, \n",
    "        connectivity_dir\n",
    "    )\n",
    "    \n",
    "    return examples\n",
    "\n",
    "def create_individual_connectivity_visualization(example, example_idx, telugu_font, cmap, save_path):\n",
    "    \"\"\"Create a detailed connectivity visualization for a single example\"\"\"\n",
    "    # Determine if Telugu is in source or target based on character codes\n",
    "    first_src_char = example['src_tokens'][0] if example['src_tokens'] else ''\n",
    "    is_src_telugu = False\n",
    "    if first_src_char and ord(first_src_char) > 3000:  # Telugu unicode range\n",
    "        is_src_telugu = True\n",
    "    \n",
    "    # Create figure with appropriate layout\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[1, 3])\n",
    "    \n",
    "    # Top subplot for text display\n",
    "    ax_text = plt.subplot(gs[0])\n",
    "    # Bottom subplot for attention heatmap\n",
    "    ax_heatmap = plt.subplot(gs[1])\n",
    "    \n",
    "    # Display source and prediction text\n",
    "    status = \"✓ Correct\" if example['correct'] else \"✗ Incorrect\"\n",
    "    title = f\"Example {example_idx}: {status}\\nSource: '{example['src_text']}' → Target: '{example['tgt_text']}'\\nPrediction: '{example['pred_text']}'\"\n",
    "    \n",
    "    # Display source and prediction with appropriate fonts\n",
    "    if is_src_telugu and telugu_font:\n",
    "        ax_text.text(0.05, 0.7, \"Source:\", fontsize=12)\n",
    "        ax_text.text(0.15, 0.7, example['src_text'], fontproperties=telugu_font, fontsize=12)\n",
    "        ax_text.text(0.05, 0.3, \"Prediction:\", fontsize=12)\n",
    "        ax_text.text(0.15, 0.3, example['pred_text'], fontsize=12)\n",
    "    elif not is_src_telugu and telugu_font:\n",
    "        ax_text.text(0.05, 0.7, \"Source:\", fontsize=12)\n",
    "        ax_text.text(0.15, 0.7, example['src_text'], fontsize=12)\n",
    "        ax_text.text(0.05, 0.3, \"Prediction:\", fontsize=12)\n",
    "        ax_text.text(0.15, 0.3, example['pred_text'], fontproperties=telugu_font, fontsize=12)\n",
    "    else:\n",
    "        ax_text.text(0.05, 0.7, f\"Source: {example['src_text']}\", fontsize=12)\n",
    "        ax_text.text(0.05, 0.3, f\"Prediction: {example['pred_text']}\", fontsize=12)\n",
    "    \n",
    "    # Set title and turn off axis for text display\n",
    "    ax_text.set_title(title, fontsize=14)\n",
    "    ax_text.axis('off')\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    attn = example['attn_matrix']\n",
    "    sns.heatmap(attn, cmap=cmap, ax=ax_heatmap, vmin=0, vmax=1, cbar=True)\n",
    "    \n",
    "    # Setup axis labels with appropriate fonts\n",
    "    ax_heatmap.set_xticks(np.arange(len(example['src_tokens'])))\n",
    "    ax_heatmap.set_yticks(np.arange(len(example['pred_tokens'])))\n",
    "    \n",
    "    if is_src_telugu and telugu_font:\n",
    "        ax_heatmap.set_xticklabels(example['src_tokens'], fontproperties=telugu_font, fontsize=12)\n",
    "        ax_heatmap.set_yticklabels(example['pred_tokens'], fontsize=12)\n",
    "    elif not is_src_telugu and telugu_font:\n",
    "        ax_heatmap.set_xticklabels(example['src_tokens'], fontsize=12)\n",
    "        ax_heatmap.set_yticklabels(example['pred_tokens'], fontproperties=telugu_font, fontsize=12)\n",
    "    else:\n",
    "        ax_heatmap.set_xticklabels(example['src_tokens'], fontsize=12)\n",
    "        ax_heatmap.set_yticklabels(example['pred_tokens'], fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.setp(ax_heatmap.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    \n",
    "    # Add axis labels\n",
    "    ax_heatmap.set_xlabel('Source Characters', fontsize=12)\n",
    "    ax_heatmap.set_ylabel('Generated Characters', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved connectivity visualization for example {example_idx} to {save_path}\")\n",
    "    \n",
    "    # Close figure to free memory\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_combined_connectivity_visualization(examples, telugu_font, cmap, save_path):\n",
    "    \"\"\"Create a combined visualization showing connectivity for multiple examples\"\"\"\n",
    "    # Limit examples to fit in figure\n",
    "    examples = examples[:min(3, len(examples))]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(len(examples), 1, figsize=(12, 5*len(examples)))\n",
    "    \n",
    "    # Handle case of single example\n",
    "    if len(examples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Determine if Telugu is in source or target\n",
    "    first_src_char = examples[0]['src_tokens'][0] if examples[0]['src_tokens'] else ''\n",
    "    is_src_telugu = first_src_char and ord(first_src_char) > 3000\n",
    "    \n",
    "    # Process each example\n",
    "    for i, (example, ax) in enumerate(zip(examples, axes)):\n",
    "        # Plot attention heatmap\n",
    "        attn = example['attn_matrix']\n",
    "        sns.heatmap(attn, cmap=cmap, ax=ax, vmin=0, vmax=1, cbar=True)\n",
    "        \n",
    "        # Setup axis labels\n",
    "        ax.set_xticks(np.arange(len(example['src_tokens'])))\n",
    "        ax.set_yticks(np.arange(len(example['pred_tokens'])))\n",
    "        \n",
    "        # Apply appropriate font to labels\n",
    "        if is_src_telugu and telugu_font:\n",
    "            ax.set_xticklabels(example['src_tokens'], fontproperties=telugu_font, fontsize=11)\n",
    "            ax.set_yticklabels(example['pred_tokens'], fontsize=11)\n",
    "        elif not is_src_telugu and telugu_font:\n",
    "            ax.set_xticklabels(example['src_tokens'], fontsize=11)\n",
    "            ax.set_yticklabels(example['pred_tokens'], fontproperties=telugu_font, fontsize=11)\n",
    "        else:\n",
    "            ax.set_xticklabels(example['src_tokens'], fontsize=11)\n",
    "            ax.set_yticklabels(example['pred_tokens'], fontsize=11)\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "        \n",
    "        # Add title\n",
    "        status = \"✓ Correct\" if example['correct'] else \"✗ Incorrect\"\n",
    "        ax.set_title(f\"Example {i+1}: {status}\\nSource: '{example['src_text']}' → Prediction: '{example['pred_text']}'\", \n",
    "                    fontsize=12)\n",
    "        \n",
    "        # Add axis labels\n",
    "        ax.set_xlabel('Source Characters', fontsize=10)\n",
    "        ax.set_ylabel('Generated Characters', fontsize=10)\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle(\"Character-Level Attention Connectivity Visualization\", fontsize=16, y=0.98)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved combined connectivity visualization to {save_path}\")\n",
    "    \n",
    "    # Close figure\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def generate_character_influence_visualization(examples, telugu_font, save_dir):\n",
    "    \"\"\"Generate character-by-character influence visualization similar to the Distill article\"\"\"\n",
    "    # Process the first example\n",
    "    if not examples:\n",
    "        print(\"No examples available for character influence visualization\")\n",
    "        return None\n",
    "        \n",
    "    example = examples[0]\n",
    "    \n",
    "    # Determine if Telugu is in source or target\n",
    "    first_src_char = example['src_tokens'][0] if example['src_tokens'] else ''\n",
    "    is_src_telugu = first_src_char and ord(first_src_char) > 3000\n",
    "    \n",
    "    # Create figure showing influence on each output character\n",
    "    fig, axes = plt.subplots(len(example['pred_tokens']), 1, \n",
    "                             figsize=(10, 2*len(example['pred_tokens'])))\n",
    "    \n",
    "    # Handle case of single character\n",
    "    if len(example['pred_tokens']) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Process each output character\n",
    "    for i, (char, ax) in enumerate(zip(example['pred_tokens'], axes)):\n",
    "        # Get attention weights for this character\n",
    "        weights = example['attn_matrix'][i]\n",
    "        \n",
    "        # Plot horizontal bars showing influence\n",
    "        bars = ax.barh(range(len(example['src_tokens'])), weights, color='green', alpha=0.7)\n",
    "        \n",
    "        # Set y-ticks and labels\n",
    "        ax.set_yticks(range(len(example['src_tokens'])))\n",
    "        \n",
    "        # Apply appropriate font\n",
    "        if is_src_telugu and telugu_font:\n",
    "            ax.set_yticklabels(example['src_tokens'], fontproperties=telugu_font, fontsize=12)\n",
    "            ax.set_title(f\"Output Character: '{char}'\", fontsize=12)\n",
    "        elif not is_src_telugu and telugu_font:\n",
    "            ax.set_yticklabels(example['src_tokens'], fontsize=12)\n",
    "            ax.set_title(f\"Output Character: '{char}'\", fontproperties=telugu_font, fontsize=12)\n",
    "        else:\n",
    "            ax.set_yticklabels(example['src_tokens'], fontsize=12)\n",
    "            ax.set_title(f\"Output Character: '{char}'\", fontsize=12)\n",
    "        \n",
    "        # Set x-axis limits and label\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_xlabel('Attention Weight', fontsize=10)\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle(f\"Character-by-Character Attention Analysis\\nSource: '{example['src_text']}' → Prediction: '{example['pred_text']}'\", \n",
    "                 fontsize=14, y=0.98)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = os.path.join(save_dir, \"character_influence.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved character influence visualization to {save_path}\")\n",
    "    \n",
    "    # Close figure\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# ---- Train and Evaluate Best Model (Modified with Connectivity Visualization) ----\n",
    "def train_and_evaluate_best_model(save_dir):\n",
    "    \"\"\"\n",
    "    Train a model with the best hyperparameters, evaluate on test set, and save the weights and predictions\n",
    "    \n",
    "    Args:\n",
    "        save_dir: Directory to save model weights and results\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Best hyperparameters based on previous sweep results\n",
    "    best_config = {\n",
    "        'cell_type': 'gru',\n",
    "        'dropout': 0,\n",
    "        'embedding_size': 512,\n",
    "        'num_layers': 1,\n",
    "        'batch_size': 128,\n",
    "        'hidden_size': 512,\n",
    "        'bidirectional': True,\n",
    "        'learning_rate': 0.0005,\n",
    "        'epochs': 1,\n",
    "        'teacher_forcing': 0.7,\n",
    "        'optim': 'nadam',\n",
    "    }\n",
    "    \n",
    "    # Data paths - adjust these if needed based on your environment\n",
    "    train_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'\n",
    "    dev_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
    "    test_tsv = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
    "    \n",
    "    # Load or build vocabulary\n",
    "    vocab_dir = os.path.join(save_dir, 'vocab')\n",
    "    os.makedirs(vocab_dir, exist_ok=True)\n",
    "    \n",
    "    vocab_file = os.path.join(vocab_dir, 'src_vocab.json')\n",
    "    if os.path.exists(vocab_file):\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'r') as f:\n",
    "            src_vocab = json.load(f)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'r') as f:\n",
    "            tgt_vocab = json.load(f)\n",
    "        print(\"Loaded existing vocabulary\")\n",
    "    else:\n",
    "        print(\"Building new vocabulary\")\n",
    "        train_dataset = DakshinaTSVDataset(train_tsv, build_vocab=True)\n",
    "        src_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n",
    "        \n",
    "        # Save vocabulary\n",
    "        with open(os.path.join(vocab_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(src_vocab, f, ensure_ascii=False)\n",
    "        with open(os.path.join(vocab_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "        print(\"Saved vocabulary\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DakshinaTSVDataset(train_tsv, src_vocab, tgt_vocab)\n",
    "    val_dataset = DakshinaTSVDataset(dev_tsv, src_vocab, tgt_vocab)\n",
    "    test_dataset = DakshinaTSVDataset(test_tsv, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=best_config['batch_size'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_config['batch_size'])\n",
    "    \n",
    "    # Create model components\n",
    "    encoder = Encoder(\n",
    "        input_size=len(src_vocab),\n",
    "        embedding_size=best_config['embedding_size'],\n",
    "        hidden_size=best_config['hidden_size'],\n",
    "        num_layers=best_config['num_layers'],\n",
    "        dropout=best_config['dropout'],\n",
    "        bidirectional=best_config['bidirectional'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Calculate encoder output size (doubled if bidirectional)\n",
    "    enc_hidden_size = best_config['hidden_size'] * 2 if best_config['bidirectional'] else best_config['hidden_size']\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        output_size=len(tgt_vocab),\n",
    "        embedding_size=best_config['embedding_size'],\n",
    "        enc_hidden_size=enc_hidden_size,\n",
    "        dec_hidden_size=best_config['hidden_size'],\n",
    "        num_layers=best_config['num_layers'],\n",
    "        dropout=best_config['dropout'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Create full model\n",
    "    model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=best_config['teacher_forcing'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {total_params:,} parameters ({trainable_params:,} trainable)\")\n",
    "    \n",
    "    # Loss function (ignore padding token)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 3  # Early stopping patience\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(best_config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{best_config['epochs']}\")\n",
    "        \n",
    "        # Train one epoch\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Char Acc: {train_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {train_metrics['exact_match_acc']:.4f}\")\n",
    "        print(f\"Val - Loss: {val_metrics['loss']:.4f}, Char Acc: {val_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {val_metrics['exact_match_acc']:.4f} ({val_metrics['correct']}/{val_metrics['total']})\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['exact_match_acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['exact_match_acc']\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_accuracy': val_metrics['exact_match_acc'],\n",
    "                'config': best_config\n",
    "            }, model_path)\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "                break\n",
    "    \n",
    "    # Load best model for testing\n",
    "    best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test evaluation\n",
    "    test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_metrics['loss']:.4f}, Char Acc: {test_metrics['char_acc']:.4f}, \"\n",
    "          f\"Exact Match: {test_metrics['exact_match_acc']:.4f} ({test_metrics['correct']}/{test_metrics['total']})\")\n",
    "    \n",
    "    # Save test results\n",
    "    results = {\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_char_acc': test_metrics['char_acc'],\n",
    "        'test_exact_match_acc': test_metrics['exact_match_acc'],\n",
    "        'test_correct': test_metrics['correct'],\n",
    "        'test_total': test_metrics['total']\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"test_results.json\"), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved test results to {os.path.join(save_dir, 'test_results.json')}\")\n",
    "    \n",
    "    # Generate creative examples for visualization\n",
    "    examples = generate_creative_examples(model, test_loader, save_dir, device, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Save all test predictions to CSV\n",
    "    predictions_csv = save_test_predictions_csv(model, test_loader, save_dir, device, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # NEW: Generate connectivity visualizations\n",
    "    connectivity_examples = generate_connectivity_visualization(\n",
    "        model, test_loader, save_dir, device, src_vocab, tgt_vocab, num_examples=5\n",
    "    )\n",
    "    \n",
    "    print(f\"Connectivity visualizations saved to: {os.path.join(save_dir, 'connectivity')}\")\n",
    "    \n",
    "    return model, test_metrics, examples, predictions_csv\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the best model\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    \n",
    "    # Specify the directory to save the best model and results\n",
    "    save_dir = '/kaggle/working/best_model_results'\n",
    "    \n",
    "    # Check if a save directory was provided as an argument\n",
    "    if len(sys.argv) > 1:\n",
    "        save_dir = sys.argv[1]\n",
    "    \n",
    "    print(f\"Model and results will be saved to: {save_dir}\")\n",
    "    \n",
    "    # Train the model with the best hyperparameters and compute test accuracy\n",
    "    model, test_metrics, examples, predictions_csv = train_and_evaluate_best_model(save_dir)\n",
    "    \n",
    "    print(f\"Done! Best model saved to {save_dir}\")\n",
    "    print(f\"Test exact match accuracy: {test_metrics['exact_match_acc'] * 100:.2f}%\")\n",
    "    print(f\"Correctly predicted {test_metrics['correct']} out of {test_metrics['total']} test examples\")\n",
    "    print(f\"All test predictions saved to: {predictions_csv}\")\n",
    "    \n",
    "    # Create a visualization of some sample predictions\n",
    "    print(\"\\nSample Test Predictions:\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, example in enumerate(examples[:5]):  # Show first 5 examples\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Source (Latin): {example['source']}\")\n",
    "        print(f\"  Target (Telugu): {example['target']}\")\n",
    "        print(f\"  Prediction:     {example['prediction']}\")\n",
    "        print(f\"  Status:         {'✓ Correct' if example['correct'] else '✗ Incorrect'}\")\n",
    "        print(f\"  Char Accuracy:  {example['char_accuracy']:.2%}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# If this script is run directly, call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T10:55:34.157653Z",
     "iopub.status.busy": "2025-05-20T10:55:34.156908Z",
     "iopub.status.idle": "2025-05-20T10:55:43.762259Z",
     "shell.execute_reply": "2025-05-20T10:55:43.761724Z",
     "shell.execute_reply.started": "2025-05-20T10:55:34.157627Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model_comparison_tables</strong> at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/djlaayb3' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/djlaayb3</a><br> View project at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_104208-djlaayb3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_105534-zewfp197</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/zewfp197' target=\"_blank\">model_comparison_tables</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/zewfp197' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/zewfp197</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention DataFrame dtypes: Source        object\n",
      "Target        object\n",
      "Prediction    object\n",
      "Correct         bool\n",
      "dtype: object\n",
      "Normal DataFrame dtypes: Roman        float64\n",
      "Expected      object\n",
      "Predicted     object\n",
      "Correct         bool\n",
      "dtype: object\n",
      "Created table 'Attention_Correct' with 3368 examples\n",
      "Created table 'Attention_Incorrect' with 2379 examples\n",
      "Created table 'Normal_Correct' with 3030 examples\n",
      "Created table 'Normal_Incorrect' with 2717 examples\n",
      "Attempting to merge dataframes...\n",
      "Attention Source dtype: object\n",
      "Normal Source dtype: object\n",
      "Created table 'Attention_Better_Than_Normal' with 0 examples\n",
      "Created summary statistics table\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model_comparison_tables</strong> at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/zewfp197' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/zewfp197</a><br> View project at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_105534-zewfp197/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "def create_wandb_tables(attention_csv_path, normal_csv_path, project_name=\"dakshina-transliteration\"):\n",
    "    \"\"\"\n",
    "    Create WandB tables showcasing model results\n",
    "    \n",
    "    Args:\n",
    "        attention_csv_path: Path to CSV with attention model predictions\n",
    "        normal_csv_path: Path to CSV with normal model predictions\n",
    "        project_name: WandB project name\n",
    "    \"\"\"\n",
    "    # Initialize wandb run\n",
    "    run = wandb.init(project=project_name, name=\"model_comparison_tables\")\n",
    "    \n",
    "    # Load prediction CSVs\n",
    "    attention_df = pd.read_csv(attention_csv_path)\n",
    "    normal_df = pd.read_csv(normal_csv_path)\n",
    "    \n",
    "    # Print column datatypes to debug\n",
    "    print(\"Attention DataFrame dtypes:\", attention_df.dtypes)\n",
    "    print(\"Normal DataFrame dtypes:\", normal_df.dtypes)\n",
    "    \n",
    "    # Map actual column names to expected column names\n",
    "    column_mapping = {\n",
    "        'Roman': 'Source',\n",
    "        'Expected': 'Target',\n",
    "        'Predicted': 'Prediction'\n",
    "    }\n",
    "    \n",
    "    # Rename columns for both DataFrames\n",
    "    attention_df = attention_df.rename(columns=column_mapping)\n",
    "    normal_df = normal_df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Make sure both DataFrames have the same columns\n",
    "    required_columns = ['Source', 'Target', 'Prediction', 'Correct']\n",
    "    for df in [attention_df, normal_df]:\n",
    "        assert all(col in df.columns for col in required_columns), f\"Missing required columns. Expected: {required_columns}, Got: {df.columns}\"\n",
    "    \n",
    "    # Convert key columns to string type to ensure consistent types for merging\n",
    "    attention_df['Source'] = attention_df['Source'].astype(str)\n",
    "    attention_df['Target'] = attention_df['Target'].astype(str)\n",
    "    normal_df['Source'] = normal_df['Source'].astype(str)\n",
    "    normal_df['Target'] = normal_df['Target'].astype(str)\n",
    "    \n",
    "    # 1. Create a table for Attention Model - Correct Predictions\n",
    "    attention_correct = attention_df[attention_df['Correct'] == True].copy()\n",
    "    attention_correct_table = wandb.Table(dataframe=attention_correct[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Attention_Correct\": attention_correct_table})\n",
    "    print(f\"Created table 'Attention_Correct' with {len(attention_correct)} examples\")\n",
    "    \n",
    "    # 2. Create a table for Attention Model - Incorrect Predictions\n",
    "    attention_incorrect = attention_df[attention_df['Correct'] == False].copy()\n",
    "    attention_incorrect_table = wandb.Table(dataframe=attention_incorrect[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Attention_Incorrect\": attention_incorrect_table})\n",
    "    print(f\"Created table 'Attention_Incorrect' with {len(attention_incorrect)} examples\")\n",
    "    \n",
    "    # 3. Create a table for Normal Model - Correct Predictions\n",
    "    normal_correct = normal_df[normal_df['Correct'] == True].copy()\n",
    "    normal_correct_table = wandb.Table(dataframe=normal_correct[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Normal_Correct\": normal_correct_table})\n",
    "    print(f\"Created table 'Normal_Correct' with {len(normal_correct)} examples\")\n",
    "    \n",
    "    # 4. Create a table for Normal Model - Incorrect Predictions\n",
    "    normal_incorrect = normal_df[normal_df['Correct'] == False].copy()\n",
    "    normal_incorrect_table = wandb.Table(dataframe=normal_incorrect[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Normal_Incorrect\": normal_incorrect_table})\n",
    "    print(f\"Created table 'Normal_Incorrect' with {len(normal_incorrect)} examples\")\n",
    "    \n",
    "    # 5. Create a table showing where Attention is correct but Normal model is wrong\n",
    "    # First, merge the dataframes on Source and Target\n",
    "    try:\n",
    "        print(\"Attempting to merge dataframes...\")\n",
    "        print(\"Attention Source dtype:\", attention_df['Source'].dtype)\n",
    "        print(\"Normal Source dtype:\", normal_df['Source'].dtype)\n",
    "        \n",
    "        merged_df = pd.merge(\n",
    "            attention_df[['Source', 'Target', 'Prediction', 'Correct']],\n",
    "            normal_df[['Source', 'Target', 'Prediction', 'Correct']],\n",
    "            on=['Source', 'Target'],\n",
    "            suffixes=('_attention', '_normal')\n",
    "        )\n",
    "        \n",
    "        # Filter to where attention is correct but normal is wrong\n",
    "        attention_wins = merged_df[\n",
    "            (merged_df['Correct_attention'] == True) & \n",
    "            (merged_df['Correct_normal'] == False)\n",
    "        ].copy()\n",
    "        \n",
    "        # Create a table with these examples\n",
    "        attention_wins_table = wandb.Table(\n",
    "            columns=['Source', 'Target', 'Attention_Prediction', 'Normal_Prediction'],\n",
    "            data=attention_wins[['Source', 'Target', 'Prediction_attention', 'Prediction_normal']].values\n",
    "        )\n",
    "        wandb.log({\"Attention_Better_Than_Normal\": attention_wins_table})\n",
    "        print(f\"Created table 'Attention_Better_Than_Normal' with {len(attention_wins)} examples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging dataframes: {e}\")\n",
    "        print(\"Looking at sample data to debug:\")\n",
    "        print(\"Attention DF head:\")\n",
    "        print(attention_df.head())\n",
    "        print(\"Normal DF head:\")\n",
    "        print(normal_df.head())\n",
    "        print(\"Skipping the creation of 'Attention_Better_Than_Normal' table\")\n",
    "    \n",
    "    # 6. Summary statistics table\n",
    "    stats = {\n",
    "        'Model': ['Attention', 'Normal'],\n",
    "        'Accuracy': [\n",
    "            len(attention_correct) / len(attention_df) * 100,\n",
    "            len(normal_correct) / len(normal_df) * 100\n",
    "        ],\n",
    "        'Correct_Count': [len(attention_correct), len(normal_correct)],\n",
    "        'Incorrect_Count': [len(attention_incorrect), len(normal_incorrect)]\n",
    "    }\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_table = wandb.Table(dataframe=stats_df)\n",
    "    wandb.log({\"Model_Comparison_Stats\": stats_table})\n",
    "    print(f\"Created summary statistics table\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the function with paths to your CSV files\n",
    "    create_wandb_tables(\n",
    "        attention_csv_path=\"/kaggle/input/predictions-models/test_predictions_attention.csv\",\n",
    "        normal_csv_path=\"/kaggle/input/predictions-models/test_predictions_vanilla.csv\",\n",
    "        project_name=\"dakshina-transliteration\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T11:12:12.927634Z",
     "iopub.status.busy": "2025-05-20T11:12:12.927253Z",
     "iopub.status.idle": "2025-05-20T11:12:24.384287Z",
     "shell.execute_reply": "2025-05-20T11:12:24.383678Z",
     "shell.execute_reply.started": "2025-05-20T11:12:12.927613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_111212-hauqs6rj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hauqs6rj' target=\"_blank\">model_comparison_tables</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hauqs6rj' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hauqs6rj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in attention dataframe: Source        0\n",
      "Target        0\n",
      "Prediction    0\n",
      "Correct       0\n",
      "dtype: int64\n",
      "NaN values in normal dataframe: Roman        5747\n",
      "Expected        0\n",
      "Predicted       0\n",
      "Correct         0\n",
      "dtype: int64\n",
      "\n",
      "Sample from attention_df:\n",
      "      Source   Target Prediction  Correct\n",
      "0    amkamlo   అంకంలో     అంకంలో     True\n",
      "1    ankamlo   అంకంలో     అంకంలో     True\n",
      "2   ankamloo   అంకంలో     అంకంలో     True\n",
      "3  amkitamai  అంకితమై    అంకితమై     True\n",
      "4  ankitamai  అంకితమై    అంకితమై     True\n",
      "\n",
      "Sample from normal_df:\n",
      "           Source   Target Prediction  Correct\n",
      "0  unknown_source   అంకంలో     అంకంలో     True\n",
      "1  unknown_source   అంకంలో     అంకంలో     True\n",
      "2  unknown_source   అంకంలో     అంకంలో     True\n",
      "3  unknown_source  అంకితమై    అంకితమై     True\n",
      "4  unknown_source  అంకితమై    అంకితమై     True\n",
      "Created table 'Attention_Correct' with 3368 examples\n",
      "Created table 'Attention_Incorrect' with 2379 examples\n",
      "Created table 'Normal_Correct' with 3030 examples\n",
      "Created table 'Normal_Incorrect' with 2717 examples\n",
      "\n",
      "*** Analyzing model differences ***\n",
      "Found 3368 examples where attention model is better\n",
      "No matching examples found in both datasets\n",
      "Created summary statistics table\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model_comparison_tables</strong> at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hauqs6rj' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hauqs6rj</a><br> View project at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 5 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_111212-hauqs6rj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_wandb_tables(attention_csv_path, normal_csv_path, project_name=\"dakshina-transliteration\"):\n",
    "    \"\"\"\n",
    "    Create WandB tables showcasing model results\n",
    "    \n",
    "    Args:\n",
    "        attention_csv_path: Path to CSV with attention model predictions\n",
    "        normal_csv_path: Path to CSV with normal model predictions\n",
    "        project_name: WandB project name\n",
    "    \"\"\"\n",
    "    # Initialize wandb run\n",
    "    run = wandb.init(project=project_name, name=\"model_comparison_tables\")\n",
    "    \n",
    "    # Load prediction CSVs\n",
    "    attention_df = pd.read_csv(attention_csv_path)\n",
    "    normal_df = pd.read_csv(normal_csv_path)\n",
    "    \n",
    "    # Debug information about NaN values\n",
    "    print(\"NaN values in attention dataframe:\", attention_df.isna().sum())\n",
    "    print(\"NaN values in normal dataframe:\", normal_df.isna().sum())\n",
    "    \n",
    "    # Map actual column names to expected column names\n",
    "    column_mapping = {\n",
    "        'Roman': 'Source',\n",
    "        'Expected': 'Target',\n",
    "        'Predicted': 'Prediction'\n",
    "    }\n",
    "    \n",
    "    # Rename columns for both DataFrames\n",
    "    attention_df = attention_df.rename(columns=column_mapping)\n",
    "    normal_df = normal_df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Make sure both DataFrames have the same columns\n",
    "    required_columns = ['Source', 'Target', 'Prediction', 'Correct']\n",
    "    for df in [attention_df, normal_df]:\n",
    "        assert all(col in df.columns for col in required_columns), f\"Missing required columns. Expected: {required_columns}, Got: {df.columns}\"\n",
    "    \n",
    "    # *** Handle NaN values in Source and Target columns ***\n",
    "    # Fill NaN values with a placeholder or another meaningful value\n",
    "    attention_df['Source'] = attention_df['Source'].fillna(\"unknown_source\")\n",
    "    attention_df['Target'] = attention_df['Target'].fillna(\"unknown_target\")\n",
    "    normal_df['Source'] = normal_df['Source'].fillna(\"unknown_source\")\n",
    "    normal_df['Target'] = normal_df['Target'].fillna(\"unknown_target\")\n",
    "    \n",
    "    # Convert key columns to string type to ensure consistent types for merging\n",
    "    attention_df['Source'] = attention_df['Source'].astype(str)\n",
    "    attention_df['Target'] = attention_df['Target'].astype(str)\n",
    "    normal_df['Source'] = normal_df['Source'].astype(str)\n",
    "    normal_df['Target'] = normal_df['Target'].astype(str)\n",
    "    \n",
    "    # Print some samples to verify\n",
    "    print(\"\\nSample from attention_df:\")\n",
    "    print(attention_df[['Source', 'Target', 'Prediction', 'Correct']].head())\n",
    "    print(\"\\nSample from normal_df:\")\n",
    "    print(normal_df[['Source', 'Target', 'Prediction', 'Correct']].head())\n",
    "    \n",
    "    # 1. Create a table for Attention Model - Correct Predictions\n",
    "    attention_correct = attention_df[attention_df['Correct'] == True].copy()\n",
    "    attention_correct_table = wandb.Table(dataframe=attention_correct[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Attention_Correct\": attention_correct_table})\n",
    "    print(f\"Created table 'Attention_Correct' with {len(attention_correct)} examples\")\n",
    "    \n",
    "    # 2. Create a table for Attention Model - Incorrect Predictions\n",
    "    attention_incorrect = attention_df[attention_df['Correct'] == False].copy()\n",
    "    attention_incorrect_table = wandb.Table(dataframe=attention_incorrect[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Attention_Incorrect\": attention_incorrect_table})\n",
    "    print(f\"Created table 'Attention_Incorrect' with {len(attention_incorrect)} examples\")\n",
    "    \n",
    "    # 3. Create a table for Normal Model - Correct Predictions\n",
    "    normal_correct = normal_df[normal_df['Correct'] == True].copy()\n",
    "    normal_correct_table = wandb.Table(dataframe=normal_correct[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Normal_Correct\": normal_correct_table})\n",
    "    print(f\"Created table 'Normal_Correct' with {len(normal_correct)} examples\")\n",
    "    \n",
    "    # 4. Create a table for Normal Model - Incorrect Predictions\n",
    "    normal_incorrect = normal_df[normal_df['Correct'] == False].copy()\n",
    "    normal_incorrect_table = wandb.Table(dataframe=normal_incorrect[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Normal_Incorrect\": normal_incorrect_table})\n",
    "    print(f\"Created table 'Normal_Incorrect' with {len(normal_incorrect)} examples\")\n",
    "    \n",
    "    # 5. Create a table showing where Attention is correct but Normal model is wrong\n",
    "    print(\"\\n*** Analyzing model differences ***\")\n",
    "    \n",
    "    # Alternative approach: Create a common identifier for each example\n",
    "    # This will help merge dataframes even with different formatting\n",
    "    attention_df['example_id'] = attention_df['Source'] + '|' + attention_df['Target']\n",
    "    normal_df['example_id'] = normal_df['Source'] + '|' + normal_df['Target']\n",
    "    \n",
    "    # Get sets of correctly predicted examples for each model\n",
    "    attention_correct_ids = set(attention_df[attention_df['Correct'] == True]['example_id'])\n",
    "    normal_correct_ids = set(normal_df[normal_df['Correct'] == True]['example_id'])\n",
    "    \n",
    "    # Find examples where attention is correct but normal is wrong\n",
    "    attention_better_ids = attention_correct_ids - normal_correct_ids\n",
    "    print(f\"Found {len(attention_better_ids)} examples where attention model is better\")\n",
    "    \n",
    "    # If we have examples, create a table\n",
    "    if len(attention_better_ids) > 0:\n",
    "        # Filter attention dataframe for these examples\n",
    "        attention_better_df = attention_df[attention_df['example_id'].isin(attention_better_ids)].copy()\n",
    "        \n",
    "        # For each better example, find the normal model's prediction\n",
    "        results = []\n",
    "        for _, row in attention_better_df.iterrows():\n",
    "            # Find matching row in normal_df\n",
    "            normal_row = normal_df[normal_df['example_id'] == row['example_id']]\n",
    "            if len(normal_row) > 0:\n",
    "                results.append([\n",
    "                    row['Source'],\n",
    "                    row['Target'],\n",
    "                    row['Prediction'],\n",
    "                    normal_row.iloc[0]['Prediction']\n",
    "                ])\n",
    "        \n",
    "        # Create a table with these examples\n",
    "        if len(results) > 0:\n",
    "            attention_wins_table = wandb.Table(\n",
    "                columns=['Source', 'Target', 'Attention_Prediction', 'Normal_Prediction'],\n",
    "                data=results\n",
    "            )\n",
    "            wandb.log({\"Attention_Better_Than_Normal\": attention_wins_table})\n",
    "            print(f\"Created table 'Attention_Better_Than_Normal' with {len(results)} examples\")\n",
    "        else:\n",
    "            print(\"No matching examples found in both datasets\")\n",
    "    else:\n",
    "        print(\"No examples where attention model is better than normal model\")\n",
    "    \n",
    "    # 6. Summary statistics table\n",
    "    stats = {\n",
    "        'Model': ['Attention', 'Normal'],\n",
    "        'Accuracy': [\n",
    "            len(attention_correct) / len(attention_df) * 100,\n",
    "            len(normal_correct) / len(normal_df) * 100\n",
    "        ],\n",
    "        'Correct_Count': [len(attention_correct), len(normal_correct)],\n",
    "        'Incorrect_Count': [len(attention_incorrect), len(normal_incorrect)],\n",
    "        'Total_Examples': [len(attention_df), len(normal_df)]\n",
    "    }\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_table = wandb.Table(dataframe=stats_df)\n",
    "    wandb.log({\"Model_Comparison_Stats\": stats_table})\n",
    "    print(f\"Created summary statistics table\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the function with paths to your CSV files\n",
    "    create_wandb_tables(\n",
    "        attention_csv_path=\"/kaggle/input/predictions-models/test_predictions_attention.csv\",\n",
    "        normal_csv_path=\"/kaggle/input/predictions-models/test_predictions_vanilla.csv\",\n",
    "        project_name=\"dakshina-transliteration\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T11:17:22.548763Z",
     "iopub.status.busy": "2025-05-20T11:17:22.548151Z",
     "iopub.status.idle": "2025-05-20T11:17:32.374146Z",
     "shell.execute_reply": "2025-05-20T11:17:32.373465Z",
     "shell.execute_reply.started": "2025-05-20T11:17:22.548741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (5747, 3)\n",
      "Test data sample:\n",
      "    Target      Roman  Index\n",
      "0   అంకంలో    amkamlo      1\n",
      "1   అంకంలో    ankamlo      2\n",
      "2   అంకంలో   ankamloo      1\n",
      "3  అంకితమై  amkitamai      1\n",
      "4  అంకితమై  ankitamai      2\n",
      "Vanilla predictions shape: (5747, 4)\n",
      "Vanilla predictions sample:\n",
      "   Roman Expected Predicted  Correct\n",
      "0    NaN   అంకంలో    అంకంలో     True\n",
      "1    NaN   అంకంలో    అంకంలో     True\n",
      "2    NaN   అంకంలో    అంకంలో     True\n",
      "3    NaN  అంకితమై   అంకితమై     True\n",
      "4    NaN  అంకితమై   అంకితమై     True\n",
      "Saved complete vanilla predictions to /kaggle/working/complete_test_predictions_vanilla.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_111722-9ni92pa8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/9ni92pa8' target=\"_blank\">model_comparison_tables</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/9ni92pa8' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/9ni92pa8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in attention dataframe: Source        0\n",
      "Target        0\n",
      "Prediction    0\n",
      "Correct       0\n",
      "dtype: int64\n",
      "NaN values in normal dataframe: Roman        0\n",
      "Expected     0\n",
      "Predicted    0\n",
      "Correct      0\n",
      "dtype: int64\n",
      "\n",
      "Sample from attention_df:\n",
      "      Source   Target Prediction  Correct\n",
      "0    amkamlo   అంకంలో     అంకంలో     True\n",
      "1    ankamlo   అంకంలో     అంకంలో     True\n",
      "2   ankamloo   అంకంలో     అంకంలో     True\n",
      "3  amkitamai  అంకితమై    అంకితమై     True\n",
      "4  ankitamai  అంకితమై    అంకితమై     True\n",
      "\n",
      "Sample from normal_df:\n",
      "      Source   Target Prediction  Correct\n",
      "0    amkamlo   అంకంలో     అంకంలో     True\n",
      "1    ankamlo   అంకంలో     అంకంలో     True\n",
      "2   ankamloo   అంకంలో     అంకంలో     True\n",
      "3  amkitamai  అంకితమై    అంకితమై     True\n",
      "4  ankitamai  అంకితమై    అంకితమై     True\n",
      "Created table 'Attention_Correct' with 3368 examples\n",
      "Created table 'Attention_Incorrect' with 2379 examples\n",
      "Created table 'Normal_Correct' with 3030 examples\n",
      "Created table 'Normal_Incorrect' with 2717 examples\n",
      "\n",
      "*** Analyzing model differences ***\n",
      "Successfully merged dataframes with 5747 rows\n",
      "Created table 'Attention_Better_Than_Normal' with 852 examples\n",
      "Created summary statistics table\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model_comparison_tables</strong> at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/9ni92pa8' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/9ni92pa8</a><br> View project at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_111722-9ni92pa8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# First, load and merge the data to create a complete vanilla predictions CSV\n",
    "def create_complete_vanilla_csv():\n",
    "    # Load the test data TSV file\n",
    "    test_data_path = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\"\n",
    "    test_df = pd.read_csv(test_data_path, sep='\\t', header=None)\n",
    "    \n",
    "    # Name the columns appropriately\n",
    "    test_df.columns = ['Target', 'Roman', 'Index']\n",
    "    \n",
    "    # Print info about the test data\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    print(\"Test data sample:\")\n",
    "    print(test_df.head())\n",
    "    \n",
    "    # Load the vanilla predictions CSV\n",
    "    vanilla_pred_path = \"/kaggle/input/predictions-models/test_predictions_vanilla.csv\"\n",
    "    vanilla_df = pd.read_csv(vanilla_pred_path)\n",
    "    \n",
    "    # Print info about the vanilla predictions\n",
    "    print(f\"Vanilla predictions shape: {vanilla_df.shape}\")\n",
    "    print(\"Vanilla predictions sample:\")\n",
    "    print(vanilla_df.head())\n",
    "    \n",
    "    # Check if the number of rows match\n",
    "    if test_df.shape[0] != vanilla_df.shape[0]:\n",
    "        print(f\"WARNING: Row counts don't match! Test data: {test_df.shape[0]}, Vanilla predictions: {vanilla_df.shape[0]}\")\n",
    "    \n",
    "    # Merge based on the index/order assuming they are in the same order\n",
    "    # Creating a new dataframe with all required columns\n",
    "    complete_vanilla_df = pd.DataFrame({\n",
    "        'Roman': test_df['Roman'],\n",
    "        'Expected': vanilla_df['Expected'],\n",
    "        'Predicted': vanilla_df['Predicted'],\n",
    "        'Correct': vanilla_df['Correct']\n",
    "    })\n",
    "    \n",
    "    # Save the complete vanilla predictions to the working directory\n",
    "    output_path = \"/kaggle/working/complete_test_predictions_vanilla.csv\"\n",
    "    complete_vanilla_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved complete vanilla predictions to {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def create_wandb_tables(attention_csv_path, normal_csv_path, project_name=\"dakshina-transliteration\"):\n",
    "    \"\"\"\n",
    "    Create WandB tables showcasing model results\n",
    "    \n",
    "    Args:\n",
    "        attention_csv_path: Path to CSV with attention model predictions\n",
    "        normal_csv_path: Path to CSV with normal model predictions\n",
    "        project_name: WandB project name\n",
    "    \"\"\"\n",
    "    # Initialize wandb run\n",
    "    run = wandb.init(project=project_name, name=\"model_comparison_tables\")\n",
    "    \n",
    "    # Load prediction CSVs\n",
    "    attention_df = pd.read_csv(attention_csv_path)\n",
    "    normal_df = pd.read_csv(normal_csv_path)\n",
    "    \n",
    "    # Debug information about NaN values\n",
    "    print(\"NaN values in attention dataframe:\", attention_df.isna().sum())\n",
    "    print(\"NaN values in normal dataframe:\", normal_df.isna().sum())\n",
    "    \n",
    "    # Map actual column names to expected column names\n",
    "    column_mapping = {\n",
    "        'Roman': 'Source',\n",
    "        'Expected': 'Target',\n",
    "        'Predicted': 'Prediction'\n",
    "    }\n",
    "    \n",
    "    # Rename columns for both DataFrames\n",
    "    attention_df = attention_df.rename(columns=column_mapping)\n",
    "    normal_df = normal_df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Make sure both DataFrames have the same columns\n",
    "    required_columns = ['Source', 'Target', 'Prediction', 'Correct']\n",
    "    for df in [attention_df, normal_df]:\n",
    "        assert all(col in df.columns for col in required_columns), f\"Missing required columns. Expected: {required_columns}, Got: {df.columns}\"\n",
    "    \n",
    "    # Handle NaN values in Source and Target columns\n",
    "    attention_df['Source'] = attention_df['Source'].fillna(\"unknown_source\")\n",
    "    attention_df['Target'] = attention_df['Target'].fillna(\"unknown_target\")\n",
    "    normal_df['Source'] = normal_df['Source'].fillna(\"unknown_source\")\n",
    "    normal_df['Target'] = normal_df['Target'].fillna(\"unknown_target\")\n",
    "    \n",
    "    # Convert key columns to string type to ensure consistent types for merging\n",
    "    attention_df['Source'] = attention_df['Source'].astype(str)\n",
    "    attention_df['Target'] = attention_df['Target'].astype(str)\n",
    "    normal_df['Source'] = normal_df['Source'].astype(str)\n",
    "    normal_df['Target'] = normal_df['Target'].astype(str)\n",
    "    \n",
    "    # Print some samples to verify\n",
    "    print(\"\\nSample from attention_df:\")\n",
    "    print(attention_df[['Source', 'Target', 'Prediction', 'Correct']].head())\n",
    "    print(\"\\nSample from normal_df:\")\n",
    "    print(normal_df[['Source', 'Target', 'Prediction', 'Correct']].head())\n",
    "    \n",
    "    # 1. Create a table for Attention Model - Correct Predictions\n",
    "    attention_correct = attention_df[attention_df['Correct'] == True].copy()\n",
    "    attention_correct_table = wandb.Table(dataframe=attention_correct[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Attention_Correct\": attention_correct_table})\n",
    "    print(f\"Created table 'Attention_Correct' with {len(attention_correct)} examples\")\n",
    "    \n",
    "    # 2. Create a table for Attention Model - Incorrect Predictions\n",
    "    attention_incorrect = attention_df[attention_df['Correct'] == False].copy()\n",
    "    attention_incorrect_table = wandb.Table(dataframe=attention_incorrect[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Attention_Incorrect\": attention_incorrect_table})\n",
    "    print(f\"Created table 'Attention_Incorrect' with {len(attention_incorrect)} examples\")\n",
    "    \n",
    "    # 3. Create a table for Normal Model - Correct Predictions\n",
    "    normal_correct = normal_df[normal_df['Correct'] == True].copy()\n",
    "    normal_correct_table = wandb.Table(dataframe=normal_correct[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Normal_Correct\": normal_correct_table})\n",
    "    print(f\"Created table 'Normal_Correct' with {len(normal_correct)} examples\")\n",
    "    \n",
    "    # 4. Create a table for Normal Model - Incorrect Predictions\n",
    "    normal_incorrect = normal_df[normal_df['Correct'] == False].copy()\n",
    "    normal_incorrect_table = wandb.Table(dataframe=normal_incorrect[['Source', 'Target', 'Prediction']])\n",
    "    wandb.log({\"Normal_Incorrect\": normal_incorrect_table})\n",
    "    print(f\"Created table 'Normal_Incorrect' with {len(normal_incorrect)} examples\")\n",
    "    \n",
    "    # 5. Create a table showing where Attention is correct but Normal model is wrong\n",
    "    print(\"\\n*** Analyzing model differences ***\")\n",
    "    \n",
    "    # First, merge the dataframes on Source and Target\n",
    "    try:\n",
    "        merged_df = pd.merge(\n",
    "            attention_df[['Source', 'Target', 'Prediction', 'Correct']],\n",
    "            normal_df[['Source', 'Target', 'Prediction', 'Correct']],\n",
    "            on=['Source', 'Target'],\n",
    "            suffixes=('_attention', '_normal')\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully merged dataframes with {len(merged_df)} rows\")\n",
    "        \n",
    "        # Filter to where attention is correct but normal is wrong\n",
    "        attention_wins = merged_df[\n",
    "            (merged_df['Correct_attention'] == True) & \n",
    "            (merged_df['Correct_normal'] == False)\n",
    "        ].copy()\n",
    "        \n",
    "        # Create a table with these examples\n",
    "        attention_wins_table = wandb.Table(\n",
    "            columns=['Source', 'Target', 'Attention_Prediction', 'Normal_Prediction'],\n",
    "            data=attention_wins[['Source', 'Target', 'Prediction_attention', 'Prediction_normal']].values\n",
    "        )\n",
    "        wandb.log({\"Attention_Better_Than_Normal\": attention_wins_table})\n",
    "        print(f\"Created table 'Attention_Better_Than_Normal' with {len(attention_wins)} examples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging dataframes: {e}\")\n",
    "        print(\"Looking at sample data to debug:\")\n",
    "        print(\"Attention DF head:\")\n",
    "        print(attention_df.head())\n",
    "        print(\"Normal DF head:\")\n",
    "        print(normal_df.head())\n",
    "        \n",
    "        # Alternative approach with common ID\n",
    "        print(\"\\nTrying alternative approach with common ID...\")\n",
    "        attention_df['example_id'] = attention_df['Source'] + '|' + attention_df['Target']\n",
    "        normal_df['example_id'] = normal_df['Source'] + '|' + normal_df['Target']\n",
    "        \n",
    "        # Get sets of correctly predicted examples for each model\n",
    "        attention_correct_ids = set(attention_df[attention_df['Correct'] == True]['example_id'])\n",
    "        normal_correct_ids = set(normal_df[normal_df['Correct'] == True]['example_id'])\n",
    "        \n",
    "        # Find examples where attention is correct but normal is wrong\n",
    "        attention_better_ids = attention_correct_ids - normal_correct_ids\n",
    "        print(f\"Found {len(attention_better_ids)} examples where attention model is better\")\n",
    "        \n",
    "        # If we have examples, create a table\n",
    "        if len(attention_better_ids) > 0:\n",
    "            # Filter attention dataframe for these examples\n",
    "            attention_better_df = attention_df[attention_df['example_id'].isin(attention_better_ids)].copy()\n",
    "            \n",
    "            # For each better example, find the normal model's prediction\n",
    "            results = []\n",
    "            for _, row in attention_better_df.iterrows():\n",
    "                # Find matching row in normal_df\n",
    "                normal_row = normal_df[normal_df['example_id'] == row['example_id']]\n",
    "                if len(normal_row) > 0:\n",
    "                    results.append([\n",
    "                        row['Source'],\n",
    "                        row['Target'],\n",
    "                        row['Prediction'],\n",
    "                        normal_row.iloc[0]['Prediction']\n",
    "                    ])\n",
    "            \n",
    "            # Create a table with these examples\n",
    "            if len(results) > 0:\n",
    "                attention_wins_table = wandb.Table(\n",
    "                    columns=['Source', 'Target', 'Attention_Prediction', 'Normal_Prediction'],\n",
    "                    data=results\n",
    "                )\n",
    "                wandb.log({\"Attention_Better_Than_Normal\": attention_wins_table})\n",
    "                print(f\"Created table 'Attention_Better_Than_Normal' with {len(results)} examples\")\n",
    "            else:\n",
    "                print(\"No matching examples found in both datasets\")\n",
    "        else:\n",
    "            print(\"No examples where attention model is better than normal model\")\n",
    "    \n",
    "    # 6. Summary statistics table\n",
    "    stats = {\n",
    "        'Model': ['Attention', 'Normal'],\n",
    "        'Accuracy': [\n",
    "            len(attention_correct) / len(attention_df) * 100,\n",
    "            len(normal_correct) / len(normal_df) * 100\n",
    "        ],\n",
    "        'Correct_Count': [len(attention_correct), len(normal_correct)],\n",
    "        'Incorrect_Count': [len(attention_incorrect), len(normal_incorrect)],\n",
    "        'Total_Examples': [len(attention_df), len(normal_df)]\n",
    "    }\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_table = wandb.Table(dataframe=stats_df)\n",
    "    wandb.log({\"Model_Comparison_Stats\": stats_table})\n",
    "    print(f\"Created summary statistics table\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # First create the complete vanilla predictions CSV\n",
    "    complete_vanilla_csv_path = create_complete_vanilla_csv()\n",
    "    \n",
    "    # Then use this complete CSV for creating the tables\n",
    "    create_wandb_tables(\n",
    "        attention_csv_path=\"/kaggle/input/predictions-models/test_predictions_attention.csv\",\n",
    "        normal_csv_path=complete_vanilla_csv_path,\n",
    "        project_name=\"dakshina-transliteration\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T11:28:43.115961Z",
     "iopub.status.busy": "2025-05-20T11:28:43.115683Z",
     "iopub.status.idle": "2025-05-20T11:28:43.121892Z",
     "shell.execute_reply": "2025-05-20T11:28:43.120986Z",
     "shell.execute_reply.started": "2025-05-20T11:28:43.115940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the best model\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    \n",
    "    # Specify the directory to save the best model and results\n",
    "    save_dir = '/kaggle/working/best_model_results'\n",
    "    \n",
    "    # Check if a save directory was provided as an argument\n",
    "    if len(sys.argv) > 1:\n",
    "        save_dir = sys.argv[1]\n",
    "    \n",
    "    print(f\"Model and results will be saved to: {save_dir}\")\n",
    "    \n",
    "    # Train the model with the best hyperparameters and compute test accuracy\n",
    "    model, test_metrics, examples, predictions_csv, attn_examples = train_and_evaluate_best_model(save_dir)\n",
    "    \n",
    "    # Generate the connectivity visualizations\n",
    "    connectivity_examples, connectivity_fig = create_connectivity_visualization(\n",
    "        model, test_loader, save_dir, device, src_vocab, tgt_vocab, num_examples=4\n",
    "    )\n",
    "    \n",
    "    # Generate the enhanced connectivity visualization for a more detailed view\n",
    "    enhanced_examples, enhanced_fig = create_enhanced_connectivity_visualization(\n",
    "        model, test_loader, save_dir, device, src_vocab, tgt_vocab, num_examples=3\n",
    "    )\n",
    "    \n",
    "    print(f\"Done! Best model saved to {save_dir}\")\n",
    "    print(f\"Test exact match accuracy: {test_metrics['exact_match_acc'] * 100:.2f}%\")\n",
    "    print(f\"Correctly predicted {test_metrics['correct']} out of {test_metrics['total']} test examples\")\n",
    "    print(f\"All test predictions saved to: {predictions_csv}\")\n",
    "    print(f\"Attention visualizations saved to: {os.path.join(save_dir, 'attention_visualization.png')}\")\n",
    "    print(f\"Connectivity visualizations saved to: {os.path.join(save_dir, 'connectivity_visualization.png')}\")\n",
    "    print(f\"Enhanced connectivity visualization saved to: {os.path.join(save_dir, 'enhanced_connectivity_visualization.png')}\")\n",
    "    \n",
    "    # Rest of your code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T11:28:51.127522Z",
     "iopub.status.busy": "2025-05-20T11:28:51.126826Z",
     "iopub.status.idle": "2025-05-20T11:34:16.602478Z",
     "shell.execute_reply": "2025-05-20T11:34:16.601499Z",
     "shell.execute_reply.started": "2025-05-20T11:28:51.127496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and results will be saved to: -f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_112851-mkj66mk3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/seq2seq_transliteration/runs/mkj66mk3' target=\"_blank\">attention_visualization_run</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/seq2seq_transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/seq2seq_transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/seq2seq_transliteration/runs/mkj66mk3' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/seq2seq_transliteration/runs/mkj66mk3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Building new vocabulary\n",
      "Loaded 58550 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkita', Native: 'అంకిత'\n",
      "  Roman: 'ankita', Native: 'అంకిత'\n",
      "  Roman: 'ankitha', Native: 'అంకిత'\n",
      "Vocab sizes -> src: 30, tgt: 67\n",
      "Saved vocabulary\n",
      "Loaded 58550 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkita', Native: 'అంకిత'\n",
      "  Roman: 'ankita', Native: 'అంకిత'\n",
      "  Roman: 'ankitha', Native: 'అంకిత'\n",
      "Loaded 5683 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amka', Native: 'అంక'\n",
      "  Roman: 'anka', Native: 'అంక'\n",
      "  Roman: 'amkam', Native: 'అంకం'\n",
      "Loaded 5747 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamloo', Native: 'అంకంలో'\n",
      "Model has 7,275,075 parameters (7,275,075 trainable)\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:51<00:00,  1.57it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.6441, Char Acc: 0.8273, Exact Match: 0.4087\n",
      "Val - Loss: 0.3537, Char Acc: 0.9001, Exact Match: 0.5173 (2940/5683)\n",
      "Saved new best model with validation accuracy: 0.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 45/45 [00:13<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Loss: 0.4016, Char Acc: 0.8908, Exact Match: 0.5156 (2962/5747)\n",
      "Saved test results to -f/test_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_creative_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/451043146.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35/4030199754.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Train the model with the best hyperparameters and compute test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Generate the connectivity visualizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/4043400625.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_best_model\u001b[0;34m(save_dir)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Generate creative examples for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_creative_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Save all test predictions to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_creative_examples' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:35:42.703273Z",
     "iopub.status.busy": "2025-05-20T15:35:42.702665Z",
     "iopub.status.idle": "2025-05-20T15:35:42.742103Z",
     "shell.execute_reply": "2025-05-20T15:35:42.741414Z",
     "shell.execute_reply.started": "2025-05-20T15:35:42.703232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import matplotlib.font_manager as fm\n",
    "import wandb\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def setup_telugu_font():\n",
    "    \"\"\"Download and set up Telugu font for visualization\"\"\"\n",
    "    font_path = '/tmp/NotoSansTelugu-Regular.ttf'\n",
    "    if not os.path.exists(font_path):\n",
    "        try:\n",
    "            import urllib.request\n",
    "            font_url = 'https://github.com/googlefonts/noto-fonts/raw/main/unhinted/ttf/NotoSansTelugu/NotoSansTelugu-Regular.ttf'\n",
    "            urllib.request.urlretrieve(font_url, font_path)\n",
    "            print(\"Downloaded Telugu font successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download Telugu font: {e}\")\n",
    "            font_path = None\n",
    "    \n",
    "    if font_path and os.path.exists(font_path):\n",
    "        try:\n",
    "            telugu_font = fm.FontProperties(fname=font_path)\n",
    "            return telugu_font\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load Telugu font: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_attention_cmap():\n",
    "    \"\"\"Create a colormap for attention visualization (yellow to blue)\"\"\"\n",
    "    colors = [\n",
    "        (1.0, 1.0, 0.8),  # Light yellow\n",
    "        (0.9, 1.0, 0.8),  # Pale yellow\n",
    "        (0.8, 1.0, 0.8),  # Yellow-green\n",
    "        (0.6, 0.9, 0.9),  # Pale blue\n",
    "        (0.4, 0.7, 1.0),  # Light blue\n",
    "        (0.2, 0.4, 0.9),  # Blue\n",
    "    ]\n",
    "    return mcolors.LinearSegmentedColormap.from_list('attention_cmap', colors, N=256)\n",
    "\n",
    "def get_colored_text_html(text: str, attention_weights: np.ndarray, cmap) -> str:\n",
    "    \"\"\"\n",
    "    Generate HTML for text with color highlighting based on attention weights\n",
    "    \n",
    "    Args:\n",
    "        text: The input text\n",
    "        attention_weights: Attention weights for each character\n",
    "        cmap: Colormap to use for highlighting\n",
    "    \n",
    "    Returns:\n",
    "        HTML string with colored text\n",
    "    \"\"\"\n",
    "    # Pad or truncate attention weights to match text length\n",
    "    if len(attention_weights) < len(text):\n",
    "        attention_weights = np.pad(attention_weights, (0, len(text) - len(attention_weights)), \n",
    "                                  mode='constant', constant_values=0)\n",
    "    elif len(attention_weights) > len(text):\n",
    "        attention_weights = attention_weights[:len(text)]\n",
    "    \n",
    "    # Generate HTML with colored spans\n",
    "    html = \"\"\n",
    "    for i, char in enumerate(text):\n",
    "        weight = attention_weights[i]\n",
    "        color = mcolors.rgb2hex(cmap(weight)[:3])  # Convert to hex color\n",
    "        html += f'<span style=\"background-color: {color}; padding: 2px;\">{char}</span>'\n",
    "    \n",
    "    return html\n",
    "\n",
    "def generate_character_attention_visualizations(\n",
    "    model, \n",
    "    dataloader, \n",
    "    save_dir: str,\n",
    "    device, \n",
    "    src_vocab: dict, \n",
    "    tgt_vocab: dict, \n",
    "    num_examples: int = 3,\n",
    "    top_k_predictions: int = 5,\n",
    "    interactive: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate visualizations showing which input characters the model attends to\n",
    "    when predicting each output character.\n",
    "    \n",
    "    Args:\n",
    "        model: The sequence-to-sequence model\n",
    "        dataloader: DataLoader containing test examples\n",
    "        save_dir: Directory to save visualizations\n",
    "        device: Device to run model on\n",
    "        src_vocab: Source vocabulary\n",
    "        tgt_vocab: Target vocabulary\n",
    "        num_examples: Number of examples to visualize\n",
    "        top_k_predictions: Number of top predictions to show\n",
    "        interactive: Whether to create interactive widgets (for notebook)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of visualization data\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating character-level attention visualizations...\")\n",
    "    \n",
    "    # Setup Telugu font\n",
    "    telugu_font = setup_telugu_font()\n",
    "    \n",
    "    # Create reverse vocabulary mappings\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    # Create colormap for attention visualization\n",
    "    cmap = create_attention_cmap()\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Store examples with attention data\n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get batch of examples\n",
    "        for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "            if batch_idx > 3:  # Limit to first few batches\n",
    "                break\n",
    "                \n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Generate translations with attention\n",
    "            outputs, attentions = model.decode(src)\n",
    "            \n",
    "            # Get model predictions for each position\n",
    "            # Shape: [batch_size, tgt_len, vocab_size]\n",
    "            predictions = model.decoder.get_all_predictions(src, outputs)\n",
    "            \n",
    "            # Process each example\n",
    "            for i in range(min(num_examples, src.size(0))):\n",
    "                # Extract source characters (skipping special tokens)\n",
    "                src_chars = []\n",
    "                for idx in src[i]:\n",
    "                    if idx.item() not in [0, 1, 2, 3]:  # Skip special tokens\n",
    "                        src_chars.append(id_to_src.get(idx.item(), '<unk>'))\n",
    "                \n",
    "                # Extract target characters\n",
    "                tgt_chars = []\n",
    "                for idx in tgt[i, 1:]:  # Skip <sos>\n",
    "                    if idx.item() not in [0, 1, 2, 3]:  # Skip special tokens\n",
    "                        tgt_chars.append(id_to_tgt.get(idx.item(), '<unk>'))\n",
    "                \n",
    "                # Extract prediction characters\n",
    "                pred_chars = []\n",
    "                for idx in outputs[i, 1:]:  # Skip <sos>\n",
    "                    if idx.item() not in [0, 1, 2, 3]:  # Skip special tokens\n",
    "                        pred_chars.append(id_to_tgt.get(idx.item(), '<unk>'))\n",
    "                \n",
    "                # Get attention matrix\n",
    "                # Shape: [tgt_len, src_len]\n",
    "                attn = attentions[i, :len(pred_chars), 1:len(src_chars)+1].cpu().numpy()\n",
    "                \n",
    "                # Get top-k predictions for each position\n",
    "                # predictions[i] shape: [tgt_len, vocab_size]\n",
    "                top_k_preds = []\n",
    "                for pos in range(min(len(pred_chars), predictions.size(1)-1)):\n",
    "                    # Get logits for this position\n",
    "                    logits = predictions[i, pos+1, :].cpu()\n",
    "                    \n",
    "                    # Get top-k indices and probabilities\n",
    "                    probs = torch.softmax(logits, dim=0)\n",
    "                    top_k_values, top_k_indices = torch.topk(probs, k=top_k_predictions)\n",
    "                    \n",
    "                    # Convert to characters\n",
    "                    pred_chars_at_pos = [id_to_tgt.get(idx.item(), '<unk>') for idx in top_k_indices]\n",
    "                    pred_probs_at_pos = top_k_values.numpy().tolist()\n",
    "                    \n",
    "                    top_k_preds.append({\n",
    "                        'position': pos,\n",
    "                        'chars': pred_chars_at_pos,\n",
    "                        'probs': pred_probs_at_pos\n",
    "                    })\n",
    "                \n",
    "                # Full text strings\n",
    "                src_text = ''.join(src_chars)\n",
    "                tgt_text = ''.join(tgt_chars)\n",
    "                pred_text = ''.join(pred_chars)\n",
    "                \n",
    "                # Store example\n",
    "                examples.append({\n",
    "                    'src_text': src_text,\n",
    "                    'tgt_text': tgt_text,\n",
    "                    'pred_text': pred_text,\n",
    "                    'src_chars': src_chars,\n",
    "                    'tgt_chars': tgt_chars,\n",
    "                    'pred_chars': pred_chars,\n",
    "                    'attn_matrix': attn.tolist(),  # Convert to list for JSON serialization\n",
    "                    'top_k_predictions': top_k_preds,\n",
    "                    'correct': pred_text == tgt_text\n",
    "                })\n",
    "            \n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "    \n",
    "    # Save examples as JSON\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(os.path.join(save_dir, 'attention_examples.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(examples, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(examples)} examples to {os.path.join(save_dir, 'attention_examples.json')}\")\n",
    "    \n",
    "    # Function to create visualization for a specific example and character position\n",
    "    def create_visualization(example_idx, char_pos):\n",
    "        example = examples[example_idx]\n",
    "        \n",
    "        # Get attention weights for this position\n",
    "        if char_pos < len(example['attn_matrix']):\n",
    "            attention_weights = np.array(example['attn_matrix'][char_pos])\n",
    "        else:\n",
    "            attention_weights = np.zeros(len(example['src_chars']))\n",
    "        \n",
    "        # Get top-k predictions for this position\n",
    "        if char_pos < len(example['top_k_predictions']):\n",
    "            top_k = example['top_k_predictions'][char_pos]\n",
    "            pred_chars = top_k['chars']\n",
    "            pred_probs = top_k['probs']\n",
    "        else:\n",
    "            pred_chars = ['<eos>'] * top_k_predictions\n",
    "            pred_probs = [0.0] * top_k_predictions\n",
    "        \n",
    "        # Create figure\n",
    "        fig = Figure(figsize=(10, 6))\n",
    "        canvas = FigureCanvas(fig)\n",
    "        \n",
    "        # Add title\n",
    "        fig.suptitle(f\"Input: {example['src_text']}\\nPredictions:\", fontsize=14)\n",
    "        \n",
    "        # Generate HTML for colored input text\n",
    "        colored_text_html = get_colored_text_html(example['src_text'], attention_weights, cmap)\n",
    "        \n",
    "        # Create table for visualization\n",
    "        table_data = []\n",
    "        for i, (char, prob) in enumerate(zip(pred_chars, pred_probs)):\n",
    "            table_data.append([\n",
    "                f\"character at index {char_pos} of {char}\",\n",
    "                colored_text_html\n",
    "            ])\n",
    "        \n",
    "        # Create table\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create table\n",
    "        table = ax.table(\n",
    "            cellText=table_data,\n",
    "            colLabels=[\"Character in Prediction Focused\", \"Attention Visualization\"],\n",
    "            cellLoc='center',\n",
    "            loc='center'\n",
    "        )\n",
    "        \n",
    "        # Set table properties\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 1.5)\n",
    "        \n",
    "        # Adjust cell widths\n",
    "        table.auto_set_column_width([0, 1])\n",
    "        \n",
    "        # Save figure\n",
    "        fig_path = os.path.join(save_dir, f'example_{example_idx}_char_{char_pos}.png')\n",
    "        fig.savefig(fig_path, bbox_inches='tight', dpi=150)\n",
    "        \n",
    "        return fig, fig_path\n",
    "    \n",
    "    # Create visualizations for each example, focus on a few character positions\n",
    "    all_visualizations = []\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        # Choose a few interesting positions\n",
    "        pos_to_visualize = []\n",
    "        \n",
    "        # Include beginning, middle, and end positions\n",
    "        if len(example['pred_chars']) > 0:\n",
    "            pos_to_visualize.append(0)  # First character\n",
    "        \n",
    "        if len(example['pred_chars']) > 2:\n",
    "            pos_to_visualize.append(len(example['pred_chars']) // 2)  # Middle character\n",
    "        \n",
    "        if len(example['pred_chars']) > 1:\n",
    "            pos_to_visualize.append(len(example['pred_chars']) - 1)  # Last character\n",
    "        \n",
    "        # Create visualizations for selected positions\n",
    "        example_visualizations = []\n",
    "        for pos in pos_to_visualize:\n",
    "            fig, fig_path = create_visualization(i, pos)\n",
    "            \n",
    "            # Add to list\n",
    "            example_visualizations.append({\n",
    "                'position': pos,\n",
    "                'path': fig_path\n",
    "            })\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                f\"example_{i}_position_{pos}\": wandb.Image(\n",
    "                    fig, \n",
    "                    caption=f\"Example {i}: Character {pos} attention\"\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            # Close figure to free memory\n",
    "            plt.close(fig)\n",
    "        \n",
    "        all_visualizations.append({\n",
    "            'example_idx': i,\n",
    "            'visualizations': example_visualizations\n",
    "        })\n",
    "    \n",
    "    # Create a summary visualization with all examples\n",
    "    summary_fig = plt.figure(figsize=(15, 5 * len(examples)))\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        plt.subplot(len(examples), 1, i + 1)\n",
    "        plt.title(f\"Example {i}: {example['src_text']} → {example['pred_text']}\")\n",
    "        \n",
    "        # Plot attention heatmap\n",
    "        attn_matrix = np.array(example['attn_matrix'])\n",
    "        plt.imshow(attn_matrix, cmap=cmap, aspect='auto')\n",
    "        \n",
    "        # Set axis labels\n",
    "        plt.yticks(range(len(example['pred_chars'])), example['pred_chars'])\n",
    "        plt.xticks(range(len(example['src_chars'])), example['src_chars'])\n",
    "        \n",
    "        plt.xlabel('Input Characters')\n",
    "        plt.ylabel('Output Characters')\n",
    "        \n",
    "        plt.colorbar(label='Attention Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save summary visualization\n",
    "    summary_path = os.path.join(save_dir, 'attention_summary.png')\n",
    "    plt.savefig(summary_path, bbox_inches='tight', dpi=150)\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        \"attention_summary\": wandb.Image(\n",
    "            summary_fig, \n",
    "            caption=\"Summary of attention patterns\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    plt.close(summary_fig)\n",
    "    print(f\"Saved summary visualization to {summary_path}\")\n",
    "    \n",
    "    # If in a notebook environment and interactive is True, create widgets\n",
    "    if interactive:\n",
    "        try:\n",
    "            # Create widgets for interactive exploration\n",
    "            example_dropdown = widgets.Dropdown(\n",
    "                options=[(f\"Example {i}: {ex['src_text']} → {ex['pred_text']}\", i) \n",
    "                        for i, ex in enumerate(examples)],\n",
    "                description='Example:',\n",
    "                disabled=False,\n",
    "            )\n",
    "            \n",
    "            # Create slider for character position\n",
    "            position_slider = widgets.IntSlider(\n",
    "                value=0,\n",
    "                min=0,\n",
    "                max=max(len(ex['pred_chars']) - 1 for ex in examples),\n",
    "                step=1,\n",
    "                description='Character:',\n",
    "                disabled=False,\n",
    "                continuous_update=False,\n",
    "            )\n",
    "            \n",
    "            # Function to update output when widgets change\n",
    "            def update_output(example_idx, char_pos):\n",
    "                # Get example\n",
    "                example = examples[example_idx]\n",
    "                \n",
    "                # Adjust max position based on selected example\n",
    "                position_slider.max = max(0, len(example['pred_chars']) - 1)\n",
    "                \n",
    "                # Ensure char_pos is within bounds\n",
    "                char_pos = min(char_pos, position_slider.max)\n",
    "                \n",
    "                # Get attention weights for this position\n",
    "                if char_pos < len(example['attn_matrix']):\n",
    "                    attention_weights = np.array(example['attn_matrix'][char_pos])\n",
    "                else:\n",
    "                    attention_weights = np.zeros(len(example['src_chars']))\n",
    "                \n",
    "                # Get top-k predictions for this position\n",
    "                if char_pos < len(example['top_k_predictions']):\n",
    "                    top_k = example['top_k_predictions'][char_pos]\n",
    "                    pred_chars = top_k['chars']\n",
    "                    pred_probs = top_k['probs']\n",
    "                else:\n",
    "                    pred_chars = ['<eos>'] * top_k_predictions\n",
    "                    pred_probs = [0.0] * top_k_predictions\n",
    "                \n",
    "                # Generate colored text HTML\n",
    "                colored_text = get_colored_text_html(example['src_text'], attention_weights, cmap)\n",
    "                \n",
    "                # Create HTML table with predictions and colored text\n",
    "                table_html = f\"\"\"\n",
    "                <h3>INPUT: {example['src_text']}</h3>\n",
    "                <h4>Top {top_k_predictions} predictions:</h4>\n",
    "                <ul>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Add predictions\n",
    "                for char, prob in zip(pred_chars, pred_probs):\n",
    "                    table_html += f\"<li>{char} ({prob:.4f})</li>\"\n",
    "                \n",
    "                table_html += f\"\"\"\n",
    "                </ul>\n",
    "                <h4>Visualizing attention for character at position {char_pos}:</h4>\n",
    "                <table border=\"1\" style=\"border-collapse: collapse;\">\n",
    "                    <tr>\n",
    "                        <th>Character in Prediction Focused</th>\n",
    "                        <th>Attention Visualization</th>\n",
    "                    </tr>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Add rows for each prediction\n",
    "                for char, prob in zip(pred_chars, pred_probs):\n",
    "                    table_html += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>character at index {char_pos} of {char}<br>({prob:.4f})</td>\n",
    "                        <td style=\"text-align: center; font-size: 20px; letter-spacing: 3px;\">{colored_text}</td>\n",
    "                    </tr>\n",
    "                    \"\"\"\n",
    "                \n",
    "                table_html += \"</table>\"\n",
    "                \n",
    "                return table_html\n",
    "            \n",
    "            # Create output widget\n",
    "            output_html = widgets.HTML(\n",
    "                value=update_output(example_dropdown.value, position_slider.value)\n",
    "            )\n",
    "            \n",
    "            # Update output when widgets change\n",
    "            def on_change(change):\n",
    "                output_html.value = update_output(example_dropdown.value, position_slider.value)\n",
    "            \n",
    "            example_dropdown.observe(on_change, names='value')\n",
    "            position_slider.observe(on_change, names='value')\n",
    "            \n",
    "            # Display widgets\n",
    "            display(widgets.VBox([\n",
    "                example_dropdown,\n",
    "                position_slider,\n",
    "                output_html\n",
    "            ]))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create interactive widgets: {e}\")\n",
    "            print(\"You may not be in a Jupyter notebook environment.\")\n",
    "    \n",
    "    return {\n",
    "        'examples': examples,\n",
    "        'visualizations': all_visualizations,\n",
    "        'summary_path': summary_path\n",
    "    }\n",
    "\n",
    "\n",
    "# Add this method to the Decoder class\n",
    "def get_all_predictions(self, encoder_outputs, decoder_outputs=None):\n",
    "    \"\"\"\n",
    "    Get predictions for all positions in the target sequence\n",
    "    \n",
    "    Args:\n",
    "        encoder_outputs: Output from the encoder\n",
    "        decoder_outputs: Outputs from the decoder (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of predictions for each position [batch_size, max_len, vocab_size]\n",
    "    \"\"\"\n",
    "    batch_size = encoder_outputs.size(0)\n",
    "    max_len = decoder_outputs.size(1) if decoder_outputs is not None else 50\n",
    "    \n",
    "    # Initialize predictions tensor\n",
    "    all_predictions = torch.zeros(batch_size, max_len, self.output_size).to(encoder_outputs.device)\n",
    "    \n",
    "    # Process encoder outputs\n",
    "    encoder_hidden = self.get_encoder_hidden(encoder_outputs)\n",
    "    \n",
    "    # Initialize decoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # Initialize decoder input\n",
    "    decoder_input = torch.ones(batch_size, 1, device=encoder_outputs.device, dtype=torch.long)\n",
    "    \n",
    "    # Generate predictions for each position\n",
    "    for t in range(max_len-1):\n",
    "        # If decoder_outputs is provided, use it\n",
    "        if decoder_outputs is not None and t < decoder_outputs.size(1)-1:\n",
    "            decoder_input = decoder_outputs[:, t:t+1]\n",
    "        \n",
    "        # Get prediction for this position\n",
    "        decoder_output, decoder_hidden, attention = self.forward_step(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        \n",
    "        # Store prediction\n",
    "        all_predictions[:, t+1, :] = decoder_output.squeeze(1)\n",
    "        \n",
    "        # Use predicted token as next input\n",
    "        if decoder_outputs is None:\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze(-1).detach()\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Example of how to run the visualization with your existing model\n",
    "# (no need to reload the model or vocab files)\n",
    "def visualize_character_attention(model, dataloader, device, src_vocab, tgt_vocab, save_dir=None):\n",
    "    \"\"\"\n",
    "    Wrapper function to run the attention visualization with existing model and data\n",
    "    \n",
    "    Args:\n",
    "        model: Your already loaded Seq2Seq model\n",
    "        dataloader: DataLoader with test examples\n",
    "        device: Device to run on (cuda or cpu)\n",
    "        src_vocab: Source vocabulary dictionary\n",
    "        tgt_vocab: Target vocabulary dictionary\n",
    "        save_dir: Directory to save visualizations (defaults to current directory)\n",
    "    \"\"\"\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(os.getcwd(), 'character_attention_vis')\n",
    "    \n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize wandb if not already initialized\n",
    "    if wandb.run is None:\n",
    "        wandb.init(\n",
    "            project=\"dakshina-transliteration\",\n",
    "            name=\"character-attention-vis\",\n",
    "            config={\n",
    "                \"model\": \"seq2seq-attention\",\n",
    "                \"dataset\": \"dakshina-telugu-latin\"\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Add get_all_predictions method to Decoder class if it doesn't exist\n",
    "    if not hasattr(model.decoder, 'get_all_predictions'):\n",
    "        import types\n",
    "        model.decoder.get_all_predictions = types.MethodType(get_all_predictions, model.decoder)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    visualizations = generate_character_attention_visualizations(\n",
    "        model, dataloader, save_dir, device, src_vocab, tgt_vocab,\n",
    "        num_examples=5, top_k_predictions=5, interactive=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Visualization complete! Results saved to {save_dir}\")\n",
    "    return visualizations\n",
    "\n",
    "# Example usage:\n",
    "# visualize_character_attention(model, test_loader, device, src_vocab, tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:42:12.209306Z",
     "iopub.status.busy": "2025-05-20T15:42:12.208966Z",
     "iopub.status.idle": "2025-05-20T15:42:19.361390Z",
     "shell.execute_reply": "2025-05-20T15:42:19.360472Z",
     "shell.execute_reply.started": "2025-05-20T15:42:12.209276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_154212-g87kyc9o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/g87kyc9o' target=\"_blank\">connectivity-vis</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/g87kyc9o' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/g87kyc9o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/3529505378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# Run the visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mcreate_connectivity_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Add this code to your notebook after you've loaded your model\n",
    "\n",
    "def create_connectivity_visualization(model, dataloader, device, src_vocab, tgt_vocab, num_examples=5):\n",
    "    \"\"\"Create connectivity heatmap visualizations for your model\"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    import wandb\n",
    "    \n",
    "    # Create reverse mappings\n",
    "    id_to_src = {v: k for k, v in src_vocab.items()}\n",
    "    id_to_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "    \n",
    "    # Create a purple colormap for visualization\n",
    "    purple_cm = LinearSegmentedColormap.from_list(\n",
    "        'purple_cm', \n",
    "        [(1, 1, 1, 0),            # Transparent white\n",
    "         (0.95, 0.9, 0.95, 0.5),  # Very light purple\n",
    "         (0.8, 0.6, 0.8, 0.7),    # Light purple\n",
    "         (0.6, 0.4, 0.7, 0.8),    # Medium purple\n",
    "         (0.5, 0.2, 0.5, 0.9),    # Dark purple\n",
    "         (0.4, 0.1, 0.4, 1.0)],   # Very dark purple\n",
    "        N=256\n",
    "    )\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Track examples processed\n",
    "        examples_processed = 0\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "            if examples_processed >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Generate outputs and attention weights\n",
    "            outputs, attentions = model.decode(src)\n",
    "            \n",
    "            # Process examples from this batch\n",
    "            for i in range(min(len(src), src.size(0))):\n",
    "                if examples_processed >= num_examples:\n",
    "                    break\n",
    "                    \n",
    "                # Extract source characters (skipping special tokens)\n",
    "                src_tokens = []\n",
    "                for idx in src[i]:\n",
    "                    if idx.item() not in [0, 1, 2, 3]:  # Skip special tokens\n",
    "                        src_tokens.append(id_to_src.get(idx.item(), ''))\n",
    "                \n",
    "                # Extract prediction characters\n",
    "                pred_tokens = []\n",
    "                for idx in outputs[i, 1:]:  # Skip <sos>\n",
    "                    if idx.item() not in [0, 1, 2, 3]:  # Skip special tokens\n",
    "                        pred_tokens.append(id_to_tgt.get(idx.item(), ''))\n",
    "                \n",
    "                # Get attention matrix\n",
    "                attn = attentions[i, :len(pred_tokens), 1:len(src_tokens)+1].cpu().numpy()\n",
    "                \n",
    "                # Full text strings\n",
    "                src_text = ''.join(src_tokens)\n",
    "                pred_text = ''.join(pred_tokens)\n",
    "                \n",
    "                # Create figure\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                ax = plt.subplot(111)\n",
    "                \n",
    "                # Plot attention heatmap\n",
    "                ax.imshow(attn, cmap=purple_cm, aspect='auto', interpolation='nearest')\n",
    "                \n",
    "                # Add grid lines\n",
    "                ax.set_xticks(np.arange(-.5, len(src_tokens), 1), minor=True)\n",
    "                ax.set_yticks(np.arange(-.5, len(pred_tokens), 1), minor=True)\n",
    "                ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "                \n",
    "                # Set labels\n",
    "                ax.set_xlabel('Input Sequence', fontsize=14)\n",
    "                ax.set_ylabel('Output Sequence', fontsize=14)\n",
    "                \n",
    "                # Set ticks\n",
    "                ax.set_xticks(np.arange(len(src_tokens)))\n",
    "                ax.set_xticklabels(src_tokens, fontsize=12)\n",
    "                ax.set_yticks(np.arange(len(pred_tokens)))\n",
    "                ax.set_yticklabels(pred_tokens, fontsize=12)\n",
    "                \n",
    "                # Add title\n",
    "                plt.title('Connectivity: Which input character is attended to for each output?', \n",
    "                         fontsize=16)\n",
    "                \n",
    "                # Add input/output text at bottom\n",
    "                plt.figtext(0.5, 0.01, f'Input: {src_text}', ha='center', fontsize=12)\n",
    "                plt.figtext(0.5, 0.04, f'Output: {pred_text}', ha='center', fontsize=12)\n",
    "                \n",
    "                # Adjust layout\n",
    "                plt.tight_layout(rect=[0, 0.06, 1, 0.96])\n",
    "                \n",
    "                # Log to wandb\n",
    "                wandb.log({\n",
    "                    f\"connectivity_example_{examples_processed}\": wandb.Image(\n",
    "                        plt, caption=f\"Attention Connectivity: {src_text} → {pred_text}\"\n",
    "                    )\n",
    "                })\n",
    "                \n",
    "                # Display the plot in notebook\n",
    "                plt.show()\n",
    "                \n",
    "                # Close to free memory\n",
    "                plt.close()\n",
    "                \n",
    "                examples_processed += 1\n",
    "\n",
    "# Initialize wandb (if not already initialized)\n",
    "import wandb\n",
    "if wandb.run is None:\n",
    "    wandb.init(project=\"dakshina-transliteration\", name=\"connectivity-vis\")\n",
    "\n",
    "# Run the visualization\n",
    "create_connectivity_visualization(model, test_loader, device, src_vocab, tgt_vocab, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:50:04.437407Z",
     "iopub.status.busy": "2025-05-20T16:50:04.437098Z",
     "iopub.status.idle": "2025-05-20T17:00:28.391427Z",
     "shell.execute_reply": "2025-05-20T17:00:28.390911Z",
     "shell.execute_reply.started": "2025-05-20T16:50:04.437386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">d3js-attention-visualization</strong> at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/49vc7sy9' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/49vc7sy9</a><br> View project at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_164521-49vc7sy9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_165004-x1ntbry9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/x1ntbry9' target=\"_blank\">d3js-attention-visualization-trained</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/x1ntbry9' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/x1ntbry9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized wandb\n",
      "Building vocabulary from training data\n",
      "Loaded 58550 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkita', Native: 'అంకిత'\n",
      "  Roman: 'ankita', Native: 'అంకిత'\n",
      "  Roman: 'ankitha', Native: 'అంకిత'\n",
      "Vocab sizes -> src: 30, tgt: 67\n",
      "Loaded 5683 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amka', Native: 'అంక'\n",
      "  Roman: 'anka', Native: 'అంక'\n",
      "  Roman: 'amkam', Native: 'అంకం'\n",
      "Loaded 5747 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamloo', Native: 'అంకంలో'\n",
      "Model has 7,275,075 parameters (7,275,075 trainable)\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:51<00:00,  1.57it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.6563, Char Acc: 0.8231, Exact Match: 0.4029\n",
      "Val - Loss: 0.3639, Char Acc: 0.8984, Exact Match: 0.5043 (2866/5683)\n",
      "Saved new best model with validation accuracy: 0.5043\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 458/458 [04:43<00:00,  1.61it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:12<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.2915, Char Acc: 0.9240, Exact Match: 0.6384\n",
      "Val - Loss: 0.3301, Char Acc: 0.9093, Exact Match: 0.5652 (3212/5683)\n",
      "Saved new best model with validation accuracy: 0.5652\n",
      "Loaded best model for visualization\n",
      "Saved HTML visualization to /kaggle/working/d3js_attention_vis/attention_visualization.html\n",
      "Logged visualization to wandb\n",
      "Visualized 5 examples\n",
      "First sample:\n",
      "Source: vyavaharinchae\n",
      "Prediction: ్యవహరించే<eos>\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>num_samples</td><td>▁</td></tr><tr><td>train_char_accuracy</td><td>▁█</td></tr><tr><td>train_exact_match</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>val_char_accuracy</td><td>▁█</td></tr><tr><td>val_exact_match</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>num_samples</td><td>5</td></tr><tr><td>train_char_accuracy</td><td>0.92404</td></tr><tr><td>train_exact_match</td><td>0.63843</td></tr><tr><td>train_loss</td><td>0.29151</td></tr><tr><td>val_char_accuracy</td><td>0.90925</td></tr><tr><td>val_exact_match</td><td>0.56519</td></tr><tr><td>val_loss</td><td>0.33005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">d3js-attention-visualization-trained</strong> at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/x1ntbry9' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/x1ntbry9</a><br> View project at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_165004-x1ntbry9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Dataset Definition ----\n",
    "class DakshinaTSVDataset(Dataset):\n",
    "    def __init__(self, tsv_file, src_vocab=None, tgt_vocab=None, max_len=64, build_vocab=False):\n",
    "        try:\n",
    "            df = pd.read_csv(tsv_file, sep='\\t', header=None,\n",
    "                            names=['native', 'roman', 'freq'], usecols=[0, 1], dtype=str)\n",
    "            # Fix the pandas warning by using a copy\n",
    "            df = df.copy()\n",
    "            df['native'] = df['native'].fillna('')\n",
    "            df['roman'] = df['roman'].fillna('')\n",
    "            self.pairs = list(zip(df['roman'], df['native']))\n",
    "            print(f\"Loaded {len(self.pairs)} examples from {tsv_file}\")\n",
    "            \n",
    "            # Store romanized words for visualization\n",
    "            self.romanized_words = df['roman'].tolist()\n",
    "            \n",
    "            # Print a few examples\n",
    "            if len(self.pairs) > 0:\n",
    "                print(\"Sample examples:\")\n",
    "                for i in range(min(3, len(self.pairs))):\n",
    "                    print(f\"  Roman: '{self.pairs[i][0]}', Native: '{self.pairs[i][1]}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            # Fallback to some example data if file not found\n",
    "            print(\"Using example data instead\")\n",
    "            self.pairs = [\n",
    "                ('farm', 'ఫార్మ్'),\n",
    "                ('adhikaash', 'అధికాష్'),\n",
    "                ('emilie', 'ఎమిలి'),\n",
    "                ('krishna', 'కృష్ణ'),\n",
    "                ('rama', 'రామ'),\n",
    "                ('amma', 'అమ్మ'),\n",
    "                ('bharathi', 'భారతి'),\n",
    "                ('srinu', 'శ్రీను'),\n",
    "                ('telugu', 'తెలుగు'),\n",
    "                ('hyderabad', 'హైదరాబాద్')\n",
    "            ]\n",
    "            # Create romanized words list\n",
    "            self.romanized_words = [pair[0] for pair in self.pairs]\n",
    "                \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.src_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self.tgt_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n",
    "            # Ensure special tokens exist\n",
    "            for v in ('<eos>', '<sos>'):\n",
    "                if v not in self.src_vocab: self.src_vocab[v] = len(self.src_vocab)\n",
    "                if v not in self.tgt_vocab: self.tgt_vocab[v] = len(self.tgt_vocab)\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        for src, tgt in self.pairs:\n",
    "            for ch in src:\n",
    "                if ch not in self.src_vocab: self.src_vocab[ch] = len(self.src_vocab)\n",
    "            for ch in tgt:\n",
    "                if ch not in self.tgt_vocab: self.tgt_vocab[ch] = len(self.tgt_vocab)\n",
    "        print(f\"Vocab sizes -> src: {len(self.src_vocab)}, tgt: {len(self.tgt_vocab)}\")\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        \n",
    "        # Add <sos> and <eos> tokens\n",
    "        src_idxs = [self.src_vocab['<sos>']] + [self.src_vocab.get(ch, self.src_vocab['<unk>']) for ch in src] + [self.src_vocab['<eos>']]\n",
    "        tgt_idxs = [self.tgt_vocab['<sos>']] + [self.tgt_vocab.get(ch, self.tgt_vocab['<unk>']) for ch in tgt] + [self.tgt_vocab['<eos>']]\n",
    "        \n",
    "        # Pad sequences\n",
    "        pad_src = [self.src_vocab['<pad>']] * max(0, self.max_len - len(src_idxs))\n",
    "        pad_tgt = [self.tgt_vocab['<pad>']] * max(0, self.max_len - len(tgt_idxs))\n",
    "        \n",
    "        # Truncate if necessary and convert to tensor\n",
    "        src_tensor = torch.tensor((src_idxs + pad_src)[:self.max_len], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor((tgt_idxs + pad_tgt)[:self.max_len], dtype=torch.long)\n",
    "        \n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "# ---- Encoder ----\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout=0, bidirectional=True, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "            \n",
    "        # Initialize weights\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Create mask for attention\n",
    "        mask = (x != 0).float()  # 0 is <pad>\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_size]\n",
    "        \n",
    "        # Pass through RNN\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # No need to reshape hidden states - just return them as is\n",
    "        # The decoder will handle the format conversion\n",
    "        \n",
    "        # Return encoder outputs, hidden state, and mask\n",
    "        return outputs, hidden, mask\n",
    "\n",
    "# ---- Attention Mechanism ----\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super().__init__()\n",
    "        # Create a linear layer to convert the concatenated hidden states to attention scores\n",
    "        self.energy = nn.Linear(enc_hidden_size + dec_hidden_size, dec_hidden_size)\n",
    "        self.v = nn.Linear(dec_hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, dec_hidden_size]\n",
    "        # encoder_outputs: [batch_size, src_len, enc_hidden_size]\n",
    "        # mask: [batch_size, src_len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        # [batch_size, dec_hidden_size] -> [batch_size, src_len, dec_hidden_size]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Create energy by concatenating encoder outputs and decoder hidden\n",
    "        # [batch_size, src_len, enc_hidden_size + dec_hidden_size]\n",
    "        energy = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        \n",
    "        # Apply attention layer\n",
    "        # [batch_size, src_len, dec_hidden_size]\n",
    "        energy = torch.tanh(self.energy(energy))\n",
    "        \n",
    "        # Get attention scores\n",
    "        # [batch_size, src_len, 1]\n",
    "        attention = self.v(energy)\n",
    "        \n",
    "        # [batch_size, src_len]\n",
    "        attention = attention.squeeze(2)\n",
    "        \n",
    "        # Mask out padding positions\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        # [batch_size, src_len]\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "# ---- Decoder ----\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, enc_hidden_size, dec_hidden_size, \n",
    "                 num_layers, dropout=0, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        # Initialize attention mechanism\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        \n",
    "        # Context vector + embedding size as input to RNN\n",
    "        rnn_input_size = embedding_size + enc_hidden_size\n",
    "        \n",
    "        # Initialize RNN based on cell type\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        # Final output layer that combines decoder output, context and embedding\n",
    "        self.fc_out = nn.Linear(dec_hidden_size + enc_hidden_size + embedding_size, output_size)\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "    \n",
    "    def forward_step(self, input, hidden, encoder_outputs, mask):\n",
    "        \"\"\"Single step forward for the decoder\"\"\"\n",
    "        # Embed input token\n",
    "        # [batch_size] -> [batch_size, 1, embedding_size]\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        \n",
    "        # Get appropriate hidden state for attention\n",
    "        if self.cell_type == 'lstm':\n",
    "            h_for_attn = hidden[0][-1]  # use last layer's hidden state\n",
    "        else:\n",
    "            h_for_attn = hidden[-1]  # use last layer's hidden state\n",
    "            \n",
    "        attn_weights = self.attention(h_for_attn, encoder_outputs, mask)\n",
    "        \n",
    "        # Create context vector by weighting encoder outputs with attention\n",
    "        # [batch_size, 1, src_len] * [batch_size, src_len, enc_hidden_size]\n",
    "        # -> [batch_size, 1, enc_hidden_size]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        # Combine embedded token and context vector\n",
    "        # [batch_size, 1, embedding_size + enc_hidden_size]\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        # output: [batch_size, 1, dec_hidden_size]\n",
    "        # hidden: [num_layers, batch_size, dec_hidden_size] or tuple for LSTM\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # Combine output, context and embedding for final prediction\n",
    "        # [batch_size, 1, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = torch.cat((output, context, embedded), dim=2)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        # [batch_size, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        # Pass through final linear layer\n",
    "        # [batch_size, output_size]\n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, hidden, attn_weights\n",
    "                \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        # Run a single forward step (for the teacher forcing loop in the Seq2Seq model)\n",
    "        return self.forward_step(input, hidden, encoder_outputs, mask)\n",
    "\n",
    "# ---- Seq2Seq Model ----\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.7):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        \n",
    "    def forward(self, src, tgt, return_attention=False):\n",
    "        \"\"\"\n",
    "        Training mode (with optional attention return)\n",
    "        \n",
    "        Args:\n",
    "            src: Source tokens\n",
    "            tgt: Target tokens\n",
    "            return_attention: If True, return attention weights\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len-1, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Tensor to store attention weights if needed\n",
    "        attentions = torch.zeros(batch_size, tgt_len-1, src.shape[1]) if return_attention else None\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input to decoder is the <sos> token\n",
    "        input = tgt[:, 0]\n",
    "        \n",
    "        # Teacher forcing ratio\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        \n",
    "        # Decode one token at a time\n",
    "        for t in range(1, tgt_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, attn = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Store output\n",
    "            outputs[:, t-1] = output\n",
    "            \n",
    "            # Store attention weights if needed\n",
    "            if return_attention:\n",
    "                attentions[:, t-1, :] = attn\n",
    "            \n",
    "            # Next input is either true target (teacher forcing) or predicted token\n",
    "            if use_teacher_forcing:\n",
    "                input = tgt[:, t]\n",
    "            else:\n",
    "                # Get highest scoring token\n",
    "                input = output.argmax(1)\n",
    "        \n",
    "        if return_attention:\n",
    "            return outputs, attentions\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "    def decode(self, src, max_len=100):\n",
    "        \"\"\"\n",
    "        Inference mode (no teacher forcing)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Output tokens\n",
    "            attentions: Attention weights\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder (bidirectional handling)\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input is <sos> token\n",
    "        input = torch.ones(batch_size, dtype=torch.long).to(self.device) * 3  # <sos> = 3\n",
    "        \n",
    "        # Track generated tokens and attention weights\n",
    "        outputs = [input]\n",
    "        attentions = []\n",
    "        \n",
    "        # Track if sequence has ended\n",
    "        ended = torch.zeros(batch_size, dtype=torch.bool).to(self.device)\n",
    "        \n",
    "        # Decode until max length or all sequences end\n",
    "        for t in range(1, max_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, attn = self.decoder.forward_step(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Get next token\n",
    "            input = output.argmax(1)\n",
    "            \n",
    "            # Store output and attention\n",
    "            outputs.append(input)\n",
    "            attentions.append(attn)\n",
    "            \n",
    "            # Check if all sequences have ended\n",
    "            ended = ended | (input == 2)  # 2 is <eos>\n",
    "            if ended.all():\n",
    "                break\n",
    "                \n",
    "        # Convert list of tensors to single tensor\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len]\n",
    "        attentions = torch.stack(attentions, dim=1)  # [batch_size, seq_len-1, src_len]\n",
    "        \n",
    "        return outputs, attentions\n",
    "\n",
    "# ---- Training & Evaluation Functions ----\n",
    "def compute_exact_match_accuracy(preds, targets, tgt_vocab):\n",
    "    \"\"\"Compute exact match accuracy between predictions and targets\"\"\"\n",
    "    batch_size = preds.size(0)\n",
    "    correct = 0\n",
    "    \n",
    "    # Convert ids to strings\n",
    "    id_to_char = {v: k for k, v in tgt_vocab.items() if k not in ['<pad>', '<sos>', '<eos>', '<unk>']}\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Extract character sequences (removing special tokens)\n",
    "        pred_seq = ''.join([id_to_char.get(idx.item(), '') for idx in preds[i, 1:] \n",
    "                            if idx.item() not in [0, 1, 2, 3]])  # Skip <pad>, <unk>, <eos>, <sos>\n",
    "        \n",
    "        # For target, skip first token (<sos>) and stop at <eos> or <pad>\n",
    "        tgt_seq = ''\n",
    "        for idx in targets[i, 1:]:  # Skip first token\n",
    "            token_id = idx.item()\n",
    "            if token_id in [0, 2]:  # <pad> or <eos>\n",
    "                break\n",
    "            if token_id not in [1, 3]:  # Skip <unk> and <sos>\n",
    "                tgt_seq += id_to_char.get(token_id, '')\n",
    "        \n",
    "        # Check for exact match\n",
    "        if pred_seq == tgt_seq:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / batch_size\n",
    "\n",
    "def compute_char_accuracy(logits, targets):\n",
    "    \"\"\"Compute character-level accuracy between logits and targets\"\"\"\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    mask = (targets != 0)  # Ignore padding\n",
    "    correct = ((preds == targets) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_char_acc = 0\n",
    "    epoch_exact_match_acc = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for src, tgt in tqdm(dataloader, desc=\"Training\"):\n",
    "        batch_size = src.size(0)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Flatten output and target tensors for loss calculation\n",
    "        # Ignore the first token in target (<sos>)\n",
    "        output_flat = output.reshape(-1, output.shape[-1])\n",
    "        target_flat = tgt[:, 1:].reshape(-1)  # Shift right to predict next token\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_flat, target_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        char_acc = compute_char_accuracy(output, tgt[:, 1:])\n",
    "        \n",
    "        # Decode for exact match accuracy\n",
    "        with torch.no_grad():\n",
    "            predictions, _ = model.decode(src)\n",
    "            exact_match_acc = compute_exact_match_accuracy(predictions, tgt, dataloader.dataset.tgt_vocab)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_char_acc += char_acc * batch_size\n",
    "        epoch_exact_match_acc += exact_match_acc * batch_size\n",
    "        total_batches += batch_size\n",
    "    \n",
    "    # Return average metrics\n",
    "    return {\n",
    "        'loss': epoch_loss / total_batches,\n",
    "        'char_acc': epoch_char_acc / total_batches,\n",
    "        'exact_match_acc': epoch_exact_match_acc / total_batches\n",
    "    }\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_char_acc = 0\n",
    "    epoch_exact_match_acc = 0\n",
    "    total_batches = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_size = src.size(0)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass (use teacher forcing for loss calculation)\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Flatten output and target tensors for loss calculation\n",
    "            output_flat = output.reshape(-1, output.shape[-1])\n",
    "            target_flat = tgt[:, 1:].reshape(-1)  # Shift right to predict next token\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            char_acc = compute_char_accuracy(output, tgt[:, 1:])\n",
    "            \n",
    "            # Decode for exact match accuracy (no teacher forcing)\n",
    "            predictions, _ = model.decode(src)\n",
    "            exact_match_acc = compute_exact_match_accuracy(predictions, tgt, dataloader.dataset.tgt_vocab)\n",
    "            \n",
    "            # Count exact matches for reporting\n",
    "            correct_batch = int(exact_match_acc * batch_size)\n",
    "            correct_predictions += correct_batch\n",
    "            total_predictions += batch_size\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            epoch_char_acc += char_acc * batch_size\n",
    "            epoch_exact_match_acc += exact_match_acc * batch_size\n",
    "            total_batches += batch_size\n",
    "    \n",
    "    # Return average metrics\n",
    "    return {\n",
    "        'loss': epoch_loss / total_batches,\n",
    "        'char_acc': epoch_char_acc / total_batches,\n",
    "        'exact_match_acc': epoch_exact_match_acc / total_batches,\n",
    "        'correct': correct_predictions,\n",
    "        'total': total_predictions\n",
    "    }\n",
    "\n",
    "# ---- Main Function ----\n",
    "def main():\n",
    "    \"\"\"Train the model for 2 epochs and create D3.js attention visualizations\"\"\"\n",
    "    # Best config\n",
    "    config = {\n",
    "        'cell_type': 'gru',\n",
    "        'dropout': 0,\n",
    "        'embedding_size': 512,\n",
    "        'num_layers': 1,\n",
    "        'batch_size': 128,\n",
    "        'hidden_size': 512,\n",
    "        'bidirectional': True,\n",
    "        'learning_rate': 0.0005,\n",
    "        'teacher_forcing': 0.7,\n",
    "        'optim': 'adam',\n",
    "        'epochs': 2  # Train for 2 epochs\n",
    "    }\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Paths\n",
    "    data_dir = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons'\n",
    "    model_dir = '/kaggle/working/d3js_attention_vis'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=\"dakshina-transliteration\",\n",
    "            name=\"d3js-attention-visualization-trained\",\n",
    "            config=config\n",
    "        )\n",
    "        print(\"Initialized wandb\")\n",
    "    except:\n",
    "        print(\"Failed to initialize wandb - will continue without logging\")\n",
    "    \n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_tsv = os.path.join(data_dir, 'te.translit.sampled.train.tsv')\n",
    "        dev_tsv = os.path.join(data_dir, 'te.translit.sampled.dev.tsv')\n",
    "        test_tsv = os.path.join(data_dir, 'te.translit.sampled.test.tsv')\n",
    "        \n",
    "        # Build vocabulary from training data\n",
    "        print(\"Building vocabulary from training data\")\n",
    "        train_dataset = DakshinaTSVDataset(train_tsv, build_vocab=True)\n",
    "        src_vocab, tgt_vocab = train_dataset.src_vocab, train_dataset.tgt_vocab\n",
    "        \n",
    "        # Save vocabulary\n",
    "        with open(os.path.join(model_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(src_vocab, f, ensure_ascii=False)\n",
    "        with open(os.path.join(model_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "        \n",
    "        # Create the rest of the datasets\n",
    "        val_dataset = DakshinaTSVDataset(dev_tsv, src_vocab, tgt_vocab)\n",
    "        test_dataset = DakshinaTSVDataset(test_tsv, src_vocab, tgt_vocab)\n",
    "        \n",
    "        # Get romanized words for visualization\n",
    "        romanized_test_words = test_dataset.romanized_words\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading datasets: {e}\")\n",
    "        # Create example datasets\n",
    "        example_dataset = DakshinaTSVDataset(None, build_vocab=True)\n",
    "        src_vocab, tgt_vocab = example_dataset.src_vocab, example_dataset.tgt_vocab\n",
    "        \n",
    "        # Save vocabulary\n",
    "        with open(os.path.join(model_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(src_vocab, f, ensure_ascii=False)\n",
    "        with open(os.path.join(model_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "        \n",
    "        # Use the same dataset for train, val, and test\n",
    "        train_dataset = example_dataset\n",
    "        val_dataset = example_dataset\n",
    "        test_dataset = example_dataset\n",
    "        \n",
    "        # Get romanized words for visualization\n",
    "        romanized_test_words = test_dataset.romanized_words\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=min(config['batch_size'], len(train_dataset)), shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=min(config['batch_size'], len(val_dataset)))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=min(config['batch_size'], len(test_dataset)))\n",
    "    \n",
    "    # Create model\n",
    "    encoder = Encoder(\n",
    "        input_size=len(src_vocab),\n",
    "        embedding_size=config['embedding_size'],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        bidirectional=config['bidirectional'],\n",
    "        cell_type=config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Calculate encoder output size (doubled if bidirectional)\n",
    "    enc_hidden_size = config['hidden_size'] * 2 if config['bidirectional'] else config['hidden_size']\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        output_size=len(tgt_vocab),\n",
    "        embedding_size=config['embedding_size'],\n",
    "        enc_hidden_size=enc_hidden_size,\n",
    "        dec_hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        cell_type=config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Create full model\n",
    "    model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=config['teacher_forcing'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function (ignore padding token)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {total_params:,} parameters ({trainable_params:,} trainable)\")\n",
    "    \n",
    "    # Train for 2 epochs\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Char Acc: {train_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {train_metrics['exact_match_acc']:.4f}\")\n",
    "        print(f\"Val - Loss: {val_metrics['loss']:.4f}, Char Acc: {val_metrics['char_acc']:.4f}, \"\n",
    "              f\"Exact Match: {val_metrics['exact_match_acc']:.4f} ({val_metrics['correct']}/{val_metrics['total']})\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['exact_match_acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['exact_match_acc']\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(model_dir, \"best_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_accuracy': val_metrics['exact_match_acc'],\n",
    "                'config': config\n",
    "            }, model_path)\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.4f}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        try:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_metrics['loss'],\n",
    "                'train_char_accuracy': train_metrics['char_acc'],\n",
    "                'train_exact_match': train_metrics['exact_match_acc'],\n",
    "                'val_loss': val_metrics['loss'],\n",
    "                'val_char_accuracy': val_metrics['char_acc'],\n",
    "                'val_exact_match': val_metrics['exact_match_acc']\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Load the best model if it exists\n",
    "    try:\n",
    "        checkpoint = torch.load(os.path.join(model_dir, 'best_model.pt'), map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model for visualization\")\n",
    "    except:\n",
    "        print(\"Using the last trained model for visualization\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create reverse vocabulary mappings for visualization\n",
    "    IDX2CHAR_SRC = {v: k for k, v in src_vocab.items()}\n",
    "    IDX2CHAR_TGT = {v: k for k, v in tgt_vocab.items()}\n",
    "    pad_idx = src_vocab['<pad>']\n",
    "    \n",
    "    # Collect samples for visualization\n",
    "    all_samples = []\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(test_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            out, attn_weights = model(src, tgt, return_attention=True)\n",
    "            batch_size = src.size(0)\n",
    "            for b in range(batch_size):\n",
    "                try:\n",
    "                    romanized = romanized_test_words[i * batch_size + b]\n",
    "                    src_tokens = list(romanized)\n",
    "                except:\n",
    "                    # If we can't get the romanized word, use the source tokens\n",
    "                    src_tokens = [IDX2CHAR_SRC[idx.item()] for idx in src[b] if idx.item() not in [0, 1, 2, 3]]\n",
    "                \n",
    "                tgt_tokens = [IDX2CHAR_TGT[idx.item()] for idx in tgt[b][1:] if idx.item() not in [0, 1, 2, 3]]\n",
    "                pred_tokens = out.argmax(dim=2)[b][1:len(tgt_tokens)+1]\n",
    "                pred_chars = [IDX2CHAR_TGT[idx.item()] for idx in pred_tokens]\n",
    "                \n",
    "                # Get attention weights and trim to actual token lengths\n",
    "                if attn_weights.shape[1] >= len(pred_chars) and attn_weights.shape[2] >= len(src_tokens):\n",
    "                    attn = attn_weights[b][:len(pred_chars), :len(src_tokens)].cpu().numpy().tolist()\n",
    "                    all_samples.append((src_tokens, pred_chars, attn))\n",
    "    \n",
    "    # Select 5 random samples for visualization\n",
    "    num_samples = min(5, len(all_samples))\n",
    "    random_samples = random.sample(all_samples, num_samples)\n",
    "    \n",
    "    # Build HTML blocks with improved styling\n",
    "    html_blocks = []\n",
    "    for sample_count, (src_tokens, pred_chars, attn) in enumerate(random_samples):\n",
    "        input_tokens_js = json.dumps(src_tokens, ensure_ascii=False)\n",
    "        output_tokens_js = json.dumps(pred_chars, ensure_ascii=False)\n",
    "        attention_js = json.dumps(attn)\n",
    "        html_block = f\"\"\"\n",
    "        <div class=\"sample-container\">\n",
    "          <h2 class=\"sample-title\">Sample {sample_count + 1}</h2>\n",
    "          <div class=\"input-section\">\n",
    "            <div class=\"input-label\">Input (English):</div>\n",
    "            <div id=\"input-tokens-{sample_count}\" class=\"tokens-container\"></div>\n",
    "          </div>\n",
    "          <div class=\"output-section\">\n",
    "            <div class=\"output-label\">Predicted Output (Telugu):</div>\n",
    "            <div id=\"output-tokens-{sample_count}\" class=\"tokens-container\"></div>\n",
    "          </div>\n",
    "          <div class=\"attention-hint\">Hover over a Telugu character to see which input characters it's attending to!</div>\n",
    "          <script>\n",
    "            const inputTokens_{sample_count} = {input_tokens_js};\n",
    "            const outputTokens_{sample_count} = {output_tokens_js};\n",
    "            const attention_{sample_count} = {attention_js};\n",
    "            const inputDiv_{sample_count} = d3.select(\"#input-tokens-{sample_count}\");\n",
    "            const outputDiv_{sample_count} = d3.select(\"#output-tokens-{sample_count}\");\n",
    "            \n",
    "            inputTokens_{sample_count}.forEach((token, i) => {{\n",
    "              inputDiv_{sample_count}.append(\"span\")\n",
    "                .attr(\"class\", \"token input\")\n",
    "                .attr(\"id\", \"input-{sample_count}-\" + i)\n",
    "                .text(token);\n",
    "            }});\n",
    "            \n",
    "            outputTokens_{sample_count}.forEach((token, i) => {{\n",
    "              outputDiv_{sample_count}.append(\"span\")\n",
    "                .attr(\"class\", \"token output\")\n",
    "                .text(token)\n",
    "                .on(\"mouseover\", () => {{\n",
    "                  d3.selectAll(\".token.input\").style(\"background-color\", \"#fff\");\n",
    "                  attention_{sample_count}[i].forEach((score, j) => {{\n",
    "                    if (j < inputTokens_{sample_count}.length) {{\n",
    "                      const color = d3.interpolateRdPu(score);\n",
    "                      d3.select(\"#input-{sample_count}-\" + j)\n",
    "                        .style(\"background-color\", color)\n",
    "                        .text(inputTokens_{sample_count}[j] + \" (\" + Math.round(score * 100) + \"%)\");\n",
    "                    }}\n",
    "                  }});\n",
    "                }})\n",
    "                .on(\"mouseout\", () => {{\n",
    "                  d3.selectAll(\".token.input\").style(\"background-color\", \"#fff\");\n",
    "                  inputTokens_{sample_count}.forEach((token, j) => {{\n",
    "                    d3.select(\"#input-{sample_count}-\" + j).text(token);\n",
    "                  }});\n",
    "                }});\n",
    "            }});\n",
    "          </script>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        html_blocks.append(html_block)\n",
    "    \n",
    "    # Full HTML document with improved styling\n",
    "    full_html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "      <meta charset=\"UTF-8\" />\n",
    "      <title>D3.js Attention Visualizations</title>\n",
    "      <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n",
    "      <style>\n",
    "        body {{\n",
    "          font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "          margin: 0;\n",
    "          padding: 0;\n",
    "          line-height: 1.6;\n",
    "          color: #333;\n",
    "          background-color: #f8f9fa;\n",
    "        }}\n",
    "        \n",
    "        .container {{\n",
    "          max-width: 1200px;\n",
    "          margin: 0 auto;\n",
    "          padding: 30px;\n",
    "        }}\n",
    "        \n",
    "        h1 {{\n",
    "          text-align: center;\n",
    "          color: #6a1b9a;\n",
    "          margin-bottom: 30px;\n",
    "          font-size: 2.5rem;\n",
    "        }}\n",
    "        \n",
    "        .instructions {{\n",
    "          background-color: #e8eaf6;\n",
    "          border-radius: 8px;\n",
    "          padding: 15px 20px;\n",
    "          margin-bottom: 40px;\n",
    "          font-size: 1.1rem;\n",
    "          box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        \n",
    "        .instructions strong {{\n",
    "          color: #6a1b9a;\n",
    "        }}\n",
    "        \n",
    "        .sample-container {{\n",
    "          background-color: white;\n",
    "          border-radius: 8px;\n",
    "          padding: 25px;\n",
    "          margin-bottom: 40px;\n",
    "          box-shadow: 0 3px 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        \n",
    "        .sample-title {{\n",
    "          color: #6a1b9a;\n",
    "          border-bottom: 2px solid #e1bee7;\n",
    "          padding-bottom: 10px;\n",
    "          margin-top: 0;\n",
    "        }}\n",
    "        \n",
    "        .input-section, .output-section {{\n",
    "          margin-bottom: 20px;\n",
    "        }}\n",
    "        \n",
    "        .input-label, .output-label {{\n",
    "          font-weight: bold;\n",
    "          margin-bottom: 10px;\n",
    "          font-size: 1.1rem;\n",
    "          color: #333;\n",
    "        }}\n",
    "        \n",
    "        .tokens-container {{\n",
    "          display: flex;\n",
    "          flex-wrap: wrap;\n",
    "          gap: 8px;\n",
    "          margin-top: 8px;\n",
    "        }}\n",
    "        \n",
    "        .token {{\n",
    "          display: inline-block;\n",
    "          padding: 10px 15px;\n",
    "          border-radius: 6px;\n",
    "          font-size: 22px;\n",
    "          cursor: pointer;\n",
    "          user-select: none;\n",
    "          transition: all 0.3s ease;\n",
    "          min-width: 20px;\n",
    "          text-align: center;\n",
    "        }}\n",
    "        \n",
    "        .token.input {{\n",
    "          background-color: #fff;\n",
    "          border: 2px solid #e1bee7;\n",
    "          color: #333;\n",
    "        }}\n",
    "        \n",
    "        .token.output {{\n",
    "          background-color: #6a1b9a;\n",
    "          color: white;\n",
    "          border: 2px solid #6a1b9a;\n",
    "          font-size: 24px;\n",
    "        }}\n",
    "        \n",
    "        .token.output:hover {{\n",
    "          transform: scale(1.1);\n",
    "          box-shadow: 0 4px 8px rgba(0,0,0,0.2);\n",
    "        }}\n",
    "        \n",
    "        .attention-hint {{\n",
    "          font-style: italic;\n",
    "          color: #666;\n",
    "          margin-top: 15px;\n",
    "          text-align: center;\n",
    "        }}\n",
    "        \n",
    "        footer {{\n",
    "          text-align: center;\n",
    "          margin-top: 50px;\n",
    "          padding: 20px;\n",
    "          color: #666;\n",
    "          font-size: 0.9rem;\n",
    "        }}\n",
    "      </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <div class=\"container\">\n",
    "        <h1>Telugu Transliteration Attention Visualization</h1>\n",
    "        \n",
    "        <div class=\"instructions\">\n",
    "          <strong>How to use:</strong> Hover over any Telugu character in the output to see which English characters it's attending to. \n",
    "          The percentage shown on each input character represents how much attention that character receives.\n",
    "        </div>\n",
    "        \n",
    "        {''.join(html_blocks)}\n",
    "        \n",
    "        <footer>\n",
    "          Created with Seq2Seq Attention Model - Telugu Transliteration\n",
    "        </footer>\n",
    "      </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save HTML to file\n",
    "    html_path = os.path.join(model_dir, 'attention_visualization.html')\n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(full_html)\n",
    "    print(f\"Saved HTML visualization to {html_path}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    try:\n",
    "        wandb.log({\n",
    "            \"d3js_attention_visualization\": wandb.Html(full_html),\n",
    "            \"num_samples\": num_samples\n",
    "        })\n",
    "        print(\"Logged visualization to wandb\")\n",
    "    except:\n",
    "        print(\"Failed to log to wandb\")\n",
    "    \n",
    "    # For debugging\n",
    "    print(f\"Visualized {num_samples} examples\")\n",
    "    if random_samples:\n",
    "        print(\"First sample:\")\n",
    "        src_tokens, pred_chars, _ = random_samples[0]\n",
    "        print(f\"Source: {''.join(src_tokens)}\")\n",
    "        print(f\"Prediction: {''.join(pred_chars)}\")\n",
    "    \n",
    "    # Finish wandb\n",
    "    try:\n",
    "        wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:22:26.956209Z",
     "iopub.status.busy": "2025-05-20T17:22:26.955613Z",
     "iopub.status.idle": "2025-05-20T17:22:50.593939Z",
     "shell.execute_reply": "2025-05-20T17:22:50.593453Z",
     "shell.execute_reply.started": "2025-05-20T17:22:26.956186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_172227-hptb2pjo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hptb2pjo' target=\"_blank\">d3js-attention-simpler</a></strong> to <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hptb2pjo' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hptb2pjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized wandb\n",
      "Building vocabulary from test data\n",
      "Loaded 5747 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamloo', Native: 'అంకంలో'\n",
      "Vocab sizes -> src: 30, tgt: 64\n",
      "Loaded 5747 examples from /kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
      "Sample examples:\n",
      "  Roman: 'amkamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamlo', Native: 'అంకంలో'\n",
      "  Roman: 'ankamloo', Native: 'అంకంలో'\n",
      "Error loading model: Error(s) in loading state_dict for Seq2Seq:\n",
      "\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([67, 512]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n",
      "\tsize mismatch for decoder.fc_out.weight: copying a param with shape torch.Size([67, 2048]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for decoder.fc_out.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "Saved HTML visualization to /kaggle/working/d3js_attention_vis_simpler/attention_visualization.html\n",
      "Logged visualization to wandb\n",
      "Visualized 6 examples\n",
      "First sample:\n",
      "Source: cheyuchu\n",
      "Prediction: ననఒాభఎ\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>num_samples</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>num_samples</td><td>6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">d3js-attention-simpler</strong> at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hptb2pjo' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration/runs/hptb2pjo</a><br> View project at: <a href='https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/da24m016-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_172227-hptb2pjo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---- Dataset Definition ----\n",
    "class DakshinaTSVDataset(Dataset):\n",
    "    def __init__(self, tsv_file, src_vocab=None, tgt_vocab=None, max_len=64, build_vocab=False):\n",
    "        try:\n",
    "            df = pd.read_csv(tsv_file, sep='\\t', header=None,\n",
    "                            names=['native', 'roman', 'freq'], usecols=[0, 1], dtype=str)\n",
    "            # Fix the pandas warning by using a copy\n",
    "            df = df.copy()\n",
    "            df['native'] = df['native'].fillna('')\n",
    "            df['roman'] = df['roman'].fillna('')\n",
    "            self.pairs = list(zip(df['roman'], df['native']))\n",
    "            print(f\"Loaded {len(self.pairs)} examples from {tsv_file}\")\n",
    "            \n",
    "            # Store romanized words for visualization\n",
    "            self.romanized_words = df['roman'].tolist()\n",
    "            \n",
    "            # Print a few examples\n",
    "            if len(self.pairs) > 0:\n",
    "                print(\"Sample examples:\")\n",
    "                for i in range(min(3, len(self.pairs))):\n",
    "                    print(f\"  Roman: '{self.pairs[i][0]}', Native: '{self.pairs[i][1]}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            # Fallback to some example data if file not found\n",
    "            print(\"Using example data instead\")\n",
    "            self.pairs = [\n",
    "                ('farm', 'ఫార్మ్'),\n",
    "                ('adhikaash', 'అధికాష్'),\n",
    "                ('emilie', 'ఎమిలి'),\n",
    "                ('krishna', 'కృష్ణ'),\n",
    "                ('rama', 'రామ'),\n",
    "                ('amma', 'అమ్మ'),\n",
    "                ('bharathi', 'భారతి'),\n",
    "                ('srinu', 'శ్రీను'),\n",
    "                ('telugu', 'తెలుగు'),\n",
    "                ('hyderabad', 'హైదరాబాద్')\n",
    "            ]\n",
    "            # Create romanized words list\n",
    "            self.romanized_words = [pair[0] for pair in self.pairs]\n",
    "                \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.src_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self.tgt_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "            self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n",
    "            # Ensure special tokens exist\n",
    "            for v in ('<eos>', '<sos>'):\n",
    "                if v not in self.src_vocab: self.src_vocab[v] = len(self.src_vocab)\n",
    "                if v not in self.tgt_vocab: self.tgt_vocab[v] = len(self.tgt_vocab)\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        for src, tgt in self.pairs:\n",
    "            for ch in src:\n",
    "                if ch not in self.src_vocab: self.src_vocab[ch] = len(self.src_vocab)\n",
    "            for ch in tgt:\n",
    "                if ch not in self.tgt_vocab: self.tgt_vocab[ch] = len(self.tgt_vocab)\n",
    "        print(f\"Vocab sizes -> src: {len(self.src_vocab)}, tgt: {len(self.tgt_vocab)}\")\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        \n",
    "        # Add <sos> and <eos> tokens\n",
    "        src_idxs = [self.src_vocab['<sos>']] + [self.src_vocab.get(ch, self.src_vocab['<unk>']) for ch in src] + [self.src_vocab['<eos>']]\n",
    "        tgt_idxs = [self.tgt_vocab['<sos>']] + [self.tgt_vocab.get(ch, self.tgt_vocab['<unk>']) for ch in tgt] + [self.tgt_vocab['<eos>']]\n",
    "        \n",
    "        # Pad sequences\n",
    "        pad_src = [self.src_vocab['<pad>']] * max(0, self.max_len - len(src_idxs))\n",
    "        pad_tgt = [self.tgt_vocab['<pad>']] * max(0, self.max_len - len(tgt_idxs))\n",
    "        \n",
    "        # Truncate if necessary and convert to tensor\n",
    "        src_tensor = torch.tensor((src_idxs + pad_src)[:self.max_len], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor((tgt_idxs + pad_tgt)[:self.max_len], dtype=torch.long)\n",
    "        \n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "# ---- Encoder ----\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout=0, bidirectional=True, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                embedding_size, \n",
    "                hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "            \n",
    "        # Initialize weights\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Create mask for attention\n",
    "        mask = (x != 0).float()  # 0 is <pad>\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_size]\n",
    "        \n",
    "        # Pass through RNN\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # No need to reshape hidden states - just return them as is\n",
    "        # The decoder will handle the format conversion\n",
    "        \n",
    "        # Return encoder outputs, hidden state, and mask\n",
    "        return outputs, hidden, mask\n",
    "\n",
    "# ---- Attention Mechanism ----\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super().__init__()\n",
    "        # Create a linear layer to convert the concatenated hidden states to attention scores\n",
    "        self.energy = nn.Linear(enc_hidden_size + dec_hidden_size, dec_hidden_size)\n",
    "        self.v = nn.Linear(dec_hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, dec_hidden_size]\n",
    "        # encoder_outputs: [batch_size, src_len, enc_hidden_size]\n",
    "        # mask: [batch_size, src_len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        # [batch_size, dec_hidden_size] -> [batch_size, src_len, dec_hidden_size]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Create energy by concatenating encoder outputs and decoder hidden\n",
    "        # [batch_size, src_len, enc_hidden_size + dec_hidden_size]\n",
    "        energy = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        \n",
    "        # Apply attention layer\n",
    "        # [batch_size, src_len, dec_hidden_size]\n",
    "        energy = torch.tanh(self.energy(energy))\n",
    "        \n",
    "        # Get attention scores\n",
    "        # [batch_size, src_len, 1]\n",
    "        attention = self.v(energy)\n",
    "        \n",
    "        # [batch_size, src_len]\n",
    "        attention = attention.squeeze(2)\n",
    "        \n",
    "        # Mask out padding positions\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        # [batch_size, src_len]\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "# ---- Decoder ----\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, enc_hidden_size, dec_hidden_size, \n",
    "                 num_layers, dropout=0, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type.lower()\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        # Initialize attention mechanism\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        \n",
    "        # Context vector + embedding size as input to RNN\n",
    "        rnn_input_size = embedding_size + enc_hidden_size\n",
    "        \n",
    "        # Initialize RNN based on cell type\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif self.cell_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(\n",
    "                rnn_input_size, \n",
    "                dec_hidden_size, \n",
    "                num_layers=num_layers, \n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        # Final output layer that combines decoder output, context and embedding\n",
    "        self.fc_out = nn.Linear(dec_hidden_size + enc_hidden_size + embedding_size, output_size)\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'embedding' not in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "    \n",
    "    def forward_step(self, input, hidden, encoder_outputs, mask):\n",
    "        \"\"\"Single step forward for the decoder\"\"\"\n",
    "        # Embed input token\n",
    "        # [batch_size] -> [batch_size, 1, embedding_size]\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        \n",
    "        # Get appropriate hidden state for attention\n",
    "        if self.cell_type == 'lstm':\n",
    "            h_for_attn = hidden[0][-1]  # use last layer's hidden state\n",
    "        else:\n",
    "            h_for_attn = hidden[-1]  # use last layer's hidden state\n",
    "            \n",
    "        attn_weights = self.attention(h_for_attn, encoder_outputs, mask)\n",
    "        \n",
    "        # Create context vector by weighting encoder outputs with attention\n",
    "        # [batch_size, 1, src_len] * [batch_size, src_len, enc_hidden_size]\n",
    "        # -> [batch_size, 1, enc_hidden_size]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        # Combine embedded token and context vector\n",
    "        # [batch_size, 1, embedding_size + enc_hidden_size]\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        # output: [batch_size, 1, dec_hidden_size]\n",
    "        # hidden: [num_layers, batch_size, dec_hidden_size] or tuple for LSTM\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # Combine output, context and embedding for final prediction\n",
    "        # [batch_size, 1, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = torch.cat((output, context, embedded), dim=2)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        # [batch_size, dec_hidden_size + enc_hidden_size + embedding_size]\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        # Pass through final linear layer\n",
    "        # [batch_size, output_size]\n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, hidden, attn_weights\n",
    "                \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        # Run a single forward step (for the teacher forcing loop in the Seq2Seq model)\n",
    "        return self.forward_step(input, hidden, encoder_outputs, mask)\n",
    "\n",
    "# ---- Seq2Seq Model ----\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.7):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        \n",
    "    def forward(self, src, tgt, return_attention=False):\n",
    "        \"\"\"\n",
    "        Training mode (with optional attention return)\n",
    "        \n",
    "        Args:\n",
    "            src: Source tokens\n",
    "            tgt: Target tokens\n",
    "            return_attention: If True, return attention weights\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len-1, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Tensor to store attention weights if needed\n",
    "        attentions = torch.zeros(batch_size, tgt_len-1, src.shape[1]).to(self.device) if return_attention else None\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input to decoder is the <sos> token\n",
    "        input = tgt[:, 0]\n",
    "        \n",
    "        # Teacher forcing ratio\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        \n",
    "        # Decode one token at a time\n",
    "        for t in range(1, tgt_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, attn = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Store output\n",
    "            outputs[:, t-1] = output\n",
    "            \n",
    "            # Store attention weights if needed\n",
    "            if return_attention:\n",
    "                attentions[:, t-1, :] = attn\n",
    "            \n",
    "            # Next input is either true target (teacher forcing) or predicted token\n",
    "            if use_teacher_forcing:\n",
    "                input = tgt[:, t]\n",
    "            else:\n",
    "                # Get highest scoring token\n",
    "                input = output.argmax(1)\n",
    "        \n",
    "        if return_attention:\n",
    "            return outputs, attentions\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "    def decode(self, src, max_len=100):\n",
    "        \"\"\"\n",
    "        Inference mode (no teacher forcing)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Output tokens\n",
    "            attentions: Attention weights\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # Encode source\n",
    "        encoder_outputs, hidden, mask = self.encoder(src)\n",
    "        \n",
    "        # Process hidden state for decoder (bidirectional handling)\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            hidden_state, cell_state = hidden\n",
    "            \n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                dec_c = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden_state[i*2], hidden_state[i*2+1]], dim=1)\n",
    "                        c_concat = torch.cat([cell_state[i*2], cell_state[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                            dec_c[i] = c_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                            dec_c[i, :, :self.decoder.dec_hidden_size] = c_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = (dec_h, dec_c)\n",
    "        else:  # GRU or RNN\n",
    "            if self.encoder.bidirectional:\n",
    "                dec_h = torch.zeros(self.decoder.num_layers, batch_size, \n",
    "                                   self.decoder.dec_hidden_size).to(self.device)\n",
    "                \n",
    "                for i in range(self.decoder.num_layers):\n",
    "                    if i < self.encoder.num_layers:\n",
    "                        h_concat = torch.cat([hidden[i*2], hidden[i*2+1]], dim=1)\n",
    "                        \n",
    "                        if h_concat.size(1) == self.decoder.dec_hidden_size:\n",
    "                            dec_h[i] = h_concat\n",
    "                        else:\n",
    "                            dec_h[i, :, :self.decoder.dec_hidden_size] = h_concat[:, :self.decoder.dec_hidden_size]\n",
    "                \n",
    "                hidden = dec_h\n",
    "        \n",
    "        # First input is <sos> token\n",
    "        input = torch.ones(batch_size, dtype=torch.long).to(self.device) * 3  # <sos> = 3\n",
    "        \n",
    "        # Track generated tokens and attention weights\n",
    "        outputs = [input]\n",
    "        attentions = []\n",
    "        \n",
    "        # Track if sequence has ended\n",
    "        ended = torch.zeros(batch_size, dtype=torch.bool).to(self.device)\n",
    "        \n",
    "        # Decode until max length or all sequences end\n",
    "        for t in range(1, max_len):\n",
    "            # Get output from decoder\n",
    "            output, hidden, attn = self.decoder.forward_step(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            # Get next token\n",
    "            input = output.argmax(1)\n",
    "            \n",
    "            # Store output and attention\n",
    "            outputs.append(input)\n",
    "            attentions.append(attn)\n",
    "            \n",
    "            # Check if all sequences have ended\n",
    "            ended = ended | (input == 2)  # 2 is <eos>\n",
    "            if ended.all():\n",
    "                break\n",
    "                \n",
    "        # Convert list of tensors to single tensor\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len]\n",
    "        attentions = torch.stack(attentions, dim=1)  # [batch_size, seq_len-1, src_len]\n",
    "        \n",
    "        return outputs, attentions\n",
    "\n",
    "# ---- Main Function ----\n",
    "def main():\n",
    "    \"\"\"Create D3.js attention visualizations using saved model weights\"\"\"\n",
    "    # Best config\n",
    "    config = {\n",
    "        'cell_type': 'gru',\n",
    "        'dropout': 0,\n",
    "        'embedding_size': 512,\n",
    "        'num_layers': 1,\n",
    "        'batch_size': 64,  # Smaller batch size for visualization\n",
    "        'hidden_size': 512,\n",
    "        'bidirectional': True,\n",
    "        'learning_rate': 0.0005,\n",
    "        'teacher_forcing': 0.7,\n",
    "        'optim': 'adam'\n",
    "    }\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Paths\n",
    "    data_dir = '/kaggle/input/dakshina/dakshina_dataset_v1.0/te/lexicons'\n",
    "    model_dir = '/kaggle/working/d3js_attention_vis_simpler'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=\"dakshina-transliteration\",\n",
    "            name=\"d3js-attention-simpler\",\n",
    "            config=config\n",
    "        )\n",
    "        print(\"Initialized wandb\")\n",
    "    except:\n",
    "        print(\"Failed to initialize wandb - will continue without logging\")\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        # First try to load vocabulary files\n",
    "        if os.path.exists(os.path.join(model_dir, 'src_vocab.json')):\n",
    "            # Load existing vocabulary\n",
    "            with open(os.path.join(model_dir, 'src_vocab.json'), 'r') as f:\n",
    "                src_vocab = json.load(f)\n",
    "            with open(os.path.join(model_dir, 'tgt_vocab.json'), 'r') as f:\n",
    "                tgt_vocab = json.load(f)\n",
    "            print(\"Loaded vocabulary from files\")\n",
    "        # Then try to load from best_model_results\n",
    "        elif os.path.exists(os.path.join('/kaggle/working/best_model_results', 'vocab', 'src_vocab.json')):\n",
    "            # Load from best_model_results\n",
    "            with open(os.path.join('/kaggle/working/best_model_results', 'vocab', 'src_vocab.json'), 'r') as f:\n",
    "                src_vocab = json.load(f)\n",
    "            with open(os.path.join('/kaggle/working/best_model_results', 'vocab', 'tgt_vocab.json'), 'r') as f:\n",
    "                tgt_vocab = json.load(f)\n",
    "            \n",
    "            # Save to model_dir for convenience\n",
    "            with open(os.path.join(model_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(src_vocab, f, ensure_ascii=False)\n",
    "            with open(os.path.join(model_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "            \n",
    "            print(\"Loaded vocabulary from best_model_results\")\n",
    "        else:\n",
    "            # Build vocabulary from test data\n",
    "            print(\"Building vocabulary from test data\")\n",
    "            test_tsv = os.path.join(data_dir, 'te.translit.sampled.test.tsv')\n",
    "            temp_dataset = DakshinaTSVDataset(test_tsv, build_vocab=True)\n",
    "            src_vocab, tgt_vocab = temp_dataset.src_vocab, temp_dataset.tgt_vocab\n",
    "            \n",
    "            # Save vocabulary\n",
    "            with open(os.path.join(model_dir, 'src_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(src_vocab, f, ensure_ascii=False)\n",
    "            with open(os.path.join(model_dir, 'tgt_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(tgt_vocab, f, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading vocabulary: {e}\")\n",
    "        # Create minimal vocabulary\n",
    "        src_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "        tgt_vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "    \n",
    "    # Load test dataset\n",
    "    try:\n",
    "        test_tsv = os.path.join(data_dir, 'te.translit.sampled.test.tsv')\n",
    "        test_dataset = DakshinaTSVDataset(test_tsv, src_vocab, tgt_vocab)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Get romanized words for visualization\n",
    "        romanized_test_words = test_dataset.romanized_words\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {e}\")\n",
    "        # Create example dataset\n",
    "        test_dataset = DakshinaTSVDataset(None, src_vocab, tgt_vocab)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Get romanized words for visualization\n",
    "        romanized_test_words = test_dataset.romanized_words\n",
    "    \n",
    "    # Create model\n",
    "    encoder = Encoder(\n",
    "        input_size=len(src_vocab),\n",
    "        embedding_size=config['embedding_size'],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        bidirectional=config['bidirectional'],\n",
    "        cell_type=config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Calculate encoder output size (doubled if bidirectional)\n",
    "    enc_hidden_size = config['hidden_size'] * 2 if config['bidirectional'] else config['hidden_size']\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        output_size=len(tgt_vocab),\n",
    "        embedding_size=config['embedding_size'],\n",
    "        enc_hidden_size=enc_hidden_size,\n",
    "        dec_hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        cell_type=config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Create full model\n",
    "    model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=config['teacher_forcing'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    model_loaded = False\n",
    "    try:\n",
    "        # Try different paths for model weights\n",
    "        model_paths = [\n",
    "            os.path.join('/kaggle/working/best_model_results', 'best_model.pt'),\n",
    "            os.path.join('/kaggle/working/connectivity_vis', 'best_model.pt'),\n",
    "            os.path.join('/kaggle/working/d3js_attention_vis', 'best_model.pt')\n",
    "        ]\n",
    "        \n",
    "        for path in model_paths:\n",
    "            if os.path.exists(path):\n",
    "                checkpoint = torch.load(path, map_location=device)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(f\"Loaded model from {path}\")\n",
    "                model_loaded = True\n",
    "                break\n",
    "                \n",
    "        if not model_loaded:\n",
    "            print(\"Could not find saved model weights in expected locations\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create reverse vocabulary mappings for visualization\n",
    "    IDX2CHAR_SRC = {v: k for k, v in src_vocab.items()}\n",
    "    IDX2CHAR_TGT = {v: k for k, v in tgt_vocab.items()}\n",
    "    pad_idx = src_vocab['<pad>']\n",
    "    \n",
    "    # Collect all samples\n",
    "    all_samples = []\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(test_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            out, attn_weights = model(src, tgt, return_attention=True)\n",
    "            batch_size = src.size(0)\n",
    "            for b in range(batch_size):\n",
    "                try:\n",
    "                    romanized = romanized_test_words[i * batch_size + b]\n",
    "                    src_tokens = list(romanized)\n",
    "                except:\n",
    "                    # If we can't get the romanized word, use the source tokens\n",
    "                    src_tokens = [IDX2CHAR_SRC[idx.item()] for idx in src[b] if idx.item() != pad_idx and idx.item() > 3]\n",
    "                \n",
    "                tgt_tokens = [IDX2CHAR_TGT[idx.item()] for idx in tgt[b][1:] if idx.item() != pad_idx and idx.item() > 3]\n",
    "                pred_tokens = out.argmax(dim=2)[b][1:len(tgt_tokens)+1]\n",
    "                pred_chars = [IDX2CHAR_TGT[idx.item()] for idx in pred_tokens]\n",
    "                \n",
    "                # Get attention weights and trim to actual token lengths\n",
    "                if len(src_tokens) > 0 and len(pred_chars) > 0:\n",
    "                    # Make sure attention matrix dimensions match token lengths\n",
    "                    if attn_weights.shape[1] >= len(pred_chars) and attn_weights.shape[2] >= len(src_tokens):\n",
    "                        attn = attn_weights[b][:len(pred_chars), :len(src_tokens)].cpu().numpy().tolist()\n",
    "                        all_samples.append((src_tokens, pred_chars, attn))\n",
    "    \n",
    "    # Select 6 random samples for visualization (to arrange in 2 rows of 3)\n",
    "    num_samples = min(6, len(all_samples))\n",
    "    random_samples = random.sample(all_samples, num_samples)\n",
    "    \n",
    "    # Function to create HTML blocks for each sample\n",
    "    def create_html_blocks(samples):\n",
    "        html_blocks = []\n",
    "        for sample_count, (src_tokens, pred_chars, attn) in enumerate(samples):\n",
    "            input_tokens_js = json.dumps(src_tokens, ensure_ascii=False)\n",
    "            output_tokens_js = json.dumps(pred_chars, ensure_ascii=False)\n",
    "            attention_js = json.dumps(attn)\n",
    "            \n",
    "            html_block = f\"\"\"\n",
    "            <div class=\"sample-card\">\n",
    "              <h2 class=\"sample-title\">Sample {sample_count + 1}</h2>\n",
    "              <div class=\"input-section\">\n",
    "                <div class=\"input-label\">Input (Latin):</div>\n",
    "                <div id=\"input-container-{sample_count}\" class=\"input-container\">\n",
    "                  <div id=\"input-text-{sample_count}\" class=\"input-text\"></div>\n",
    "                  <div id=\"input-percentages-{sample_count}\" class=\"percentage-row\"></div>\n",
    "                </div>\n",
    "              </div>\n",
    "              <div class=\"output-section\">\n",
    "                <div class=\"output-label\">Output (Telugu):</div>\n",
    "                <div id=\"output-tokens-{sample_count}\" class=\"output-container\"></div>\n",
    "              </div>\n",
    "              \n",
    "              <script>\n",
    "                const inputTokens_{sample_count} = {input_tokens_js};\n",
    "                const outputTokens_{sample_count} = {output_tokens_js};\n",
    "                const attention_{sample_count} = {attention_js};\n",
    "                \n",
    "                // Create text spans for input\n",
    "                const inputTextDiv_{sample_count} = document.getElementById(\"input-text-{sample_count}\");\n",
    "                const percentageDiv_{sample_count} = document.getElementById(\"input-percentages-{sample_count}\");\n",
    "                const outputDiv_{sample_count} = document.getElementById(\"output-tokens-{sample_count}\");\n",
    "                \n",
    "                // Add input characters as simple text\n",
    "                inputTokens_{sample_count}.forEach((token, i) => {{\n",
    "                  // Create character span\n",
    "                  const charSpan = document.createElement(\"span\");\n",
    "                  charSpan.className = \"input-char\";\n",
    "                  charSpan.textContent = token;\n",
    "                  charSpan.id = \"input-char-{sample_count}-\" + i;\n",
    "                  inputTextDiv_{sample_count}.appendChild(charSpan);\n",
    "                  \n",
    "                  // Create percentage span \n",
    "                  const percentSpan = document.createElement(\"span\");\n",
    "                  percentSpan.className = \"percentage\";\n",
    "                  percentSpan.id = \"percentage-{sample_count}-\" + i;\n",
    "                  percentSpan.textContent = \"\";\n",
    "                  percentageDiv_{sample_count}.appendChild(percentSpan);\n",
    "                }});\n",
    "                \n",
    "                // Add output characters with hover effects\n",
    "                outputTokens_{sample_count}.forEach((token, i) => {{\n",
    "                  const tokenSpan = document.createElement(\"span\");\n",
    "                  tokenSpan.className = \"output-char\";\n",
    "                  tokenSpan.textContent = token;\n",
    "                  \n",
    "                  // Add hover events\n",
    "                  tokenSpan.addEventListener(\"mouseover\", () => {{\n",
    "                    // Reset all input chars and percentages\n",
    "                    document.querySelectorAll(\"#input-text-{sample_count} .input-char\").forEach(el => {{\n",
    "                      el.style.backgroundColor = \"\";\n",
    "                      el.style.color = \"\";\n",
    "                    }});\n",
    "                    \n",
    "                    document.querySelectorAll(\"#input-percentages-{sample_count} .percentage\").forEach(el => {{\n",
    "                      el.textContent = \"\";\n",
    "                    }});\n",
    "                    \n",
    "                    // Apply attention highlighting\n",
    "                    if (i < attention_{sample_count}.length) {{\n",
    "                      attention_{sample_count}[i].forEach((score, j) => {{\n",
    "                        if (j < inputTokens_{sample_count}.length) {{\n",
    "                          // Only highlight if score is significant\n",
    "                          if (score > 0.01) {{\n",
    "                            const color = d3.interpolateRdPu(score);\n",
    "                            const charEl = document.getElementById(\"input-char-{sample_count}-\" + j);\n",
    "                            const percentEl = document.getElementById(\"percentage-{sample_count}-\" + j);\n",
    "                            \n",
    "                            // Highlight character based on attention\n",
    "                            charEl.style.backgroundColor = color;\n",
    "                            charEl.style.color = score > 0.5 ? \"white\" : \"black\";\n",
    "                            \n",
    "                            // Show percentage \n",
    "                            percentEl.textContent = Math.round(score * 100) + \"%\";\n",
    "                          }}\n",
    "                        }}\n",
    "                      }});\n",
    "                    }}\n",
    "                  }});\n",
    "                  \n",
    "                  tokenSpan.addEventListener(\"mouseout\", () => {{\n",
    "                    // Reset all highlighting and percentages\n",
    "                    document.querySelectorAll(\"#input-text-{sample_count} .input-char\").forEach(el => {{\n",
    "                      el.style.backgroundColor = \"\";\n",
    "                      el.style.color = \"\";\n",
    "                    }});\n",
    "                    \n",
    "                    document.querySelectorAll(\"#input-percentages-{sample_count} .percentage\").forEach(el => {{\n",
    "                      el.textContent = \"\";\n",
    "                    }});\n",
    "                  }});\n",
    "                  \n",
    "                  outputDiv_{sample_count}.appendChild(tokenSpan);\n",
    "                }});\n",
    "              </script>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            html_blocks.append(html_block)\n",
    "        return html_blocks\n",
    "    \n",
    "    # Create HTML blocks\n",
    "    html_blocks = create_html_blocks(random_samples)\n",
    "    \n",
    "    # Full HTML document with simplified styling\n",
    "    full_html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "      <meta charset=\"UTF-8\" />\n",
    "      <title>d3js_attention_visualization</title>\n",
    "      <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n",
    "      <style>\n",
    "        body {{\n",
    "          font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "          margin: 0;\n",
    "          padding: 0;\n",
    "          line-height: 1.6;\n",
    "          color: #333;\n",
    "          background-color: #f8f9fa;\n",
    "        }}\n",
    "        \n",
    "        .container {{\n",
    "          max-width: 1400px;\n",
    "          margin: 0 auto;\n",
    "          padding: 20px;\n",
    "        }}\n",
    "        \n",
    "        h1 {{\n",
    "          text-align: center;\n",
    "          color: #6a1b9a;\n",
    "          margin-bottom: 20px;\n",
    "          font-size: 2rem;\n",
    "        }}\n",
    "        \n",
    "        /* Grid layout for samples */\n",
    "        .samples-grid {{\n",
    "          display: grid;\n",
    "          grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));\n",
    "          gap: 25px;\n",
    "        }}\n",
    "        \n",
    "        .sample-card {{\n",
    "          background-color: white;\n",
    "          border-radius: 8px;\n",
    "          padding: 20px;\n",
    "          box-shadow: 0 3px 10px rgba(0,0,0,0.1);\n",
    "          height: 100%;\n",
    "        }}\n",
    "        \n",
    "        .sample-title {{\n",
    "          color: #6a1b9a;\n",
    "          border-bottom: 2px solid #e1bee7;\n",
    "          padding-bottom: 10px;\n",
    "          margin-top: 0;\n",
    "          margin-bottom: 15px;\n",
    "          font-size: 1.4rem;\n",
    "        }}\n",
    "        \n",
    "        .input-section, .output-section {{\n",
    "          margin-bottom: 15px;\n",
    "        }}\n",
    "        \n",
    "        .input-label, .output-label {{\n",
    "          font-weight: bold;\n",
    "          margin-bottom: 8px;\n",
    "          font-size: 1rem;\n",
    "          color: #555;\n",
    "        }}\n",
    "        \n",
    "        .input-container {{\n",
    "          margin-top: 8px;\n",
    "        }}\n",
    "        \n",
    "        .input-text {{\n",
    "          margin-bottom: 5px;\n",
    "          letter-spacing: 1px;\n",
    "          font-size: 20px;\n",
    "        }}\n",
    "        \n",
    "        .percentage-row {{\n",
    "          display: flex;\n",
    "          font-size: 12px;\n",
    "          color: #6a1b9a;\n",
    "          font-weight: bold;\n",
    "          min-height: 16px;\n",
    "          letter-spacing: 0.5px;\n",
    "        }}\n",
    "        \n",
    "        .input-char {{\n",
    "          display: inline-block;\n",
    "          width: 20px;\n",
    "          text-align: center;\n",
    "          margin-right: 4px;\n",
    "          padding: 3px;\n",
    "          border-radius: 3px;\n",
    "          transition: background-color 0.2s ease, color 0.2s ease;\n",
    "        }}\n",
    "        \n",
    "        .percentage {{\n",
    "          display: inline-block;\n",
    "          width: 20px;\n",
    "          text-align: center;\n",
    "          margin-right: 4px;\n",
    "          padding: 2px 3px;\n",
    "        }}\n",
    "        \n",
    "        .output-container {{\n",
    "          display: flex;\n",
    "          flex-wrap: wrap;\n",
    "          gap: 8px;\n",
    "          margin-top: 8px;\n",
    "        }}\n",
    "        \n",
    "        .output-char {{\n",
    "          display: inline-flex;\n",
    "          align-items: center;\n",
    "          justify-content: center;\n",
    "          padding: 8px 12px;\n",
    "          background-color: #6a1b9a;\n",
    "          color: white;\n",
    "          border-radius: 6px;\n",
    "          font-size: 22px;\n",
    "          cursor: pointer;\n",
    "          user-select: none;\n",
    "          transition: transform 0.2s ease, box-shadow 0.2s ease;\n",
    "        }}\n",
    "        \n",
    "        .output-char:hover {{\n",
    "          transform: scale(1.05);\n",
    "          box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "        }}\n",
    "      </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <div class=\"container\">\n",
    "        <h1>d3js_attention_visualization</h1>\n",
    "        \n",
    "        <div class=\"samples-grid\">\n",
    "          {''.join(html_blocks)}\n",
    "        </div>\n",
    "      </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save HTML to file\n",
    "    html_path = os.path.join(model_dir, 'attention_visualization.html')\n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(full_html)\n",
    "    print(f\"Saved HTML visualization to {html_path}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    try:\n",
    "        wandb.log({\n",
    "            \"d3js_attention_visualization\": wandb.Html(full_html),\n",
    "            \"num_samples\": num_samples\n",
    "        })\n",
    "        print(\"Logged visualization to wandb\")\n",
    "    except:\n",
    "        print(\"Failed to log to wandb\")\n",
    "    \n",
    "    # For debugging\n",
    "    print(f\"Visualized {num_samples} examples\")\n",
    "    if random_samples:\n",
    "        print(\"First sample:\")\n",
    "        src_tokens, pred_chars, _ = random_samples[0]\n",
    "        print(f\"Source: {''.join(src_tokens)}\")\n",
    "        print(f\"Prediction: {''.join(pred_chars)}\")\n",
    "    \n",
    "    # Finish wandb\n",
    "    try:\n",
    "        wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7456456,
     "sourceId": 11865995,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7468217,
     "sourceId": 11882713,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
